"""
special_ru.py - ä¿„è¯­è¯­è¨€è§„åˆ™æ£€æµ‹æ¨¡å—
ç‰ˆæœ¬: 3.0.0 (ä¼˜åŒ–ç‰ˆ)
ä½œè€…: AI Assistant
æ—¥æœŸ: 2024

ä¼˜åŒ–è¦ç‚¹:
1. ç»Ÿä¸€çš„åº“åŠ è½½å’Œç¼“å­˜æœºåˆ¶
2. å‡å°‘é‡å¤ä»£ç 
3. æ¯ä¸ªè§„åˆ™ç‹¬ç«‹å®Œæ•´ï¼ˆæ–¹ä¾¿å•ç‹¬ä¿®æ”¹ï¼‰
4. ç»Ÿä¸€çš„é”™è¯¯å¤„ç†å’Œæ—¥å¿—
5. æ€§èƒ½ä¼˜åŒ–ï¼ˆç¼“å­˜ã€æ­£åˆ™é¢„ç¼–è¯‘ï¼‰
"""




import re
import json
import os
import inspect
from collections import defaultdict
from functools import lru_cache
try:
    import pymorphy2
    pymorphy2_AVAILABLE = True
except ImportError:
    pymorphy2_AVAILABLE = False
    print("pymorphy2åº“æœªå®‰è£…ï¼Œæ­£åœ¨è‡ªåŠ¨å®‰è£…...")
    try:
        import subprocess
        import sys
        subprocess.check_call([sys.executable, "-m", "pip", "install", "pymorphy2", "-i", "https://pypi.tuna.tsinghua.edu.cn/simple"])
        print("pymorphy2åº“å®‰è£…æˆåŠŸï¼Œæ­£åœ¨å¯¼å…¥...")
        import pymorphy2
        pymorphy2_AVAILABLE = True
        print("âœ… pymorphy2åº“å·²æˆåŠŸå¯¼å…¥")
    except Exception as e:
        print(f"âŒ è‡ªåŠ¨å®‰è£…å¤±è´¥: {e}")
        print("è¯·æ‰‹åŠ¨è¿è¡Œ: pip install pymorphy2")
        pymorphy2_AVAILABLE = False

try:
    import russtress
    russtress_AVAILABLE = True
    print("âœ… russtressåº“å·²å­˜åœ¨")
except ImportError:
    russtress_AVAILABLE = False
    print("russtressåº“æœªå®‰è£…ï¼Œæ­£åœ¨è‡ªåŠ¨å®‰è£…...")
    try:
        import subprocess
        import sys
        
        # åŒæ—¶å®‰è£… russtress å’Œç²¾ç¡®ç‰ˆæœ¬çš„ NumPy
        subprocess.check_call([
            sys.executable, "-m", "pip", "install", 
            "russtress",
            "numpy==1.25.0",  # ç²¾ç¡®é”å®šä¸º 1.25.0
            "-i", "https://pypi.tuna.tsinghua.edu.cn/simple"
        ])
        
        print("russtressåº“å®‰è£…æˆåŠŸï¼Œæ­£åœ¨å¯¼å…¥...")
        import russtress
        import numpy as np
        russtress_AVAILABLE = True
        print(f"âœ… russtressåº“å·²æˆåŠŸå¯¼å…¥")
        print(f"âœ… NumPy ç‰ˆæœ¬: {np.__version__}")
        
    except Exception as e:
        print(f"âŒ è‡ªåŠ¨å®‰è£…å¤±è´¥: {e}")
        print("è¯·æ‰‹åŠ¨è¿è¡Œ: pip install russtress numpy==1.25.0")
        russtress_AVAILABLE = False



try:
    import tensorflow
    tensorflow_AVAILABLE = True
except ImportError:
    tensorflow_AVAILABLE = False
    print("tensorflowåº“æœªå®‰è£…ï¼Œæ­£åœ¨è‡ªåŠ¨å®‰è£…...")
    try:
        import subprocess
        import sys
        subprocess.check_call([sys.executable, "-m", "pip", "install", "tensorflow", "-i", "https://pypi.tuna.tsinghua.edu.cn/simple"])
        print("tensorflowåº“å®‰è£…æˆåŠŸï¼Œæ­£åœ¨å¯¼å…¥...")
        import tensorflow
        tensorflow_AVAILABLE = True
        print("âœ… tensorflowåº“å·²æˆåŠŸå¯¼å…¥")
    except Exception as e:
        print(f"âŒ è‡ªåŠ¨å®‰è£…å¤±è´¥: {e}")
        print("è¯·æ‰‹åŠ¨è¿è¡Œ: pip install tensorflow")
        tensorflow_AVAILABLE = False
# ==================== ç‰ˆæœ¬æ ‡è®° ====================
__VERSION__ = "3.0.0"

# ==================== å…¼å®¹æ€§ä¿®å¤ ====================
if not hasattr(inspect, 'getargspec'):
    inspect.getargspec = inspect.getfullargspec


# ==================== å…¨å±€åº“ç®¡ç†å™¨ï¼ˆç»Ÿä¸€ç®¡ç†å¤–éƒ¨ä¾èµ–ï¼‰ ====================
class LibraryManager:
    """ç»Ÿä¸€ç®¡ç† Pymorphy2 å’Œ Russtress çš„åŠ è½½å’Œç¼“å­˜"""
    
    _morph = None
    _morph_available = None
    _stresser = None
    _stresser_available = None
    
    @classmethod
    def get_morph(cls):
        """è·å– Pymorphy2 åˆ†æå™¨ï¼ˆå»¶è¿ŸåŠ è½½+ç¼“å­˜ï¼‰"""
        if cls._morph_available is None:
            try:
                import pymorphy2
                cls._morph = pymorphy2.MorphAnalyzer()
                cls._morph_available = True
            except ImportError:
                cls._morph = None
                cls._morph_available = False
            except Exception as e:
                print(f"[ERROR] Pymorphy2 åŠ è½½å¤±è´¥: {e}")
                cls._morph = None
                cls._morph_available = False
        
        return cls._morph, cls._morph_available
    
    @classmethod
    def get_stresser(cls):
        """è·å– Russtress é‡éŸ³æ ‡æ³¨å™¨ï¼ˆå»¶è¿ŸåŠ è½½+ç¼“å­˜ï¼‰"""
        if cls._stresser_available is None:
            try:
                os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
                os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'
                
                import tensorflow as tf
                if hasattr(tf, 'compat') and hasattr(tf.compat, 'v1'):
                    tf.compat.v1.disable_eager_execution()
                
                import russtress
                accent_marker = russtress.Accent()
                
                class StresserWrapper:
                    def __init__(self, marker):
                        self.marker = marker
                    def stress(self, text):
                        result = self.marker.put_stress(text)
                        return result[0] if isinstance(result, list) and result else text
                
                cls._stresser = StresserWrapper(accent_marker)
                cls._stresser_available = True
            except Exception:
                cls._stresser = None
                cls._stresser_available = False
        
        return cls._stresser, cls._stresser_available


# ==================== é€šç”¨å·¥å…·å‡½æ•°ï¼ˆæ‰€æœ‰è§„åˆ™å…±ç”¨ï¼‰ ====================
def create_logger(debug=False):
    """åˆ›å»ºç»Ÿä¸€çš„æ—¥å¿—å‡½æ•°"""
    def log(message):
        if debug:
            print(message)
    return log


def extract_russian_words(text):
    """æå–ä¿„è¯­å•è¯"""
    return re.findall(r'\b[Ğ°-ÑÑ‘Ğ-Ğ¯Ğ]+\b', text)


def parse_keywords(keywords):
    """è§£æå…³é”®è¯å‚æ•°ï¼ˆæ”¯æŒå¤šç§æ ¼å¼ï¼‰"""
    if not keywords:
        return []
    
    if isinstance(keywords, list):
        return keywords
    
    if isinstance(keywords, str):
        keywords = keywords.strip()
        
        if keywords.startswith('[') and keywords.endswith(']'):
            try:
                parsed = json.loads(keywords)
                if isinstance(parsed, list):
                    return [str(k).strip() for k in parsed]
            except json.JSONDecodeError:
                try:
                    inner = keywords[1:-1]
                    return [item.strip().strip('"\'') for item in inner.split(',') if item.strip()]
                except:
                    pass
        
        if ',' in keywords:
            return [k.strip() for k in keywords.split(',') if k.strip()]
        
        return [keywords]
    
    return [str(keywords)]


# ==================== è§„åˆ™ 1: ä¿„è¯­é‡éŸ³å˜ä¹‰è¯æ£€æµ‹ ====================
def rus_stress_homonym_usage(content_list, target_word, required_count):
    """
    ä¿„è¯­é‡éŸ³å˜ä¹‰è¯æ£€æµ‹è§„åˆ™ï¼ˆåŸºäºè¯­ä¹‰åˆ¤æ–­ï¼‰
    
    æ£€æµ‹æ–‡æœ¬ä¸­æ˜¯å¦æ­£ç¡®ä½¿ç”¨äº†é‡éŸ³å˜ä¹‰è¯çš„ä¸åŒè¯­ä¹‰å½¢å¼ã€‚
    ç”±äºä¿„è¯­æ–‡æœ¬é€šå¸¸ä¸æ ‡æ³¨é‡éŸ³ï¼Œæœ¬è§„åˆ™ä¸»è¦é€šè¿‡è¯­ä¹‰ä¸Šä¸‹æ–‡åˆ¤æ–­ã€‚
    
    Args:
        content_list: æ–‡æœ¬å†…å®¹åˆ—è¡¨
        target_word: ç›®æ ‡é‡éŸ³å˜ä¹‰è¯ï¼ˆæ— é‡éŸ³ç‰ˆæœ¬ï¼‰
        required_count: è¦æ±‚çš„ä¸åŒè¯­ä¹‰ç±»å‹æ•°é‡
    
    Returns:
        tuple: (1/0, è¯´æ˜ä¿¡æ¯)
    """
    # è¾“å…¥éªŒè¯
    if not isinstance(content_list, list):
        return 0, "âŒ content is not a list format"
    
    if not content_list:
        return 0, "âŒ content list is empty"
    
    # âœ… æ¸…ç† target_word ä¸­å¯èƒ½å­˜åœ¨çš„é‡éŸ³ç¬¦å·
    target_word = target_word.replace('Ì', '')
    
    combined_text = ' '.join(content_list)
    
    # æ­¥éª¤1: æ£€æµ‹å…³é”®è¯æ˜¯å¦å­˜åœ¨
    word_result = _stress_detect_target_word(combined_text, target_word)
    if word_result[0] == 0:
        return word_result
    
    # æ­¥éª¤2: æå–æ‰€æœ‰å…³é”®è¯å‡ºç°ä½ç½®ï¼ˆåŒ…æ‹¬å¸¦é‡éŸ³å’Œä¸å¸¦é‡éŸ³çš„ï¼‰
    all_matches = _stress_find_all_occurrences(combined_text, target_word)
    
    if not all_matches:
        return 0, f"âŒ æœªæ‰¾åˆ°å…³é”®è¯'{target_word}'çš„ä»»ä½•å‡ºç°"
    
    # æ­¥éª¤3: åˆ†ææ¯ä¸ªå‡ºç°ä½ç½®çš„è¯­ä¹‰
    semantic_result = _stress_analyze_all_semantics(
        combined_text, target_word, required_count, all_matches
    )
    
    return semantic_result


def _stress_detect_target_word(text, target_word):
    """æ­¥éª¤1: æ£€æµ‹å…³é”®è¯æ˜¯å¦å­˜åœ¨"""
    # æ¸…ç†é‡éŸ³ç¬¦å·
    clean_target = target_word.replace('Ì', '')
    
    # åŒ¹é…ä¸å¸¦é‡éŸ³çš„åŸºç¡€å½¢å¼
    basic_pattern = re.compile(r'\b' + re.escape(clean_target) + r'\b', re.IGNORECASE)
    basic_matches = basic_pattern.findall(text.lower())
    
    # åŒ¹é…å¸¦é‡éŸ³çš„å˜ä½“
    stress_variants = _stress_get_variants(clean_target)
    stress_matches = []
    
    for variant in stress_variants:
        variant_pattern = re.compile(r'\b' + re.escape(variant) + r'\b', re.IGNORECASE)
        stress_matches.extend(variant_pattern.findall(text))
    
    total = len(basic_matches) + len(stress_matches)
    
    if total == 0:
        return 0, f"âŒ æœªæ‰¾åˆ°å…³é”®è¯'{target_word}'"
    
    return 1, f"âœ… æ‰¾åˆ°å…³é”®è¯'{target_word}' {total}æ¬¡"


def _stress_find_all_occurrences(text, target_word):
    """æ­¥éª¤2: æŸ¥æ‰¾æ‰€æœ‰å‡ºç°ä½ç½®ï¼ˆä¸å¸¦é‡éŸ³å’Œå¸¦é‡éŸ³çš„éƒ½æ‰¾ï¼‰"""
    clean_target = target_word.replace('Ì', '')
    all_matches = []
    
    # æŸ¥æ‰¾ä¸å¸¦é‡éŸ³çš„åŸºç¡€å½¢å¼
    basic_pattern = re.compile(r'\b' + re.escape(clean_target) + r'\b', re.IGNORECASE)
    for match in basic_pattern.finditer(text):
        all_matches.append({
            'word': match.group(),
            'start': match.start(),
            'end': match.end(),
            'has_stress_mark': False
        })
    
    # æŸ¥æ‰¾å¸¦é‡éŸ³çš„å˜ä½“
    stress_variants = _stress_get_variants(clean_target)
    for variant in stress_variants:
        variant_pattern = re.compile(r'\b' + re.escape(variant) + r'\b')
        for match in variant_pattern.finditer(text):
            all_matches.append({
                'word': match.group(),
                'start': match.start(),
                'end': match.end(),
                'has_stress_mark': True,
                'variant': variant
            })
    
    # æŒ‰ä½ç½®æ’åºå¹¶å»é‡
    all_matches.sort(key=lambda x: x['start'])
    
    # å»é™¤é‡å¤ä½ç½®ï¼ˆå¦‚æœåŒä¸€ä½ç½®æ—¢æœ‰å¸¦é‡éŸ³åˆæœ‰ä¸å¸¦é‡éŸ³çš„åŒ¹é…ï¼‰
    unique_matches = []
    seen_positions = set()
    for match in all_matches:
        if match['start'] not in seen_positions:
            unique_matches.append(match)
            seen_positions.add(match['start'])
    
    return unique_matches


def _stress_find_sentence(text, start_pos, end_pos):
    """æå–åŒ…å«å…³é”®è¯çš„å®Œæ•´å¥å­"""
    # å®šä¹‰å¥å­ç»“æŸæ ‡è®°
    sentence_endings = '.!?ã€‚ï¼ï¼Ÿ\n'
    
    # å‘å‰æŸ¥æ‰¾å¥å­å¼€å§‹
    sentence_start = start_pos
    for i in range(start_pos - 1, -1, -1):
        if text[i] in sentence_endings:
            sentence_start = i + 1
            break
        if i == 0:
            sentence_start = 0
    
    # å‘åæŸ¥æ‰¾å¥å­ç»“æŸ
    sentence_end = end_pos
    for i in range(end_pos, len(text)):
        if text[i] in sentence_endings:
            sentence_end = i + 1
            break
        if i == len(text) - 1:
            sentence_end = len(text)
    
    # æå–å¹¶æ¸…ç†å¥å­
    sentence = text[sentence_start:sentence_end].strip()
    return sentence


def _stress_analyze_all_semantics(text, target_word, required_count, all_matches):
    """æ­¥éª¤3: åˆ†ææ‰€æœ‰å‡ºç°ä½ç½®çš„è¯­ä¹‰"""
    clean_target = target_word.replace('Ì', '')
    
    # æ£€æŸ¥æ˜¯å¦æ˜¯å·²çŸ¥çš„é‡éŸ³å˜ä¹‰è¯
    semantic_categories = _stress_get_semantic_categories(clean_target)
    if not semantic_categories:
        return 0, f"âŒ '{target_word}'ä¸æ˜¯å·²çŸ¥çš„é‡éŸ³å˜ä¹‰è¯"
    
    semantic_results = []
    
    for match_info in all_matches:
        start = match_info['start']
        end = match_info['end']
        word = match_info['word']
        
        # æå–åŒ…å«å…³é”®è¯çš„å®Œæ•´å¥å­
        sentence = _stress_find_sentence(text, start, end)
        
        # æå–ä¸Šä¸‹æ–‡ç”¨äºè¯­ä¹‰åˆ†æ
        context = text[max(0, start-150):min(len(text), end+150)].lower()
        
        # åˆ†æè¯­ä¹‰
        semantic_type = _stress_analyze_context(context, word, clean_target, semantic_categories)
        
        semantic_results.append({
            'word': word,
            'sentence': sentence,
            'semantic': semantic_type
        })
    
    # ç»Ÿè®¡ä¸åŒçš„è¯­ä¹‰ç±»å‹ï¼ˆæ’é™¤ unknownï¼‰
    unique_semantics = set(result['semantic'] for result in semantic_results if result['semantic'] != 'unknown')
    
    # ç”Ÿæˆè¾“å‡ºä¿¡æ¯
    output_lines = []
    output_lines.append(f"å…³é”®è¯'{target_word}'å…±å‡ºç° {len(all_matches)} æ¬¡ï¼š")
    
    for i, result in enumerate(semantic_results, 1):
        output_lines.append(f"\nã€ç¬¬{i}æ¬¡ã€‘{result['word']}")
        output_lines.append(f"å¥å­ï¼š{result['sentence']}")
    
    # åˆ¤æ–­ç»“æœ
    output_lines.append("")
    if len(unique_semantics) >= required_count:
        output_lines.append(f"âœ… ç¬¦åˆé¢˜ç›®è¦æ±‚")
        return 1, "\n".join(output_lines)
    else:
        output_lines.append(f"âŒ ä¸ç¬¦åˆé¢˜ç›®è¦æ±‚")
        return 0, "\n".join(output_lines)


def _stress_get_variants(target_word):
    """è·å–é‡éŸ³å˜ä¹‰è¯å˜ä½“"""
    # âœ… æ¸…ç†é‡éŸ³ç¬¦å·
    clean_word = target_word.replace('Ì', '')
    
    db = {
        # å•æ•°å½¢å¼
        "Ğ°Ñ‚Ğ»Ğ°Ñ": ["Ğ°ÌÑ‚Ğ»Ğ°Ñ", "Ğ°Ñ‚Ğ»Ğ°ÌÑ"],
        "Ğ¼ÑƒĞºĞ°": ["Ğ¼ÑƒÌĞºĞ°", "Ğ¼ÑƒĞºĞ°Ì"],
        "Ğ·Ğ°Ğ¼Ğ¾Ğº": ["Ğ·Ğ°ÌĞ¼Ğ¾Ğº", "Ğ·Ğ°Ğ¼Ğ¾ÌĞº"],
        "Ğ¾Ñ€Ğ³Ğ°Ğ½": ["Ğ¾ÌÑ€Ğ³Ğ°Ğ½", "Ğ¾Ñ€Ğ³Ğ°ÌĞ½"],
        "Ñ…Ğ»Ğ¾Ğ¿Ğ¾Ğº": ["Ñ…Ğ»Ğ¾ÌĞ¿Ğ¾Ğº", "Ñ…Ğ»Ğ¾Ğ¿Ğ¾ÌĞº"],
        "ÑÑ‚Ñ€ĞµĞ»ĞºĞ°": ["ÑÑ‚Ñ€ĞµÌĞ»ĞºĞ°", "ÑÑ‚Ñ€ĞµĞ»ĞºĞ°Ì"],
        "Ğ¿Ğ¾Ğ»ĞºĞ°": ["Ğ¿Ğ¾ÌĞ»ĞºĞ°", "Ğ¿Ğ¾Ğ»ĞºĞ°Ì"],
        "Ğ¿Ğ¸Ğ»Ğ¸": ["Ğ¿Ğ¸ÌĞ»Ğ¸", "Ğ¿Ğ¸Ğ»Ğ¸Ì"],
        "Ğ¿Ğ°Ñ€Ğ¸Ñ‚ÑŒ": ["Ğ¿Ğ°ÌÑ€Ğ¸Ñ‚ÑŒ", "Ğ¿Ğ°Ñ€Ğ¸ÌÑ‚ÑŒ"],
        "Ğ¿Ñ€Ğ¾Ğ¿Ğ°ÑÑ‚ÑŒ": ["Ğ¿Ñ€Ğ¾ÌĞ¿Ğ°ÑÑ‚ÑŒ", "Ğ¿Ñ€Ğ¾Ğ¿Ğ°ÌÑÑ‚ÑŒ"],
        
        # å¤æ•°/å˜æ ¼å½¢å¼
        "Ñ‡Ğ°ÑÑ‹": ["Ñ‡Ğ°ÑÑ‹Ì", "Ñ‡Ğ°ÌÑÑ‹"],
        "ĞºÑ€ÑƒĞ¶ĞºĞ¸": ["ĞºÑ€ÑƒÌĞ¶ĞºĞ¸", "ĞºÑ€ÑƒĞ¶ĞºĞ¸Ì"],
        "ÑÑ‚Ñ€ĞµĞ»ĞºĞ¸": ["ÑÑ‚Ñ€ĞµÌĞ»ĞºĞ¸", "ÑÑ‚Ñ€ĞµĞ»ĞºĞ¸Ì"],
        "Ğ¿Ğ¾Ğ»ĞºĞ¸": ["Ğ¿Ğ¾ÌĞ»ĞºĞ¸", "Ğ¿Ğ¾Ğ»ĞºĞ¸Ì"],
        
        # å˜æ ¼å½¢å¼è¡¥å……
        "Ğ°Ñ‚Ğ»Ğ°ÑĞ°": ["Ğ°ÌÑ‚Ğ»Ğ°ÑĞ°", "Ğ°Ñ‚Ğ»Ğ°ÌÑĞ°"],
        "Ğ¼ÑƒĞºĞ¸": ["Ğ¼ÑƒÌĞºĞ¸", "Ğ¼ÑƒĞºĞ¸Ì"],
        "Ğ·Ğ°Ğ¼ĞºĞ°": ["Ğ·Ğ°ÌĞ¼ĞºĞ°", "Ğ·Ğ°Ğ¼ĞºĞ°Ì"],
        "Ğ¾Ñ€Ğ³Ğ°Ğ½Ğ°": ["Ğ¾ÌÑ€Ğ³Ğ°Ğ½Ğ°", "Ğ¾Ñ€Ğ³Ğ°ÌĞ½Ğ°"],
        "Ñ…Ğ»Ğ¾Ğ¿ĞºĞ°": ["Ñ…Ğ»Ğ¾ÌĞ¿ĞºĞ°", "Ñ…Ğ»Ğ¾Ğ¿ĞºĞ°Ì"],
        "ÑÑ‚Ñ€ĞµĞ»ĞºÑƒ": ["ÑÑ‚Ñ€ĞµÌĞ»ĞºÑƒ", "ÑÑ‚Ñ€ĞµĞ»ĞºÑƒÌ"],
        "ÑÑ‚Ñ€ĞµĞ»ĞºĞ¾Ğ¹": ["ÑÑ‚Ñ€ĞµÌĞ»ĞºĞ¾Ğ¹", "ÑÑ‚Ñ€ĞµĞ»ĞºĞ¾ÌĞ¹"],
    }
    
    return db.get(clean_word, [])


def _stress_get_semantic_categories(target_word):
    """è·å–ç‰¹å®šè¯çš„è¯­ä¹‰ç±»åˆ«"""
    clean_word = target_word.replace('Ì', '')
    
    semantic_map = {
        "Ğ°Ñ‚Ğ»Ğ°Ñ": ['geography', 'fabric'],
        "Ğ¼ÑƒĞºĞ°": ['flour', 'suffering'],
        "Ğ·Ğ°Ğ¼Ğ¾Ğº": ['castle', 'lock'],
        "Ğ¾Ñ€Ğ³Ğ°Ğ½": ['anatomy', 'instrument'],
        "Ñ…Ğ»Ğ¾Ğ¿Ğ¾Ğº": ['cotton', 'clap'],
        "ÑÑ‚Ñ€ĞµĞ»ĞºĞ°": ['pointer', 'shooter'],
        "Ğ¿Ğ¾Ğ»ĞºĞ°": ['shelf', 'regiment'],
        "Ğ¿Ğ¸Ğ»Ğ¸": ['drink_past', 'saw_past'],
        "Ğ¿Ğ°Ñ€Ğ¸Ñ‚ÑŒ": ['soar', 'steam'],
        "Ğ¿Ñ€Ğ¾Ğ¿Ğ°ÑÑ‚ÑŒ": ['abyss', 'disappear'],
        "Ñ‡Ğ°ÑÑ‹": ['device', 'duration'],
        "ĞºÑ€ÑƒĞ¶ĞºĞ¸": ['cup', 'circle'],
        "ÑÑ‚Ñ€ĞµĞ»ĞºĞ¸": ['pointer', 'shooter'],
        "Ğ¿Ğ¾Ğ»ĞºĞ¸": ['shelf', 'regiment'],
    }
    
    return semantic_map.get(clean_word, [])


def _stress_analyze_context(context, variant, base_word, valid_categories):
    """åˆ†æè¯­ä¹‰è¯­å¢ƒï¼ˆåªè¿”å›è¯¥è¯çš„æœ‰æ•ˆè¯­ä¹‰ç±»å‹ï¼‰"""
    indicators = {
        # Ğ°Ñ‚Ğ»Ğ°Ñ
        'geography': ['ĞºĞ°Ñ€Ñ‚Ğ°', 'Ğ³ĞµĞ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ', 'ÑÑ‚Ñ€Ğ°Ğ½Ğ°', 'Ğ³Ğ¾Ñ€Ğ¾Ğ´', 'Ğ¿ÑƒÑ‚ĞµÑˆĞµÑÑ‚Ğ²Ğ¸Ğµ', 'Ğ¼Ğ¸Ñ€', 'ĞºĞ¾Ğ½Ñ‚Ğ¸Ğ½ĞµĞ½Ñ‚', 'Ğ³Ğ»Ğ¾Ğ±ÑƒÑ'],
        'fabric': ['Ñ‚ĞºĞ°Ğ½ÑŒ', 'Ğ¼Ğ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ»', 'Ğ¾Ğ´ĞµĞ¶Ğ´Ğ°', 'Ğ¿Ğ»Ğ°Ñ‚ÑŒĞµ', 'ÑˆĞµĞ»Ğº', 'Ğ±Ğ»ĞµÑÑ‚ÑÑ‰Ğ¸Ğ¹', 'Ğ³Ğ»Ğ°Ğ´ĞºĞ¸Ğ¹'],
        
        # Ğ¼ÑƒĞºĞ°
        'flour': ['Ñ…Ğ»ĞµĞ±', 'Ñ‚ĞµÑÑ‚Ğ¾', 'Ğ²Ñ‹Ğ¿ĞµÑ‡ĞºĞ°', 'Ğ³Ğ¾Ñ‚Ğ¾Ğ²Ğ¸Ñ‚ÑŒ', 'Ğ¸Ğ½Ğ³Ñ€ĞµĞ´Ğ¸ĞµĞ½Ñ‚', 'Ğ¼ĞµÑˆĞ¾Ğº', 'Ğ¿ÑˆĞµĞ½Ğ¸Ñ‡Ğ½', 'Ğ¿ĞµÑ‡ÑŒ'],
        'suffering': ['Ğ±Ğ¾Ğ»ÑŒ', 'ÑÑ‚Ñ€Ğ°Ğ´Ğ°Ğ½Ğ¸Ğµ', 'Ğ¼ÑƒÑ‡ĞµĞ½Ğ¸Ğµ', 'Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¾', 'Ñ‚ÑĞ¶ĞµĞ»Ğ¾', 'Ñ‚ĞµÑ€Ğ¿ĞµÑ‚ÑŒ', 'Ğ¸ÑĞ¿Ñ‹Ñ‚Ğ°Ğ½Ğ¸Ğµ', 'Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ°'],
        
        # Ğ·Ğ°Ğ¼Ğ¾Ğº
        'castle': ['Ğ´Ğ²Ğ¾Ñ€ĞµÑ†', 'ĞºÑ€ĞµĞ¿Ğ¾ÑÑ‚ÑŒ', 'ÑÑ€ĞµĞ´Ğ½ĞµĞ²ĞµĞºĞ¾Ğ²Ñ‹Ğ¹', 'Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ñ', 'Ğ¼ÑƒĞ·ĞµĞ¹', 'Ğ±Ğ°ÑˆĞ½Ñ', 'ÑÑ‚ĞµĞ½Ğ°', 'ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»', 'ĞµĞ´Ğ¸Ğ½ÑÑ‚Ğ²Ğ¾', 'Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾Ğ¸Ñ‚ÑŒ', 'ÑƒĞºÑ€ĞµĞ¿Ğ¸Ñ‚ÑŒ', 'ĞºÑ€ĞµĞ¿ĞºĞ¸Ğ¹', 'ÑÑ‚Ğ°Ñ€Ğ¸Ğ½Ğ½Ñ‹Ğ¹', 'Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¼ĞµÑÑ‚Ğ°'],
        'lock': ['ĞºĞ»ÑÑ‡', 'Ğ´Ğ²ĞµÑ€ÑŒ', 'ÑĞµĞ¹Ñ„', 'Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚ÑŒ', 'Ğ·Ğ°ĞºÑ€Ñ‹Ñ‚ÑŒ', 'Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚ÑŒ', 'Ğ·Ğ°Ğ¿ĞµÑ€', 'Ğ·Ğ°Ğ¼ĞºĞ½ÑƒÑ‚', 'Ğ·Ğ°Ğ¼Ğ¾Ğº Ğ½Ğ°'],
        
        # Ğ¾Ñ€Ğ³Ğ°Ğ½
        'anatomy': ['Ñ‚ĞµĞ»Ğ¾', 'Ğ·Ğ´Ğ¾Ñ€Ğ¾Ğ²ÑŒĞµ', 'Ğ²Ñ€Ğ°Ñ‡', 'Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½Ğ°', 'Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ñ', 'Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ğ¹', 'Ğ¾Ñ€Ğ³Ğ°Ğ½Ğ¸Ğ·Ğ¼', 'Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ'],
        'instrument': ['Ğ¼ÑƒĞ·Ñ‹ĞºĞ°', 'Ñ†ĞµÑ€ĞºĞ¾Ğ²ÑŒ', 'ĞºĞ¾Ğ½Ñ†ĞµÑ€Ñ‚', 'Ğ¸Ğ³Ñ€Ğ°Ñ‚ÑŒ', 'Ğ·Ğ²ÑƒĞº', 'ĞºĞ»Ğ°Ğ²Ğ¸ÑˆĞ¸', 'Ğ¸ÑĞ¿Ğ¾Ğ»Ğ½', 'Ğ¼ĞµĞ»Ğ¾Ğ´Ğ¸Ñ'],
        
        # Ñ…Ğ»Ğ¾Ğ¿Ğ¾Ğº
        'cotton': ['Ñ‚ĞºĞ°Ğ½ÑŒ', 'Ñ€Ğ°ÑÑ‚ĞµĞ½Ğ¸Ğµ', 'Ğ¿Ğ¾Ğ»Ğµ', 'Ğ¾Ğ´ĞµĞ¶Ğ´Ğ°', 'Ñ‚ĞµĞºÑÑ‚Ğ¸Ğ»ÑŒ', 'Ğ²Ğ¾Ğ»Ğ¾ĞºĞ½Ğ¾', 'ÑƒÑ€Ğ¾Ğ¶Ğ°Ğ¹', 'Ğ²Ñ‹Ñ€Ğ°Ñ‰Ğ¸Ğ²Ğ°'],
        'clap': ['Ğ·Ğ²ÑƒĞº', 'Ñ…Ğ»Ğ¾Ğ¿Ğ°Ñ‚ÑŒ', 'ÑƒĞ´Ğ°Ñ€', 'Ğ»Ğ°Ğ´Ğ¾Ğ½Ğ¸', 'Ğ°Ğ¿Ğ»Ğ¾Ğ´Ğ¸ÑĞ¼ĞµĞ½Ñ‚Ñ‹', 'Ñ…Ğ»Ğ¾Ğ¿Ğ½Ñƒ', 'Ğ³Ñ€Ğ¾Ğ¼ĞºĞ¾'],
        
        # ÑÑ‚Ñ€ĞµĞ»ĞºĞ°
        'pointer': ['ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ‚ÑŒ', 'Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ', 'Ñ‡Ğ°ÑÑ‹', 'Ñ†Ğ¸Ñ„ĞµÑ€Ğ±Ğ»Ğ°Ñ‚', 'ĞºĞ¾Ğ¼Ğ¿Ğ°Ñ', 'Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ‚ÑŒ', 'ÑÑ‚Ñ€ĞµĞ»ĞºĞ¸', 'Ğ²Ñ€ĞµĞ¼Ñ', 'Ğ¿ÑƒÑ‚ÑŒ'],
        'shooter': ['ÑÑ‚Ñ€ĞµĞ»ÑÑ‚ÑŒ', 'Ğ¾Ñ…Ğ¾Ñ‚Ğ½Ğ¸Ğº', 'Ğ»ÑƒÑ‡Ğ½Ğ¸Ğº', 'Ğ¼ĞµÑ‚ĞºĞ¸Ğ¹', 'Ñ†ĞµĞ»ÑŒ', 'Ğ¾Ñ€ÑƒĞ¶Ğ¸Ğµ', 'Ğ²Ñ‹ÑÑ‚Ñ€ĞµĞ»', 'Ğ¿Ğ¾Ğ¿Ğ°Ğ´Ğ°Ğ½Ğ¸Ğµ'],
        
        # Ğ¿Ğ¾Ğ»ĞºĞ°/Ğ¿Ğ¾Ğ»ĞºĞ¸
        'shelf': ['ĞºĞ½Ğ¸Ğ³Ğ¸', 'Ğ¼ĞµĞ±ĞµĞ»ÑŒ', 'Ñ…Ñ€Ğ°Ğ½Ğ¸Ñ‚ÑŒ', 'ÑÑ‚ĞµĞ½Ğ°', 'ÑˆĞºĞ°Ñ„', 'Ğ²ĞµÑ‰Ğ¸', 'ÑÑ‚Ğ¾ÑÑ‚', 'Ğ»ĞµĞ¶Ğ°Ñ‚'],
        'regiment': ['Ğ°Ñ€Ğ¼Ğ¸Ñ', 'Ğ²Ğ¾Ğ¹ÑĞºĞ¾', 'Ğ²Ğ¾ĞµĞ½Ğ½Ñ‹Ğ¹', 'ÑĞ¾Ğ»Ğ´Ğ°Ñ‚', 'ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´Ğ¸Ñ€', 'ÑÑ‚Ñ€Ğ¾Ğ¹', 'Ğ±Ğ°Ñ‚Ğ°Ğ»ÑŒĞ¾Ğ½', 'Ñ€Ğ¾Ñ‚Ğ°'],
        
        # Ñ‡Ğ°ÑÑ‹
        'device': ['Ñ†Ğ¸Ñ„ĞµÑ€Ğ±Ğ»Ğ°Ñ‚', 'ÑÑ‚Ñ€ĞµĞ»ĞºĞ¸', 'Ğ²Ñ€ĞµĞ¼Ñ', 'Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼', 'Ğ½Ğ°Ñ€ÑƒÑ‡Ğ½Ñ‹Ğµ', 'Ğ½Ğ°ÑÑ‚ĞµĞ½Ğ½Ñ‹Ğµ', 'Ñ‚Ğ¸ĞºĞ°ÑÑ‚', 'Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚', 'Ñ‡Ğ°ÑÑ‹'],
        'duration': ['Ñ‡Ğ°Ñ', 'Ğ¼Ğ¸Ğ½ÑƒÑ‚', 'Ğ´Ğ¾Ğ»Ğ³Ğ¾', 'Ğ¿Ñ€Ğ¾Ğ´Ğ¾Ğ»Ğ¶Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ', 'Ğ¿ĞµÑ€Ğ¸Ğ¾Ğ´', 'Ğ´Ğ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ', 'Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ñ‡Ğ°ÑĞ¾Ğ²'],
        
        # Ğ¿Ğ¸Ğ»Ğ¸
        'drink_past': ['Ğ¿Ğ¸Ñ‚ÑŒ', 'Ñ‡Ğ°Ğ¹', 'ĞºĞ¾Ñ„Ğµ', 'Ğ²Ğ¾Ğ´Ğ°', 'Ğ½Ğ°Ğ¿Ğ¸Ñ‚Ğ¾Ğº', 'Ğ²Ñ‡ĞµÑ€Ğ°', 'Ğ²Ñ‹Ğ¿Ğ¸', 'Ğ¿ÑŒÑĞ½'],
        'saw_past': ['Ğ¿Ğ¸Ğ»Ğ¸Ñ‚ÑŒ', 'Ğ´Ñ€Ğ¾Ğ²Ğ°', 'Ğ´ĞµÑ€ĞµĞ²Ğ¾', 'Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚', 'Ñ€Ğ°ÑĞ¿Ğ¸Ğ»Ğ¸Ñ‚ÑŒ', 'Ğ±Ñ€ĞµĞ²Ğ½Ğ¾', 'Ğ´Ğ¾ÑĞºĞ¸'],
        
        # Ğ¿Ğ°Ñ€Ğ¸Ñ‚ÑŒ
        'soar': ['Ğ»ĞµÑ‚Ğ°Ñ‚ÑŒ', 'Ğ½ĞµĞ±Ğ¾', 'Ğ²Ñ‹ÑĞ¾ĞºĞ¾', 'Ğ¿Ñ‚Ğ¸Ñ†Ğ°', 'Ğ²Ğ¾Ğ·Ğ´ÑƒÑ…', 'Ğ¿Ğ°Ñ€ĞµĞ½Ğ¸Ğµ', 'Ğ²Ğ·Ğ»ĞµÑ‚', 'Ğ¾Ğ±Ğ»Ğ°ĞºĞ°'],
        'steam': ['Ğ³Ğ¾Ñ‚Ğ¾Ğ²Ğ¸Ñ‚ÑŒ', 'Ğ¿Ğ°Ñ€', 'ĞºĞ°ÑÑ‚Ñ€ÑĞ»Ñ', 'Ğ¾Ğ²Ğ¾Ñ‰Ğ¸', 'Ğ²Ğ°Ñ€Ğ¸Ñ‚ÑŒ', 'Ğ¿Ğ°Ñ€Ğ¾Ğ²Ğ¾Ğ¹', 'ĞºĞ¸Ğ¿ÑÑ‚'],
        
        # Ğ¿Ñ€Ğ¾Ğ¿Ğ°ÑÑ‚ÑŒ
        'abyss': ['Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¸Ğ¹', 'Ğ±ĞµĞ·Ğ´Ğ½Ğ°', 'Ğ¿Ñ€Ğ¾Ğ¿Ğ°ÑÑ‚ÑŒ', 'Ğ¾Ğ±Ñ€Ñ‹Ğ²', 'ĞºÑ€Ğ°Ğ¹', 'ÑƒĞ¿Ğ°ÑÑ‚ÑŒ', 'Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ğ°'],
        'disappear': ['Ğ¸ÑÑ‡ĞµĞ·Ğ½ÑƒÑ‚ÑŒ', 'Ğ¿Ğ¾Ñ‚ĞµÑ€ÑÑ‚ÑŒÑÑ', 'Ğ¿Ñ€Ğ¾Ğ¿Ğ°ÑÑ‚ÑŒ', 'Ğ¸ÑÑ‡ĞµĞ·', 'Ğ½Ğµ Ğ½Ğ°Ğ¹Ñ‚Ğ¸', 'ĞºÑƒĞ´Ğ°-Ñ‚Ğ¾'],
        
        # ĞºÑ€ÑƒĞ¶ĞºĞ¸
        'cup': ['Ñ‡Ğ°Ğ¹', 'ĞºĞ¾Ñ„Ğµ', 'Ğ¿Ğ¸Ñ‚ÑŒ', 'Ğ¿Ğ¾ÑÑƒĞ´Ğ°', 'ĞºĞµÑ€Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞº', 'Ñ„Ğ°Ñ€Ñ„Ğ¾Ñ€', 'Ğ½Ğ°Ğ¿Ğ¸Ñ‚Ğ¾Ğº'],
        'circle': ['ĞºĞ»ÑƒĞ±', 'Ğ³Ñ€ÑƒĞ¿Ğ¿Ğ°', 'Ğ·Ğ°Ğ½ÑÑ‚Ğ¸Ñ', 'Ğ¸Ğ½Ñ‚ĞµÑ€ĞµÑ', 'ÑĞµĞºÑ†Ğ¸Ñ', 'ĞºÑ€ÑƒĞ¶Ğ¾Ğº', 'ÑÑ‚ÑƒĞ´Ğ¸Ñ']
    }
    
    # âœ… åªè®¡ç®—è¯¥è¯æœ‰æ•ˆçš„è¯­ä¹‰ç±»å‹
    scores = {}
    for category in valid_categories:
        if category in indicators:
            keywords = indicators[category]
            score = sum(1 for kw in keywords if kw in context)
            if score > 0:
                scores[category] = score
    
    # å¦‚æœæ²¡æœ‰åŒ¹é…ï¼Œè¿”å› unknown
    if not scores:
        return 'unknown'
    
    # è¿”å›å¾—åˆ†æœ€é«˜çš„è¯­ä¹‰ç±»å‹
    return max(scores, key=scores.get)



# ==================== è§„åˆ™ 2: è¯„ä»·æ€§åç¼€åè¯æ£€æµ‹ ====================
def detect_russian_evaluative_nouns_contextual(content_list, required_count, target_suffixes=None):
    """
    åŸºäºè¯­å¢ƒçš„ä¿„è¯­è¯„ä»·æ€§åç¼€åè¯æ£€æµ‹
    
    ç‰¹ç‚¹:
    1. æ’é™¤ç”Ÿé€ è¯å’Œæ‹Ÿè¯
    2. æ ¹æ®è¯­å¢ƒåˆ¤æ–­è¯æ±‡çš„å®é™…æ„ä¹‰
    3. åªè¯†åˆ«çœŸå®å­˜åœ¨ä¸”æœ‰å®é™…è¯„ä»·æ„ä¹‰çš„è¯æ±‡
    4. ç²¾ç¡®åŒ¹é…æŒ‡å®šåç¼€
    
    Args:
        content_list: å¾…æ£€æµ‹çš„æ–‡æœ¬å†…å®¹
        required_count: è¦æ±‚çš„æœ€å°æ•°é‡  
        target_suffixes: ç›®æ ‡åç¼€åˆ—è¡¨
    
    Returns:
        tuple: (1/0, è¯´æ˜ä¿¡æ¯)
    """
    # è¾“å…¥éªŒè¯
    if content_list == "INVALID" or content_list is None:
        return 0, "âŒ è¾“å…¥æ–‡æœ¬æ— æ•ˆ"
    
    try:
        required_count = int(required_count)
    except (ValueError, TypeError):
        return 0, f"âŒ required_count å¿…é¡»æ˜¯æ•´æ•°: '{required_count}'"

    # æ–‡æœ¬é¢„å¤„ç†
    if isinstance(content_list, list):
        text = ' '.join(str(item) for item in content_list if item and str(item) != "INVALID")
    else:
        text = str(content_list)
    
    if not text.strip():
        return 1 if required_count <= 0 else 0, "âœ… å†…å®¹ä¸ºç©º" if required_count <= 0 else "âŒ å†…å®¹ä¸ºç©º"
    
    # å¤„ç†ç›®æ ‡åç¼€
    target_suffixes = _eval_normalize_suffixes(target_suffixes)

    try:
        # çœŸå®è¯æ±‡åº“
        AUTHENTIC_WORDS = _eval_get_word_database()
        
        # æ’é™¤è¯
        EXCLUDED = _eval_get_excluded_words()
        
        # æ ¹æ®åç¼€ç­›é€‰
        if target_suffixes == "all":
            target_words = AUTHENTIC_WORDS
        else:
            target_words = {w: info for w, info in AUTHENTIC_WORDS.items() 
                          if info['suffix'] in target_suffixes}
        
        # æå–è¯æ±‡
        found = []
        text_words = re.findall(r'[Ğ°-ÑÑ‘Ğ-Ğ¯Ğ]+', text.lower())
        
        for word in text_words:
            if word in EXCLUDED:
                continue
            
            if word in target_words:
                info = target_words[word]
                if _eval_validate_context(word, info, text):
                    display = f"{word}{info['suffix']}"
                    found.append({'display': display, 'word': word, 'info': info})
        
        # å»é‡
        unique = []
        seen = set()
        for item in found:
            if item['display'] not in seen:
                unique.append(item['display'])
                seen.add(item['display'])
        
        total = len(unique)
        
        # ç»“æœ
        suffix_desc = "è¯„ä»·æ€§åç¼€" if target_suffixes == "all" else "ã€".join(target_suffixes) + "åç¼€"
        
        if total >= required_count:
            return 1, f"âœ… æ‰¾åˆ° {total} ä¸ªå¸¦{suffix_desc}çš„çœŸå®è¯„ä»·æ€§åè¯ (è¦æ±‚â‰¥{required_count}ä¸ª): {unique}"
        else:
            return 0, f"âŒ åªæ‰¾åˆ° {total} ä¸ªå¸¦{suffix_desc}çš„çœŸå®è¯„ä»·æ€§åè¯ (è¦æ±‚â‰¥{required_count}ä¸ª): {unique}"
    
    except Exception as e:
        return 0, f"âŒ å‡½æ•°æ‰§è¡Œå¼‚å¸¸: {e}"


def _eval_normalize_suffixes(suffixes):
    """æ ‡å‡†åŒ–åç¼€"""
    if suffixes is None:
        return "all"
    
    if isinstance(suffixes, str):
        if suffixes.startswith('[') and suffixes.endswith(']'):
            try:
                import ast
                suffixes = ast.literal_eval(suffixes)
            except:
                suffixes = [suffixes.strip('[]"\'')]
        else:
            suffixes = [suffixes]
    
    if suffixes != "all":
        return [f"-{s}" if not s.startswith('-') else s for s in suffixes]
    
    return suffixes


def _eval_get_word_database():
    """è¯„ä»·æ€§è¯æ±‡æ•°æ®åº“"""
    return {
        # -Ğ¸Ğº åç¼€ï¼ˆæŒ‡å°çˆ±ç§°ï¼‰
        'Ğ´Ğ¾Ğ¼Ğ¸Ğº': {'suffix': '-Ğ¸Ğº', 'base': 'Ğ´Ğ¾Ğ¼', 'meaning': 'å°æˆ¿å­', 'evaluation': 'diminutive_affectionate'},
        'ĞºĞ¾Ñ‚Ğ¸Ğº': {'suffix': '-Ğ¸Ğº', 'base': 'ĞºĞ¾Ñ‚', 'meaning': 'å°çŒ«å’ª', 'evaluation': 'diminutive_affectionate'},
        'Ğ¿ĞµÑĞ¸Ğº': {'suffix': '-Ğ¸Ğº', 'base': 'Ğ¿Ñ‘Ñ', 'meaning': 'å°ç‹—ç‹—', 'evaluation': 'diminutive_affectionate'},
        'Ğ´Ğ²Ğ¾Ñ€Ğ¸Ğº': {'suffix': '-Ğ¸Ğº', 'base': 'Ğ´Ğ²Ğ¾Ñ€', 'meaning': 'å°é™¢å­', 'evaluation': 'diminutive_affectionate'},
        'ÑĞ°Ğ´Ğ¸Ğº': {'suffix': '-Ğ¸Ğº', 'base': 'ÑĞ°Ğ´', 'meaning': 'å°èŠ±å›­', 'evaluation': 'diminutive_affectionate'},
        'ÑÑ‚Ğ¾Ğ»Ğ¸Ğº': {'suffix': '-Ğ¸Ğº', 'base': 'ÑÑ‚Ğ¾Ğ»', 'meaning': 'å°æ¡Œå­', 'evaluation': 'diminutive_affectionate'},
        'Ğ½Ğ¾ÑĞ¸Ğº': {'suffix': '-Ğ¸Ğº', 'base': 'Ğ½Ğ¾Ñ', 'meaning': 'å°é¼»å­', 'evaluation': 'diminutive_affectionate'},
        'Ñ€Ğ¾Ñ‚Ğ¸Ğº': {'suffix': '-Ğ¸Ğº', 'base': 'Ñ€Ğ¾Ñ‚', 'meaning': 'å°å˜´å·´', 'evaluation': 'diminutive_affectionate'},
        'Ğ»ÑƒÑ‡Ğ¸Ğº': {'suffix': '-Ğ¸Ğº', 'base': 'Ğ»ÑƒÑ‡', 'meaning': 'å°å…‰æŸ', 'evaluation': 'diminutive_affectionate'},
        'Ñ†Ğ²ĞµÑ‚Ğ¸Ğº': {'suffix': '-Ğ¸Ğº', 'base': 'Ñ†Ğ²ĞµÑ‚', 'meaning': 'å°èŠ±æœµ', 'evaluation': 'diminutive_affectionate'},
        'Ğ»Ğ¸ÑÑ‚Ğ¸Ğº': {'suffix': '-Ğ¸Ğº', 'base': 'Ğ»Ğ¸ÑÑ‚', 'meaning': 'å°å¶å­', 'evaluation': 'diminutive_affectionate'},
        'Ğ´Ğ¾Ğ¶Ğ´Ğ¸Ğº': {'suffix': '-Ğ¸Ğº', 'base': 'Ğ´Ğ¾Ğ¶Ğ´ÑŒ', 'meaning': 'å°é›¨', 'evaluation': 'diminutive_affectionate'},
        'Ğ²ĞµÑ‚ĞµÑ€Ğ¸Ğº': {'suffix': '-Ğ¸Ğº', 'base': 'Ğ²ĞµÑ‚ĞµÑ€', 'meaning': 'å¾®é£', 'evaluation': 'diminutive_affectionate'},
        'Ğ¼Ğ¾ÑÑ‚Ğ¸Ğº': {'suffix': '-Ğ¸Ğº', 'base': 'Ğ¼Ğ¾ÑÑ‚', 'meaning': 'å°æ¡¥', 'evaluation': 'diminutive_affectionate'},
        'Ñ…Ğ¾Ğ»Ğ¼Ğ¸Ğº': {'suffix': '-Ğ¸Ğº', 'base': 'Ñ…Ğ¾Ğ»Ğ¼', 'meaning': 'å°å±±ä¸˜', 'evaluation': 'diminutive_affectionate'},
        'ÑˆĞ°Ñ€Ğ¸Ğº': {'suffix': '-Ğ¸Ğº', 'base': 'ÑˆĞ°Ñ€', 'meaning': 'å°çƒ', 'evaluation': 'diminutive_affectionate'},
        'ĞºÑƒĞ±Ğ¸Ğº': {'suffix': '-Ğ¸Ğº', 'base': 'ĞºÑƒĞ±', 'meaning': 'å°æ–¹å—', 'evaluation': 'diminutive_affectionate'},
        'ĞºĞ»ÑÑ‡Ğ¸Ğº': {'suffix': '-Ğ¸Ğº', 'base': 'ĞºĞ»ÑÑ‡', 'meaning': 'å°é’¥åŒ™', 'evaluation': 'diminutive_affectionate'},
        'Ğ±Ñ€Ğ°Ñ‚Ğ¸Ğº': {'suffix': '-Ğ¸Ğº', 'base': 'Ğ±Ñ€Ğ°Ñ‚', 'meaning': 'å°å¼Ÿå¼Ÿ', 'evaluation': 'diminutive_affectionate'},
        'Ñ…Ğ²Ğ¾ÑÑ‚Ğ¸Ğº': {'suffix': '-Ğ¸Ğº', 'base': 'Ñ…Ğ²Ğ¾ÑÑ‚', 'meaning': 'å°å°¾å·´', 'evaluation': 'diminutive_affectionate'},
        'Ğ·Ğ°Ğ¹Ñ‡Ğ¸Ğº': {'suffix': '-Ğ¸Ğº', 'base': 'Ğ·Ğ°ÑÑ†', 'meaning': 'å°å…”å­', 'evaluation': 'diminutive_affectionate'},
        'Ğ¿Ğ°Ğ»ÑŒÑ‡Ğ¸Ğº': {'suffix': '-Ğ¸Ğº', 'base': 'Ğ¿Ğ°Ğ»ĞµÑ†', 'meaning': 'å°æ‰‹æŒ‡', 'evaluation': 'diminutive_affectionate'},
        'Ğ³Ğ»Ğ°Ğ·Ğ¸Ğº': {'suffix': '-Ğ¸Ğº', 'base': 'Ğ³Ğ»Ğ°Ğ·', 'meaning': 'å°çœ¼ç›', 'evaluation': 'diminutive_affectionate'},
        'ÑƒÑˆĞ¸Ğº': {'suffix': '-Ğ¸Ğº', 'base': 'ÑƒÑ…Ğ¾', 'meaning': 'å°è€³æœµ', 'evaluation': 'diminutive_affectionate'},
        
        # -Ğ¾Ğº åç¼€ï¼ˆæŒ‡å°ï¼‰
        'Ğ»ĞµÑĞ¾Ğº': {'suffix': '-Ğ¾Ğº', 'base': 'Ğ»ĞµÑ', 'meaning': 'å°æ ‘æ—', 'evaluation': 'diminutive_affectionate'},
        'Ğ³Ğ¾Ñ€Ğ¾Ğ´Ğ¾Ğº': {'suffix': '-Ğ¾Ğº', 'base': 'Ğ³Ğ¾Ñ€Ğ¾Ğ´', 'meaning': 'å°åŸé•‡', 'evaluation': 'diminutive_affectionate'},
        'ÑƒĞ³Ğ¾Ğ»Ğ¾Ğº': {'suffix': '-Ğ¾Ğº', 'base': 'ÑƒĞ³Ğ¾Ğ»', 'meaning': 'å°è§’è½', 'evaluation': 'diminutive_affectionate'},
        'Ğ³Ğ¾Ğ»Ğ¾ÑĞ¾Ğº': {'suffix': '-Ğ¾Ğº', 'base': 'Ğ³Ğ¾Ğ»Ğ¾Ñ', 'meaning': 'å°å—“éŸ³', 'evaluation': 'diminutive_affectionate'},
        'ÑÑ‹Ğ½Ğ¾Ğº': {'suffix': '-Ğ¾Ğº', 'base': 'ÑÑ‹Ğ½', 'meaning': 'å„¿å­ï¼ˆçˆ±ç§°ï¼‰', 'evaluation': 'diminutive_affectionate'},
        'Ğ´Ñ€ÑƒĞ¶Ğ¾Ğº': {'suffix': '-Ğ¾Ğº', 'base': 'Ğ´Ñ€ÑƒĞ³', 'meaning': 'å°æœ‹å‹', 'evaluation': 'diminutive_affectionate'},
        'Ğ²ĞµÑ‚ĞµÑ€Ğ¾Ğº': {'suffix': '-Ğ¾Ğº', 'base': 'Ğ²ĞµÑ‚ĞµÑ€', 'meaning': 'å¾®é£', 'evaluation': 'diminutive_affectionate'},
        'Ğ»ÑƒĞ¶Ğ¾Ğº': {'suffix': '-Ğ¾Ğº', 'base': 'Ğ»ÑƒĞ³', 'meaning': 'å°è‰åœ°', 'evaluation': 'diminutive_affectionate'},
        'ÑĞ½ĞµĞ¶Ğ¾Ğº': {'suffix': '-Ğ¾Ğº', 'base': 'ÑĞ½ĞµĞ³', 'meaning': 'å°é›ªèŠ±', 'evaluation': 'diminutive_affectionate'},
        'ÑÑ‚Ğ¸ÑˆĞ¾Ğº': {'suffix': '-Ğ¾Ğº', 'base': 'ÑÑ‚Ğ¸Ñ…', 'meaning': 'å°è¯—', 'evaluation': 'diminutive_affectionate'},
        'Ğ»Ğ¸ÑÑ‚Ğ¾Ğº': {'suffix': '-Ğ¾Ğº', 'base': 'Ğ»Ğ¸ÑÑ‚', 'meaning': 'å¶å­', 'evaluation': 'diminutive_neutral'},
        'Ñ†Ğ²ĞµÑ‚Ğ¾Ğº': {'suffix': '-Ğ¾Ğº', 'base': 'Ñ†Ğ²ĞµÑ‚', 'meaning': 'èŠ±æœµ', 'evaluation': 'diminutive_neutral'},
        'Ğ¿Ğ»Ğ°Ñ‚Ğ¾Ğº': {'suffix': '-Ğ¾Ğº', 'base': 'Ğ¿Ğ»Ğ°Ñ‚', 'meaning': 'æ‰‹å¸•', 'evaluation': 'diminutive_neutral'},
        'ĞºĞ»ÑƒĞ±Ğ¾Ğº': {'suffix': '-Ğ¾Ğº', 'base': 'ĞºĞ»ÑƒĞ±', 'meaning': 'çº¿å›¢', 'evaluation': 'diminutive_neutral'},
        'Ğ¼ĞµÑˆĞ¾Ğº': {'suffix': '-Ğ¾Ğº', 'base': 'Ğ¼ĞµÑ…', 'meaning': 'è¢‹å­', 'evaluation': 'diminutive_neutral'},
        'Ğ¿Ğ¸Ñ€Ğ¾Ğ¶Ğ¾Ğº': {'suffix': '-Ğ¾Ğº', 'base': 'Ğ¿Ğ¸Ñ€Ğ¾Ğ³', 'meaning': 'å°é¦…é¥¼', 'evaluation': 'diminutive_affectionate'},
        'Ñ‚Ğ²Ğ¾Ñ€Ğ¾Ğ¶Ğ¾Ğº': {'suffix': '-Ğ¾Ğº', 'base': 'Ñ‚Ğ²Ğ¾Ñ€Ğ¾Ğ³', 'meaning': 'å°å¥¶æ¸£', 'evaluation': 'diminutive_affectionate'},
        'Ğ¼ĞµĞ´Ğ¾Ğº': {'suffix': '-Ğ¾Ğº', 'base': 'Ğ¼Ñ‘Ğ´', 'meaning': 'å°èœœç³–', 'evaluation': 'diminutive_affectionate'},
        'Ñ‚ĞµÑ€ĞµĞ¼Ğ¾Ğº': {'suffix': '-Ğ¾Ğº', 'base': 'Ñ‚ĞµÑ€ĞµĞ¼', 'meaning': 'å°æœ¨å±‹', 'evaluation': 'diminutive_affectionate'},
        
        # -Ñ‘Ğº åç¼€
        'Ğ¾Ğ³Ğ¾Ğ½Ñ‘Ğº': {'suffix': '-Ñ‘Ğº', 'base': 'Ğ¾Ğ³Ğ¾Ğ½ÑŒ', 'meaning': 'å°ç«å…‰', 'evaluation': 'diminutive_affectionate'},
        'Ğ´ĞµĞ½Ñ‘Ğº': {'suffix': '-Ñ‘Ğº', 'base': 'Ğ´ĞµĞ½ÑŒ', 'meaning': 'å°æ—¥å­', 'evaluation': 'diminutive_affectionate'},
        'Ñ€ÑƒÑ‡ĞµÑ‘Ğº': {'suffix': '-Ñ‘Ğº', 'base': 'Ñ€ÑƒÑ‡ĞµĞ¹', 'meaning': 'å°æºª', 'evaluation': 'diminutive_affectionate'},
        'Ğ¼Ğ¾Ñ‚Ñ‹Ğ»Ñ‘Ğº': {'suffix': '-Ñ‘Ğº', 'base': 'Ğ¼Ğ¾Ñ‚Ñ‹Ğ»ÑŒ', 'meaning': 'å°è›¾å­', 'evaluation': 'diminutive_affectionate'},
        'Ğ¿ÑƒĞ·Ñ‹Ñ€Ñ‘Ğº': {'suffix': '-Ñ‘Ğº', 'base': 'Ğ¿ÑƒĞ·Ñ‹Ñ€ÑŒ', 'meaning': 'å°æ³¡æ³¡', 'evaluation': 'diminutive_affectionate'},
        'ÑƒĞ³Ğ¾Ğ»Ñ‘Ğº': {'suffix': '-Ñ‘Ğº', 'base': 'ÑƒĞ³Ğ¾Ğ»ÑŒ', 'meaning': 'å°ç…¤ç‚­', 'evaluation': 'diminutive_affectionate'},
        'Ğ¿Ğ°Ñ€ĞµĞ½Ñ‘Ğº': {'suffix': '-Ñ‘Ğº', 'base': 'Ğ¿Ğ°Ñ€ĞµĞ½ÑŒ', 'meaning': 'å°ä¼™å­', 'evaluation': 'diminutive_affectionate'},
        
        # -ĞµĞº åç¼€
        'ĞºĞ°Ğ¼ĞµÑˆĞµĞº': {'suffix': '-ĞµĞº', 'base': 'ĞºĞ°Ğ¼ĞµĞ½ÑŒ', 'meaning': 'å°çŸ³å¤´', 'evaluation': 'diminutive_affectionate'},
        'Ñ†Ğ²ĞµÑ‚Ğ¾Ñ‡ĞµĞº': {'suffix': '-ĞµĞº', 'base': 'Ñ†Ğ²ĞµÑ‚Ğ¾Ğº', 'meaning': 'å°èŠ±æœµ', 'evaluation': 'diminutive_affectionate'},
        'ĞºÑƒÑĞ¾Ñ‡ĞµĞº': {'suffix': '-ĞµĞº', 'base': 'ĞºÑƒÑĞ¾Ğº', 'meaning': 'å°å—', 'evaluation': 'diminutive_affectionate'},
        'Ğ»Ğ¸ÑÑ‚Ğ¾Ñ‡ĞµĞº': {'suffix': '-ĞµĞº', 'base': 'Ğ»Ğ¸ÑÑ‚Ğ¾Ğº', 'meaning': 'å°å¶å­', 'evaluation': 'diminutive_affectionate'},
        'Ğ¼ĞµÑˆĞ¾Ñ‡ĞµĞº': {'suffix': '-ĞµĞº', 'base': 'Ğ¼ĞµÑˆĞ¾Ğº', 'meaning': 'å°è¢‹å­', 'evaluation': 'diminutive_affectionate'},
        'Ğ¿Ğ»Ğ°Ñ‚Ğ¾Ñ‡ĞµĞº': {'suffix': '-ĞµĞº', 'base': 'Ğ¿Ğ»Ğ°Ñ‚Ğ¾Ğº', 'meaning': 'å°æ‰‹å¸•', 'evaluation': 'diminutive_affectionate'},
        'ĞºĞ¾Ğ¼Ğ¾Ñ‡ĞµĞº': {'suffix': '-ĞµĞº', 'base': 'ĞºĞ¾Ğ¼Ğ¾Ğº', 'meaning': 'å°å›¢', 'evaluation': 'diminutive_affectionate'},
        'Ğ¾Ñ€ĞµÑˆĞµĞº': {'suffix': '-ĞµĞº', 'base': 'Ğ¾Ñ€ĞµÑ…', 'meaning': 'å°åšæœ', 'evaluation': 'diminutive_affectionate'},
        'Ğ³Ğ¾Ñ€Ğ¾ÑˆĞµĞº': {'suffix': '-ĞµĞº', 'base': 'Ğ³Ğ¾Ñ€Ğ¾Ñ…', 'meaning': 'è±Œè±†', 'evaluation': 'diminutive_neutral'},
        'Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµĞº': {'suffix': '-ĞµĞº', 'base': 'Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞº', 'meaning': 'å°äºº', 'evaluation': 'diminutive_affectionate'},
        'Ğ²Ğ¾Ñ€Ğ¾Ğ±Ñ‹ÑˆĞµĞº': {'suffix': '-ĞµĞº', 'base': 'Ğ²Ğ¾Ñ€Ğ¾Ğ±ĞµĞ¹', 'meaning': 'å°éº»é›€', 'evaluation': 'diminutive_affectionate'},
        
        # -Ğ¸ÑˆĞº åç¼€
        'Ğ±Ñ€Ğ°Ñ‚Ğ¸ÑˆĞºĞ°': {'suffix': '-Ğ¸ÑˆĞº', 'base': 'Ğ±Ñ€Ğ°Ñ‚', 'meaning': 'å°å…„å¼Ÿ', 'evaluation': 'diminutive_familiar'},
        'Ğ¿Ğ°Ñ€Ğ½Ğ¸ÑˆĞºĞ°': {'suffix': '-Ğ¸ÑˆĞº', 'base': 'Ğ¿Ğ°Ñ€ĞµĞ½ÑŒ', 'meaning': 'å°ä¼™å­', 'evaluation': 'diminutive_familiar'},
        'Ğ´Ğ¾Ğ¼Ğ¸ÑˆĞºĞ¾': {'suffix': '-Ğ¸ÑˆĞº', 'base': 'Ğ´Ğ¾Ğ¼', 'meaning': 'ç ´æˆ¿å­', 'evaluation': 'diminutive_pejorative'},
        'Ğ³Ğ¾Ñ€Ğ¾Ğ´Ğ¸ÑˆĞºĞ¾': {'suffix': '-Ğ¸ÑˆĞº', 'base': 'Ğ³Ğ¾Ñ€Ğ¾Ğ´', 'meaning': 'å°ç ´åŸ', 'evaluation': 'diminutive_pejorative'},
        'Ğ¼Ğ°Ğ»ÑŒÑ‡Ğ¸ÑˆĞºĞ°': {'suffix': '-Ğ¸ÑˆĞº', 'base': 'Ğ¼Ğ°Ğ»ÑŒÑ‡Ğ¸Ğº', 'meaning': 'å°ç”·å­©', 'evaluation': 'diminutive_familiar'},
        'Ğ´ĞµĞ²Ñ‡Ğ¸ÑˆĞºĞ°': {'suffix': '-Ğ¸ÑˆĞº', 'base': 'Ğ´ĞµĞ²Ğ¾Ñ‡ĞºĞ°', 'meaning': 'å°å¥³å­©', 'evaluation': 'diminutive_familiar'},
        
        # -Ğ¾Ñ‡ĞºĞ°/-ĞµÑ‡ĞºĞ°/-Ğ¸Ñ‡ĞºĞ° åç¼€
        'Ğ¼Ğ°Ğ¼Ğ¾Ñ‡ĞºĞ°': {'suffix': '-Ğ¾Ñ‡ĞºĞ°', 'base': 'Ğ¼Ğ°Ğ¼Ğ°', 'meaning': 'å¦ˆå¦ˆï¼ˆçˆ±ç§°ï¼‰', 'evaluation': 'diminutive_affectionate'},
        'Ğ¿Ğ°Ğ¿Ğ¾Ñ‡ĞºĞ°': {'suffix': '-Ğ¾Ñ‡ĞºĞ°', 'base': 'Ğ¿Ğ°Ğ¿Ğ°', 'meaning': 'çˆ¸çˆ¸ï¼ˆçˆ±ç§°ï¼‰', 'evaluation': 'diminutive_affectionate'},
        'Ğ·Ğ²Ñ‘Ğ·Ğ´Ğ¾Ñ‡ĞºĞ°': {'suffix': '-Ğ¾Ñ‡ĞºĞ°', 'base': 'Ğ·Ğ²ĞµĞ·Ğ´Ğ°', 'meaning': 'å°æ˜Ÿæ˜Ÿ', 'evaluation': 'diminutive_affectionate'},
        'Ğ»Ğ°Ğ¼Ğ¿Ğ¾Ñ‡ĞºĞ°': {'suffix': '-Ğ¾Ñ‡ĞºĞ°', 'base': 'Ğ»Ğ°Ğ¼Ğ¿Ğ°', 'meaning': 'å°ç¯æ³¡', 'evaluation': 'diminutive_neutral'},
        'Ğ²ĞµÑ‚Ğ¾Ñ‡ĞºĞ°': {'suffix': '-Ğ¾Ñ‡ĞºĞ°', 'base': 'Ğ²ĞµÑ‚ĞºĞ°', 'meaning': 'å°æ ‘æ', 'evaluation': 'diminutive_affectionate'},
        'Ğ»ĞµĞ½Ñ‚Ğ¾Ñ‡ĞºĞ°': {'suffix': '-Ğ¾Ñ‡ĞºĞ°', 'base': 'Ğ»ĞµĞ½Ñ‚Ğ°', 'meaning': 'å°ä¸å¸¦', 'evaluation': 'diminutive_affectionate'},
        'ĞºĞ¾ÑˆĞµÑ‡ĞºĞ°': {'suffix': '-ĞµÑ‡ĞºĞ°', 'base': 'ĞºĞ¾ÑˆĞºĞ°', 'meaning': 'å°çŒ«å’ª', 'evaluation': 'diminutive_affectionate'},
        'ÑĞµĞ¼ĞµÑ‡ĞºĞ°': {'suffix': '-ĞµÑ‡ĞºĞ°', 'base': 'ÑĞµĞ¼Ñ', 'meaning': 'å°ç§å­', 'evaluation': 'diminutive_neutral'},
        'Ğ¾Ğ²ĞµÑ‡ĞºĞ°': {'suffix': '-ĞµÑ‡ĞºĞ°', 'base': 'Ğ¾Ğ²Ñ†Ğ°', 'meaning': 'å°ç»µç¾Š', 'evaluation': 'diminutive_affectionate'},
        'Ğ¿Ñ‚Ğ¸Ñ‡ĞºĞ°': {'suffix': '-Ğ¸Ñ‡ĞºĞ°', 'base': 'Ğ¿Ñ‚Ğ¸Ñ†Ğ°', 'meaning': 'å°é¸Ÿ', 'evaluation': 'diminutive_affectionate'},
        'Ğ²Ğ¾Ğ´Ğ¸Ñ‡ĞºĞ°': {'suffix': '-Ğ¸Ñ‡ĞºĞ°', 'base': 'Ğ²Ğ¾Ğ´Ğ°', 'meaning': 'å°æ°´', 'evaluation': 'diminutive_affectionate'},
        'ĞºĞ¾ÑĞ¸Ñ‡ĞºĞ°': {'suffix': '-Ğ¸Ñ‡ĞºĞ°', 'base': 'ĞºĞ¾ÑĞ°', 'meaning': 'å°è¾«å­', 'evaluation': 'diminutive_affectionate'},
        
        # -ÑƒÑˆĞºĞ°/-ÑÑˆĞºĞ° åç¼€
        'ÑÑ‚Ğ°Ñ€ÑƒÑˆĞºĞ°': {'suffix': '-ÑƒÑˆĞºĞ°', 'base': 'ÑÑ‚Ğ°Ñ€ÑƒÑ…Ğ°', 'meaning': 'è€å¥¶å¥¶', 'evaluation': 'diminutive_affectionate'},
        'Ğ±Ğ°Ğ±ÑƒÑˆĞºĞ°': {'suffix': '-ÑƒÑˆĞºĞ°', 'base': 'Ğ±Ğ°Ğ±Ğ°', 'meaning': 'å¥¶å¥¶', 'evaluation': 'diminutive_affectionate'},
        'Ğ´ĞµĞ´ÑƒÑˆĞºĞ°': {'suffix': '-ÑƒÑˆĞºĞ°', 'base': 'Ğ´ĞµĞ´', 'meaning': 'çˆ·çˆ·', 'evaluation': 'diminutive_affectionate'},
        'Ğ¸Ğ·Ğ±ÑƒÑˆĞºĞ°': {'suffix': '-ÑƒÑˆĞºĞ°', 'base': 'Ğ¸Ğ·Ğ±Ğ°', 'meaning': 'å°æœ¨å±‹', 'evaluation': 'diminutive_affectionate'},
        'Ğ¿Ğ¾Ğ´Ñ€ÑƒĞ¶ĞºĞ°': {'suffix': '-ÑƒÑˆĞºĞ°', 'base': 'Ğ¿Ğ¾Ğ´Ñ€ÑƒĞ³Ğ°', 'meaning': 'å°å¥³å‹', 'evaluation': 'diminutive_affectionate'},
        'Ğ³Ğ¾Ñ€ÑƒÑˆĞºĞ°': {'suffix': '-ÑƒÑˆĞºĞ°', 'base': 'Ğ³Ğ¾Ñ€Ğ°', 'meaning': 'å°å±±', 'evaluation': 'diminutive_affectionate'},
        'Ğ²Ğ¾Ğ»ÑÑˆĞºĞ°': {'suffix': '-ÑÑˆĞºĞ°', 'base': 'Ğ²Ğ¾Ğ»Ñ', 'meaning': 'è‡ªç”±ï¼ˆçˆ±ç§°ï¼‰', 'evaluation': 'diminutive_affectionate'},
        'Ğ´Ğ¾Ğ»ÑÑˆĞºĞ°': {'suffix': '-ÑÑˆĞºĞ°', 'base': 'Ğ´Ğ¾Ğ»Ñ', 'meaning': 'å‘½è¿ï¼ˆçˆ±ç§°ï¼‰', 'evaluation': 'diminutive_affectionate'},
        'Ğ±Ğ°Ñ‚ÑÑˆĞºĞ°': {'suffix': '-ÑÑˆĞºĞ°', 'base': 'Ğ±Ğ°Ñ‚Ñ', 'meaning': 'çˆ¶äº²ï¼ˆçˆ±ç§°ï¼‰', 'evaluation': 'diminutive_affectionate'},
        'Ğ¼Ğ°Ñ‚ÑƒÑˆĞºĞ°': {'suffix': '-ÑƒÑˆĞºĞ°', 'base': 'Ğ¼Ğ°Ñ‚ÑŒ', 'meaning': 'æ¯äº²ï¼ˆçˆ±ç§°ï¼‰', 'evaluation': 'diminutive_affectionate'},
        
        # -ĞºĞ° åç¼€
        'Ğ´Ğ¾Ñ‡ĞºĞ°': {'suffix': '-ĞºĞ°', 'base': 'Ğ´Ğ¾Ñ‡ÑŒ', 'meaning': 'å¥³å„¿ï¼ˆçˆ±ç§°ï¼‰', 'evaluation': 'diminutive_affectionate'},
        'ÑĞ¾Ğ±Ğ°Ñ‡ĞºĞ°': {'suffix': '-ĞºĞ°', 'base': 'ÑĞ¾Ğ±Ğ°ĞºĞ°', 'meaning': 'å°ç‹—ç‹—', 'evaluation': 'diminutive_affectionate'},
        'Ñ€Ñ‹Ğ±ĞºĞ°': {'suffix': '-ĞºĞ°', 'base': 'Ñ€Ñ‹Ğ±Ğ°', 'meaning': 'å°é±¼', 'evaluation': 'diminutive_affectionate'},
        'Ñ€ÑƒÑ‡ĞºĞ°': {'suffix': '-ĞºĞ°', 'base': 'Ñ€ÑƒĞºĞ°', 'meaning': 'å°æ‰‹', 'evaluation': 'diminutive_affectionate'},
        'Ğ½Ğ¾Ğ¶ĞºĞ°': {'suffix': '-ĞºĞ°', 'base': 'Ğ½Ğ¾Ğ³Ğ°', 'meaning': 'å°è„š', 'evaluation': 'diminutive_affectionate'},
        'Ğ´Ğ¾Ñ€Ğ¾Ğ¶ĞºĞ°': {'suffix': '-ĞºĞ°', 'base': 'Ğ´Ğ¾Ñ€Ğ¾Ğ³Ğ°', 'meaning': 'å°è·¯', 'evaluation': 'diminutive_affectionate'},
        'Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ğ½ĞºĞ°': {'suffix': '-ĞºĞ°', 'base': 'Ñ‚Ñ€Ğ¾Ğ¿Ğ°', 'meaning': 'å°å¾„', 'evaluation': 'diminutive_affectionate'},
        'Ğ³Ğ¾Ğ»Ğ¾Ğ²ĞºĞ°': {'suffix': '-ĞºĞ°', 'base': 'Ğ³Ğ¾Ğ»Ğ¾Ğ²Ğ°', 'meaning': 'å°å¤´', 'evaluation': 'diminutive_affectionate'},
        'Ğ¼Ğ¾Ñ€ĞºĞ¾Ğ²ĞºĞ°': {'suffix': '-ĞºĞ°', 'base': 'Ğ¼Ğ¾Ñ€ĞºĞ¾Ğ²ÑŒ', 'meaning': 'èƒ¡èåœ', 'evaluation': 'diminutive_neutral'},
        'Ñ€ĞµÑ‡ĞºĞ°': {'suffix': '-ĞºĞ°', 'base': 'Ñ€ĞµĞºĞ°', 'meaning': 'å°æ²³', 'evaluation': 'diminutive_affectionate'},
        
        # å¢å¤§åç¼€ -Ğ¸Ğ½
        'Ğ¸ÑĞ¿Ğ¾Ğ»Ğ¸Ğ½': {'suffix': '-Ğ¸Ğ½', 'base': 'Ğ¸ÑĞ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ÑŒ', 'meaning': 'å·¨äºº', 'evaluation': 'augmentative_awe'},
        'Ğ³Ğ¾ÑĞ¿Ğ¾Ğ´Ğ¸Ğ½': {'suffix': '-Ğ¸Ğ½', 'base': 'Ğ³Ğ¾ÑĞ¿Ğ¾Ğ´ÑŒ', 'meaning': 'å…ˆç”Ÿã€ä¸»äºº', 'evaluation': 'augmentative_respect'},
        'Ğ±Ğ¾Ğ»ÑÑ€Ğ¸Ğ½': {'suffix': '-Ğ¸Ğ½', 'base': 'Ğ±Ğ¾Ğ»ÑÑ€', 'meaning': 'è´µæ—', 'evaluation': 'augmentative_respect'},
        
        # -Ğ°Ğ½ åç¼€
        'Ğ²ĞµĞ»Ğ¸ĞºĞ°Ğ½': {'suffix': '-Ğ°Ğ½', 'base': 'Ğ²ĞµĞ»Ğ¸ĞºĞ¸Ğ¹', 'meaning': 'å·¨äºº', 'evaluation': 'augmentative_awe'},
        'Ğ°Ñ‚Ğ°Ğ¼Ğ°Ğ½': {'suffix': '-Ğ°Ğ½', 'base': 'Ğ°Ñ‚Ğ°', 'meaning': 'é¦–é¢†', 'evaluation': 'augmentative_respect'},
        'ĞºĞ°Ğ¿Ğ¸Ñ‚Ğ°Ğ½': {'suffix': '-Ğ°Ğ½', 'base': 'ĞºĞ°Ğ¿Ğ¸Ñ‚', 'meaning': 'èˆ¹é•¿', 'evaluation': 'augmentative_respect'},
        
        # -Ğ¸Ñ‰Ğµ åç¼€
        'Ğ´Ğ¾Ğ¼Ğ¸Ñ‰Ğµ': {'suffix': '-Ğ¸Ñ‰Ğµ', 'base': 'Ğ´Ğ¾Ğ¼', 'meaning': 'å¤§æˆ¿å­', 'evaluation': 'augmentative_impressive'},
        'Ñ€ÑƒÑ‡Ğ¸Ñ‰Ğ°': {'suffix': '-Ğ¸Ñ‰Ğ°', 'base': 'Ñ€ÑƒĞºĞ°', 'meaning': 'å¤§æ‰‹', 'evaluation': 'augmentative_impressive'},
        'Ğ½Ğ¾ÑĞ¸Ñ‰Ğµ': {'suffix': '-Ğ¸Ñ‰Ğµ', 'base': 'Ğ½Ğ¾Ñ', 'meaning': 'å¤§é¼»å­', 'evaluation': 'augmentative_impressive'},
        'Ğ³Ğ¾Ğ»Ğ¾ÑĞ¸Ñ‰Ğµ': {'suffix': '-Ğ¸Ñ‰Ğµ', 'base': 'Ğ³Ğ¾Ğ»Ğ¾Ñ', 'meaning': 'å¤§å—“é—¨', 'evaluation': 'augmentative_impressive'},
        'ĞºĞ¾Ñ‚Ğ¸Ñ‰Ğµ': {'suffix': '-Ğ¸Ñ‰Ğµ', 'base': 'ĞºĞ¾Ñ‚', 'meaning': 'å¤§çŒ«', 'evaluation': 'augmentative_impressive'},
        'ÑĞ¾Ğ±Ğ°Ñ‡Ğ¸Ñ‰Ğµ': {'suffix': '-Ğ¸Ñ‰Ğµ', 'base': 'ÑĞ¾Ğ±Ğ°ĞºĞ°', 'meaning': 'å¤§ç‹—', 'evaluation': 'augmentative_impressive'},
        
        # -ÑĞº åç¼€
        'Ñ‚Ğ¾Ğ»ÑÑ‚ÑĞº': {'suffix': '-ÑĞº', 'base': 'Ñ‚Ğ¾Ğ»ÑÑ‚Ñ‹Ğ¹', 'meaning': 'èƒ–å­', 'evaluation': 'augmentative_characteristic'},
        'Ğ·Ğ´Ğ¾Ñ€Ğ¾Ğ²ÑĞº': {'suffix': '-ÑĞº', 'base': 'Ğ·Ğ´Ğ¾Ñ€Ğ¾Ğ²Ñ‹Ğ¹', 'meaning': 'å£®æ±‰', 'evaluation': 'augmentative_positive'},
        'Ğ´Ğ¾Ğ±Ñ€ÑĞº': {'suffix': '-ÑĞº', 'base': 'Ğ´Ğ¾Ğ±Ñ€Ñ‹Ğ¹', 'meaning': 'å¥½äºº', 'evaluation': 'augmentative_positive'},
        'Ğ±ĞµĞ´Ğ½ÑĞº': {'suffix': '-ÑĞº', 'base': 'Ğ±ĞµĞ´Ğ½Ñ‹Ğ¹', 'meaning': 'ç©·äºº', 'evaluation': 'augmentative_sympathetic'},
        'Ğ¿Ñ€Ğ¾ÑÑ‚ÑĞº': {'suffix': '-ÑĞº', 'base': 'Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ğ¹', 'meaning': 'è€å®äºº', 'evaluation': 'augmentative_neutral'},
        
        # -Ñ‹Ñˆ åç¼€
        'ĞºÑ€ĞµĞ¿Ñ‹Ñˆ': {'suffix': '-Ñ‹Ñˆ', 'base': 'ĞºÑ€ĞµĞ¿ĞºĞ¸Ğ¹', 'meaning': 'ç»“å®çš„äºº', 'evaluation': 'augmentative_positive'},
        'Ğ¼Ğ°Ğ»Ñ‹Ñˆ': {'suffix': '-Ñ‹Ñˆ', 'base': 'Ğ¼Ğ°Ğ»Ñ‹Ğ¹', 'meaning': 'å°å®¶ä¼™', 'evaluation': 'diminutive_affectionate'},
        'Ğ³Ğ¾Ğ»Ñ‹Ñˆ': {'suffix': '-Ñ‹Ñˆ', 'base': 'Ğ³Ğ¾Ğ»Ñ‹Ğ¹', 'meaning': 'å…‰èº«å­', 'evaluation': 'augmentative_neutral'},
        
        # -Ğ°Ñ‡ åç¼€
        'ÑĞ¸Ğ»Ğ°Ñ‡': {'suffix': '-Ğ°Ñ‡', 'base': 'ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğ¹', 'meaning': 'å¤§åŠ›å£«', 'evaluation': 'augmentative_admiration'},
        'Ğ±Ğ¾Ğ³Ğ°Ñ‡': {'suffix': '-Ğ°Ñ‡', 'base': 'Ğ±Ğ¾Ğ³Ğ°Ñ‚Ñ‹Ğ¹', 'meaning': 'å¯Œç¿', 'evaluation': 'augmentative_neutral'},
        'Ğ»Ğ¾Ğ²ĞºĞ°Ñ‡': {'suffix': '-Ğ°Ñ‡', 'base': 'Ğ»Ğ¾Ğ²ĞºĞ¸Ğ¹', 'meaning': 'çµå·§çš„äºº', 'evaluation': 'augmentative_positive'},
        'ÑƒÑĞ°Ñ‡': {'suffix': '-Ğ°Ñ‡', 'base': 'ÑƒÑ', 'meaning': 'å¤§èƒ¡å­', 'evaluation': 'augmentative_characteristic'},
        
        # -Ñ‹Ñ€ÑŒ åç¼€
        'Ğ±Ğ¾Ğ³Ğ°Ñ‚Ñ‹Ñ€ÑŒ': {'suffix': '-Ñ‹Ñ€ÑŒ', 'base': 'Ğ±Ğ¾Ğ³Ğ°Ñ‚Ñ‹Ğ¹', 'meaning': 'å‹‡å£«', 'evaluation': 'augmentative_heroic'},
        'Ğ¿ÑƒÑÑ‚Ñ‹Ñ€ÑŒ': {'suffix': '-Ñ‹Ñ€ÑŒ', 'base': 'Ğ¿ÑƒÑÑ‚Ğ¾Ğ¹', 'meaning': 'è’åœ°', 'evaluation': 'augmentative_neutral'},
        
        # å¤æ•°å½¢å¼
        'Ğ´Ğ¾Ğ¼Ğ¸ĞºĞ¸': {'suffix': '-Ğ¸Ğº', 'base': 'Ğ´Ğ¾Ğ¼', 'meaning': 'å°æˆ¿å­ä»¬', 'evaluation': 'diminutive_affectionate'},
        'ĞºĞ¾Ñ‚Ğ¸ĞºĞ¸': {'suffix': '-Ğ¸Ğº', 'base': 'ĞºĞ¾Ñ‚', 'meaning': 'å°çŒ«å’ªä»¬', 'evaluation': 'diminutive_affectionate'},
        'Ñ†Ğ²ĞµÑ‚Ğ¸ĞºĞ¸': {'suffix': '-Ğ¸Ğº', 'base': 'Ñ†Ğ²ĞµÑ‚', 'meaning': 'å°èŠ±æœµä»¬', 'evaluation': 'diminutive_affectionate'},
        'Ğ¾Ğ³Ğ¾Ğ½ÑŒĞºĞ¸': {'suffix': '-ĞºĞ¸', 'base': 'Ğ¾Ğ³Ğ¾Ğ½ÑŒ', 'meaning': 'å°ç«å…‰ä»¬', 'evaluation': 'diminutive_affectionate'},
        'Ñ†Ğ²ĞµÑ‚Ğ¾Ñ‡ĞºĞ¸': {'suffix': '-ĞºĞ¸', 'base': 'Ñ†Ğ²ĞµÑ‚Ğ¾Ğº', 'meaning': 'å°èŠ±æœµä»¬', 'evaluation': 'diminutive_affectionate'},
        'Ğ´Ğ¾Ñ€Ğ¾Ğ¶ĞºĞ¸': {'suffix': '-ĞºĞ¸', 'base': 'Ğ´Ğ¾Ñ€Ğ¾Ğ³Ğ°', 'meaning': 'å°è·¯ä»¬', 'evaluation': 'diminutive_affectionate'},
        'Ñ€ÑƒÑ‡ĞºĞ¸': {'suffix': '-ĞºĞ¸', 'base': 'Ñ€ÑƒĞºĞ°', 'meaning': 'å°æ‰‹ä»¬', 'evaluation': 'diminutive_affectionate'},
        'Ğ·Ğ²Ñ‘Ğ·Ğ´Ğ¾Ñ‡ĞºĞ¸': {'suffix': '-ĞºĞ¸', 'base': 'Ğ·Ğ²ĞµĞ·Ğ´Ğ°', 'meaning': 'å°æ˜Ÿæ˜Ÿä»¬', 'evaluation': 'diminutive_affectionate'},
        'Ğ²ĞµÑ‚Ğ¾Ñ‡ĞºĞ¸': {'suffix': '-ĞºĞ¸', 'base': 'Ğ²ĞµÑ‚ĞºĞ°', 'meaning': 'å°æ ‘æä»¬', 'evaluation': 'diminutive_affectionate'},
    }


def _eval_get_excluded_words():
    """æ’é™¤è¯åˆ—è¡¨"""
    return {
        # ç”Ÿé€ è¯
        'Ğ²ĞµĞ»Ğ¸ĞºĞ°Ğ½Ğ¾Ğº', 'Ğ¼ÑƒĞ´Ñ€ĞµÑ†Ğ¾Ğº', 'Ğ²Ñ‹ÑĞ¾Ñ‚Ğ¸Ñ‰Ğ°', 'Ğ³Ğ¾Ñ€Ğ¸Ñ‰Ğ°', 'Ñ‚Ğ°Ğ¹Ğ½Ğ¸Ñ‰Ğ°', 'Ğ¼ĞµÑ‡Ñ‚Ğ¸Ñ‰Ğ°',
        'Ğ·Ğ²ĞµĞ·Ğ´Ğ¸Ñ‰Ğµ', 'Ğ»ĞµĞ³ĞµĞ½Ğ´Ğ¸Ñ‰Ğµ', 'Ğ·ĞµĞ»ĞµĞ½Ğ¸Ñ‰Ğµ', 'Ğ¼ÑƒĞ´Ñ€Ğ¸Ñ‰Ğµ', 'Ğ³Ğ¸Ğ³Ğ°Ğ½Ñ‚Ğ¸Ñ‰Ğµ',
        'Ğ³Ğ¾Ñ€ÑƒĞ½', 'Ğ²ĞµĞ»Ğ¸Ñ‡Ğ°Ğ²Ğ¸Ñ†Ğ°', 'Ğ¼ĞµÑ‡Ñ‚Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¸Ñ†Ğ°', 'ÑĞºĞ°Ğ·Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¸', 'ĞºĞ°Ğ¼ĞµĞ½Ğ¸ÑÑ‚Ğ¸Ğº',
        
        # æ™®é€šåè¯
        'ÑƒÑ€Ğ¾Ğº', 'ÑƒÑ€Ğ¾ĞºĞ¸', 'Ğ·Ğ²ÑƒĞº', 'Ğ·Ğ²ÑƒĞºĞ¸', 'ÑĞ·Ñ‹Ğº', 'ÑĞ·Ñ‹ĞºĞ¸', 'ÑƒÑ‡ĞµĞ½Ğ¸Ğº', 'ÑƒÑ‡ĞµĞ½Ğ¸ĞºĞ¸',
        'Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸Ğº', 'Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¸', 'Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞº', 'Ğ»ÑĞ´Ğ¸', 'Ğ¼Ğ°ÑĞº', 'Ğ¼Ğ°ÑĞºĞ¸',
        'Ñ€ÑƒĞ±Ğ¸Ğ½', 'Ñ€ÑƒĞ±Ğ¸Ğ½Ñ‹', 'ÑÑ‚Ğ°Ñ€Ğ¸Ğº', 'ÑÑ‚Ğ°Ñ€Ğ¸ĞºĞ¸', 'Ğ¿Ğ¾Ñ‚Ğ¾Ğº', 'Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¸',
        'Ğ¸ÑÑ‚Ğ¾Ğº', 'Ğ¸ÑÑ‚Ğ¾ĞºĞ¸', 'Ñ‡ĞµÑ€Ñ‚Ğ¾Ğ³', 'Ñ‡ĞµÑ€Ñ‚Ğ¾Ğ³Ğ¸',
        
        # å·¥å…·
        'ÑĞ²ĞµÑ‚Ğ¸Ğ»ÑŒĞ½Ğ¸Ğº', 'Ğ±ÑƒĞ´Ğ¸Ğ»ÑŒĞ½Ğ¸Ğº', 'Ñ…Ğ¾Ğ»Ğ¾Ğ´Ğ¸Ğ»ÑŒĞ½Ğ¸Ğº', 'Ğ¿Ğ°ÑĞ»ÑŒĞ½Ğ¸Ğº', 'Ñ€ÑƒĞ±Ğ¸Ğ»ÑŒĞ½Ğ¸Ğº',
        'Ğ²Ñ‹ĞºĞ»ÑÑ‡Ğ°Ñ‚ĞµĞ»ÑŒ', 'ÑƒĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒ', 'Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒ', 'Ğ¸Ğ·Ğ¼ĞµÑ€Ğ¸Ñ‚ĞµĞ»ÑŒ', 'Ğ¾Ñ‡Ğ¸ÑÑ‚Ğ¸Ñ‚ĞµĞ»ÑŒ',
        
        # æŠ½è±¡æ¦‚å¿µ
        'ĞºÑ€Ğ°ÑĞ¾Ñ‚Ğ°', 'Ğ´Ğ¾Ğ±Ñ€Ğ¾Ñ‚Ğ°', 'Ğ¼ÑƒĞ´Ñ€Ğ¾ÑÑ‚ÑŒ', 'Ğ²ĞµĞ»Ğ¸Ñ‡Ğ¸Ğµ', 'Ğ±Ğ¾Ğ³Ğ°Ñ‚ÑÑ‚Ğ²Ğ¾', 'Ğ±ĞµĞ´Ğ½Ğ¾ÑÑ‚ÑŒ',
        'ÑÑ‚Ğ°Ñ€Ğ¸Ğ½Ğ°', 'ÑÑ‚Ğ°Ñ€Ğ¸Ğ½Ñ‹',
        
        # åœ°ç†å’Œç©ºé—´
        'Ğ²ĞµÑ€ÑˆĞ¸Ğ½Ğ°', 'Ğ²ĞµÑ€ÑˆĞ¸Ğ½Ñ‹', 'Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ğ°', 'Ğ²Ñ‹ÑĞ¾Ñ‚Ğ°', 'ÑˆĞ¸Ñ€Ğ¸Ğ½Ğ°', 'Ğ´Ğ»Ğ¸Ğ½Ğ°',
        
        # æ—¶é—´
        'Ğ²Ñ€ĞµĞ¼Ñ', 'Ğ¼ĞµÑÑ‚Ğ¾', 'Ğ´ĞµĞ»Ğ¾', 'ÑĞ»Ğ¾Ğ²Ğ¾', 'Ñ‡Ğ¸ÑĞ»Ğ¾', 'ÑÑƒÑ‚ÑŒ',
    }


def _eval_validate_context(word, info, text):
    """è¯­å¢ƒéªŒè¯"""
    evaluation = info.get('evaluation', '')
    context = text.lower()
    
    # æŒ‡å°çˆ±ç§°
    if 'diminutive_affectionate' in evaluation:
        positive_keywords = [
            'ĞºÑ€Ğ°ÑĞ¸Ğ²', 'Ğ¼Ğ¸Ğ»', 'Ğ´Ğ¾Ğ±Ñ€', 'Ğ½ĞµĞ¶Ğ½', 'Ğ»Ğ°ÑĞºĞ¾Ğ²', 'ÑƒÑÑ‚', 'Ñ‚ĞµĞ¿Ğ»', 'ÑĞ²ĞµÑ‚',
            'Ğ»ÑĞ±Ğ¾Ğ²', 'Ğ´Ğ¾Ñ€Ğ¾Ğ³', 'Ğ¿Ñ€ĞµĞºÑ€Ğ°ÑĞ½', 'Ñ‡ÑƒĞ´ĞµÑĞ½', 'Ğ²Ğ¾Ğ»ÑˆĞµĞ±Ğ½', 'ÑĞºĞ°Ğ·Ğ¾Ñ‡Ğ½',
            'Ğ·ĞµĞ»Ñ‘Ğ½', 'ÑĞ²ĞµÑ‚Ğ»', 'Ğ¿ÑƒÑ‚ĞµĞ²Ğ¾Ğ´Ğ½', 'Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½'
        ]
        
        pos = context.find(word)
        if pos != -1:
            window = context[max(0, pos-100):pos+len(word)+100]
            if any(kw in window for kw in positive_keywords):
                return True
    
    # å¢å¤§åç¼€
    elif 'augmentative' in evaluation:
        impressive_keywords = [
            'Ğ²ĞµĞ»Ğ¸Ñ‡', 'Ğ¼Ğ¾Ñ‰', 'ÑĞ¸Ğ»', 'Ğ¾Ğ³Ñ€Ğ¾Ğ¼Ğ½', 'Ğ³Ğ¸Ğ³Ğ°Ğ½Ñ‚', 'ĞºĞ¾Ğ»Ğ¾ÑÑ', 'Ğ³Ñ€Ğ¾Ğ·Ğ½',
            'Ğ¼Ğ¾Ğ³ÑƒÑ‡', 'Ğ²Ğ½ÑƒÑˆĞ¸Ñ‚ĞµĞ»ÑŒĞ½', 'Ğ²Ğ¿ĞµÑ‡Ğ°Ñ‚Ğ»ÑÑÑ‰', 'Ğ¿Ğ¾Ñ€Ğ°Ğ·Ğ¸Ñ‚ĞµĞ»ÑŒĞ½', 'Ğ³Ğ¾Ñ€Ğ´Ğ¾ÑÑ‚',
            'ĞºÑ€Ğ°ÑĞ¾Ñ‚', 'Ğ½ĞµĞ±ĞµÑĞ½', 'Ğ²ĞµÑ‡Ğ½'
        ]
        
        pos = context.find(word)
        if pos != -1:
            window = context[max(0, pos-100):pos+len(word)+100]
            if any(kw in window for kw in impressive_keywords):
                return True
    
    # é«˜ç½®ä¿¡åº¦è¯
    high_confidence = {
        'Ğ¸ÑĞ¿Ğ¾Ğ»Ğ¸Ğ½', 'Ğ²ĞµĞ»Ğ¸ĞºĞ°Ğ½', 'ĞºĞ°Ğ¼ĞµÑˆĞµĞº', 'Ğ»ĞµÑĞ¾Ğº', 'Ğ´Ğ¾Ğ¼Ğ¸Ğº', 'ĞºĞ¾Ñ‚Ğ¸Ğº', 'Ğ¿ĞµÑĞ¸Ğº',
        'Ğ´Ğ²Ğ¾Ñ€Ğ¸Ğº', 'ÑĞ°Ğ´Ğ¸Ğº', 'Ñ†Ğ²ĞµÑ‚Ğ¾Ñ‡ĞµĞº', 'Ğ»Ğ¸ÑÑ‚Ğ¾Ñ‡ĞµĞº', 'Ñ€ÑƒÑ‡ĞµÑ‘Ğº', 'Ğ¾Ğ³Ğ¾Ğ½Ñ‘Ğº', 'ÑƒĞ³Ğ¾Ğ»Ğ¾Ğº',
        'Ğ´ĞµĞ½Ñ‘Ğº', 'Ğ³Ğ¾Ğ»Ğ¾ÑĞ¾Ğº', 'ÑÑ‹Ğ½Ğ¾Ğº', 'Ğ´Ñ€ÑƒĞ¶Ğ¾Ğº', 'Ğ²ĞµÑ‚ĞµÑ€Ğ¾Ğº', 'ÑĞ½ĞµĞ¶Ğ¾Ğº', 'ÑÑ‚Ğ¸ÑˆĞ¾Ğº'
    }
    
    if word in high_confidence:
        return True
    
    return True

# ==================== è§„åˆ™ 3: ç¬¬å››æ ¼æ—¶é—´è¡¨è¾¾æ£€æµ‹ ====================
def detect_russian_time_expression_4th_case(content_list, required_count):
    """
    æ£€æµ‹ä¿„è¯­ç¬¬å››æ ¼æ—¶é—´è¡¨è¾¾æ ¼å¼
    
    Args:
        content_list: æ–‡æœ¬å†…å®¹åˆ—è¡¨
        required_count: è¦æ±‚çš„æ—¶é—´è¡¨è¾¾æ•°é‡
    
    Returns:
        tuple: (1/0, è¯´æ˜ä¿¡æ¯)
    """
    if content_list == "INVALID" or content_list is None:
        return 0, "âŒ è¾“å…¥æ–‡æœ¬æ— æ•ˆ"
    
    try:
        required_count = int(required_count)
    except (ValueError, TypeError):
        return 0, f"âŒ required_count å¿…é¡»æ˜¯æ•´æ•°: '{required_count}'"

    try:
        if isinstance(content_list, list):
            text = ' '.join(str(item) for item in content_list if item and str(item) != "INVALID")
        else:
            text = str(content_list)
        
        if not text.strip():
            if required_count == 0:
                return 1, "âœ… å†…å®¹ä¸ºç©ºï¼Œç¬¦åˆè¦æ±‚ 0 ä¸ªæ—¶é—´è¡¨è¾¾"
            else:
                return 0, "âŒ å†…å®¹ä¸ºç©ºï¼Œæ— æ³•æ£€æµ‹"
        
        patterns_4th = [
            # åŸºç¡€æ—¶é—´ç‚¹è¡¨è¾¾
            (r'[Ğ’Ğ²]\s+(?:[Ğ°-ÑÑ‘]+|\d+)\s+Ñ‡Ğ°Ñ(?:Ğ°|Ğ¾Ğ²)?\b(?:\s+(?:ÑƒÑ‚Ñ€Ğ°|Ğ´Ğ½Ñ|Ğ²ĞµÑ‡ĞµÑ€Ğ°|Ğ½Ğ¾Ñ‡Ğ¸))?', False),
            (r'[Ğ’Ğ²]\s+\d+[:.]\d+(?:\s+(?:ÑƒÑ‚Ñ€Ğ°|Ğ´Ğ½Ñ|Ğ²ĞµÑ‡ĞµÑ€Ğ°|Ğ½Ğ¾Ñ‡Ğ¸))?', False),
            
            # æ˜ŸæœŸè¡¨è¾¾
            (r'[Ğ’Ğ²]\s+(?:(?:ÑĞ»ĞµĞ´ÑƒÑÑ‰(?:Ğ¸Ğ¹|ÑƒÑ)|Ğ¿Ñ€Ğ¾ÑˆĞ»(?:Ñ‹Ğ¹|ÑƒÑ)|ÑÑ‚(?:Ğ¾Ñ‚|Ñƒ))\s+)?(?:Ğ¿Ğ¾Ğ½ĞµĞ´ĞµĞ»ÑŒĞ½Ğ¸Ğº|Ğ²Ñ‚Ğ¾Ñ€Ğ½Ğ¸Ğº|ÑÑ€ĞµĞ´Ñƒ|Ñ‡ĞµÑ‚Ğ²ĞµÑ€Ğ³|Ğ¿ÑÑ‚Ğ½Ğ¸Ñ†Ñƒ|ÑÑƒĞ±Ğ±Ğ¾Ñ‚Ñƒ|Ğ²Ğ¾ÑĞºÑ€ĞµÑĞµĞ½ÑŒĞµ)\b', False),
            
            # ğŸ”¥ æ–°å¢ï¼šĞ² Ñ‚Ğ¾ (Ğ¶Ğµ) Ğ²Ñ€ĞµĞ¼Ñ å›ºå®šçŸ­è¯­
            (r'[Ğ’Ğ²]\s+Ñ‚Ğ¾\s+(?:Ğ¶Ğµ\s+)?Ğ²Ñ€ĞµĞ¼[ÑÑ]', False),
            
            # Ğ² + å½¢å®¹è¯ + Ğ²Ñ€ĞµĞ¼Ñ ç»“æ„
            (r'[Ğ’Ğ²]\s+[Ğ°-ÑÑ‘Ğ-Ğ¯Ğ]+(?:Ğ¾Ğµ|ĞµĞµ)\s+Ğ²Ñ€ĞµĞ¼[ÑÑ](?:\s+Ğ³Ğ¾Ğ´Ğ°)?', True),
            (r'[Ğ’Ğ²]\s+(?:ÑÑ‚Ğ¾|Ğ²ÑÑ‘|ĞºĞ°ĞºĞ¾Ğµ|Ğ»ÑĞ±Ğ¾Ğµ|ĞºĞ°Ğ¶Ğ´Ğ¾Ğµ|ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞµ|Ğ¿Ñ€Ğ¾ÑˆĞ»Ğ¾Ğµ|Ğ±ÑƒĞ´ÑƒÑ‰ĞµĞµ|Ñ…Ğ¾Ğ»Ğ¾Ğ´Ğ½Ğ¾Ğµ|Ñ‚ĞµĞ¿Ğ»Ğ¾Ğµ|Ğ¶Ğ°Ñ€ĞºĞ¾Ğµ|Ğ·Ğ¸Ğ¼Ğ½ĞµĞµ|Ğ»ĞµÑ‚Ğ½ĞµĞµ|Ğ²ĞµÑĞµĞ½Ğ½ĞµĞµ|Ğ¾ÑĞµĞ½Ğ½ĞµĞµ|Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¾Ğµ|Ñ‚ÑĞ¶ĞµĞ»Ğ¾Ğµ|ÑĞ²Ğ¾Ğ±Ğ¾Ğ´Ğ½Ğ¾Ğµ|Ñ€Ğ°Ğ±Ğ¾Ñ‡ĞµĞµ|Ñ‚Ğ°ĞºĞ¾Ğµ|Ğ´Ñ€ÑƒĞ³Ğ¾Ğµ)\s+Ğ²Ñ€ĞµĞ¼[ÑÑ](?:\s+Ğ³Ğ¾Ğ´Ğ°)?', False),
            
            # å½¢å®¹è¯ + æ—¶é—´åè¯ï¼ˆå•æ•°ï¼‰
            # ä¿®å¤ï¼šå¢åŠ äº† (?:ÑÑ‚Ğ¾Ñ‚|Ñ‚Ğ¾Ñ‚|Ğ²ĞµÑÑŒ|Ğ½Ğ°Ñˆ|Ğ²Ğ°Ñˆ|ÑĞ²Ğ¾Ğ¹) æ¥åŒ¹é…ä»£è¯
            (r'[Ğ’Ğ²]\s+(?:[Ğ°-ÑÑ‘Ğ-Ğ¯Ğ]+(?:Ñ‹Ğ¹|Ğ¸Ğ¹|Ğ¾Ğ¹)|ÑÑ‚Ğ¾Ñ‚|Ñ‚Ğ¾Ñ‚|Ğ²ĞµÑÑŒ|Ğ½Ğ°Ñˆ|Ğ²Ğ°Ñˆ|ÑĞ²Ğ¾Ğ¹)\s+(?:Ğ´ĞµĞ½ÑŒ|Ğ¼Ğ¾Ğ¼ĞµĞ½Ñ‚|Ñ‡Ğ°Ñ|Ğ¿ĞµÑ€Ğ¸Ğ¾Ğ´|Ñ€Ğ°Ğ·)\b', True),
            (r'[Ğ’Ğ²]\s+[Ğ°-ÑÑ‘Ğ-Ğ¯Ğ]+ÑƒÑ\s+(?:Ğ½Ğ¾Ñ‡ÑŒ|Ğ½ĞµĞ´ĞµĞ»Ñ|Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñƒ|ÑĞµĞºÑƒĞ½Ğ´Ñƒ|Ğ¿Ğ¾Ğ³Ğ¾Ğ´Ñƒ|Ğ¿Ğ¾Ñ€Ñƒ|Ğ·Ğ¸Ğ¼Ñƒ|Ğ²ĞµÑĞ½Ñƒ|Ğ¾ÑĞµĞ½ÑŒ)\b', True),
            
            # ğŸ”¥ ä¿®å¤ï¼šå½¢å®¹è¯ + æ—¶é—´åè¯ï¼ˆå¤æ•°ï¼‰ - æ”¯æŒå¤šä¸ªå½¢å®¹è¯
            (r'[Ğ’Ğ²]\s+(?:[Ğ°-ÑÑ‘Ğ-Ğ¯Ğ]+(?:Ñ‹Ğµ|Ğ¸Ğµ)\s+){1,3}(?:Ğ´Ğ½Ğ¸|Ğ½Ğ¾Ñ‡Ğ¸|Ñ‡Ğ°ÑÑ‹|Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñ‹|ÑĞµĞºÑƒĞ½Ğ´Ñ‹|Ğ¼Ğ¾Ğ¼ĞµĞ½Ñ‚Ñ‹|Ğ½ĞµĞ´ĞµĞ»Ğ¸|Ğ¼ĞµÑÑÑ†Ñ‹|Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ°)\b', True),
            (r'[Ğ’Ğ²]\s+(?:[Ğ°-ÑÑ‘Ğ-Ğ¯Ğ]+(?:Ñ‹Ğµ|Ğ¸Ğµ)\s+)+(?:Ğ¸|Ğ¸Ğ»Ğ¸)\s+[Ğ°-ÑÑ‘Ğ-Ğ¯Ğ]+(?:Ñ‹Ğµ|Ğ¸Ğµ)\s+(?:Ğ´Ğ½Ğ¸|Ğ½Ğ¾Ñ‡Ğ¸|Ñ‡Ğ°ÑÑ‹|Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñ‹|ÑĞµĞºÑƒĞ½Ğ´Ñ‹|Ğ¼Ğ¾Ğ¼ĞµĞ½Ñ‚Ñ‹|Ğ½ĞµĞ´ĞµĞ»Ğ¸|Ğ¼ĞµÑÑÑ†Ñ‹|Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ°)\b', True),
            
            # å…¶ä»–æ—¶é—´è¡¨è¾¾
            (r'[Ğ’Ğ²]\s+(?:Ğ´ĞµĞ½ÑŒ|Ğ²ĞµÑ‡ĞµÑ€|Ğ½Ğ¾Ñ‡ÑŒ|ÑƒÑ‚Ñ€Ğ¾)\s+[Ğ°-ÑÑ‘Ğ-Ğ¯Ğ]+', False),
            
            # Ğ½Ğ° + æ—¶é—´æ®µ
            (r'[ĞĞ½]Ğ°\s+(?:Ğ½ĞµĞ´ĞµĞ»Ñ|Ğ´ĞµĞ½ÑŒ|Ğ¼ĞµÑÑÑ†|Ğ³Ğ¾Ğ´|Ñ‡Ğ°Ñ|Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñƒ|ÑĞµĞºÑƒĞ½Ğ´Ñƒ|Ğ¼Ğ³Ğ½Ğ¾Ğ²ĞµĞ½Ğ¸Ğµ)\b', False),
            (r'[ĞĞ½]Ğ°\s+(?:[Ğ°-ÑÑ‘]+|\d+)\s+(?:Ğ½ĞµĞ´ĞµĞ»[Ğ¸ÑÑŒ]|Ğ´Ğ½[ÑĞµĞ¹]|Ğ¼ĞµÑÑÑ†[Ğ°Ğµ]?|Ğ³Ğ¾Ğ´[Ğ°]?|Ğ»ĞµÑ‚|Ñ‡Ğ°Ñ[Ğ°Ğ¾Ğ²]?|Ğ¼Ğ¸Ğ½ÑƒÑ‚[ÑƒÑ‹]?)\b', False),
            (r'[ĞĞ½]Ğ°\s+(?:Ğ¿ĞµÑ€Ğ²(?:Ñ‹Ğ¹|ÑƒÑ)|Ğ²Ñ‚Ğ¾Ñ€(?:Ğ¾Ğ¹|ÑƒÑ)|Ñ‚Ñ€ĞµÑ‚(?:Ğ¸Ğ¹|ÑŒÑ)|Ñ‡ĞµÑ‚Ğ²[Ñ‘Ğµ]Ñ€Ñ‚(?:Ñ‹Ğ¹|ÑƒÑ)|Ğ¿ÑÑ‚(?:Ñ‹Ğ¹|ÑƒÑ)|ÑˆĞµÑÑ‚(?:Ğ¾Ğ¹|ÑƒÑ)|ÑĞµĞ´ÑŒĞ¼(?:Ğ¾Ğ¹|ÑƒÑ)|Ğ²Ğ¾ÑÑŒĞ¼(?:Ğ¾Ğ¹|ÑƒÑ)|Ğ´ĞµĞ²ÑÑ‚(?:Ñ‹Ğ¹|ÑƒÑ)|Ğ´ĞµÑÑÑ‚(?:Ñ‹Ğ¹|ÑƒÑ))\s+(?:Ğ´ĞµĞ½ÑŒ|Ğ½ĞµĞ´ĞµĞ»Ñ|Ğ¼ĞµÑÑÑ†|Ğ³Ğ¾Ğ´)\b', False),
            
            # Ğ¿Ğ¾ + æ—¶é—´ï¼ˆğŸ”¥ æ·»åŠ å¹´ä»½æ”¯æŒï¼‰
            (r'[ĞŸĞ¿]Ğ¾\s+(?:ÑĞ½Ğ²Ğ°Ñ€ÑŒ|Ñ„ĞµĞ²Ñ€Ğ°Ğ»ÑŒ|Ğ¼Ğ°Ñ€Ñ‚|Ğ°Ğ¿Ñ€ĞµĞ»ÑŒ|Ğ¼Ğ°Ğ¹|Ğ¸ÑĞ½ÑŒ|Ğ¸ÑĞ»ÑŒ|Ğ°Ğ²Ğ³ÑƒÑÑ‚|ÑĞµĞ½Ñ‚ÑĞ±Ñ€ÑŒ|Ğ¾ĞºÑ‚ÑĞ±Ñ€ÑŒ|Ğ½Ğ¾ÑĞ±Ñ€ÑŒ|Ğ´ĞµĞºĞ°Ğ±Ñ€ÑŒ)(?:\s+\d+)?(?:\s+Ğ³Ğ¾Ğ´Ğ°)?\b', False),
            (r'[ĞŸĞ¿]Ğ¾\s+(?:Ğ¿Ğ¾Ğ½ĞµĞ´ĞµĞ»ÑŒĞ½Ğ¸Ğº|Ğ²Ñ‚Ğ¾Ñ€Ğ½Ğ¸Ğº|ÑÑ€ĞµĞ´Ñƒ|Ñ‡ĞµÑ‚Ğ²ĞµÑ€Ğ³|Ğ¿ÑÑ‚Ğ½Ğ¸Ñ†Ñƒ|ÑÑƒĞ±Ğ±Ğ¾Ñ‚Ñƒ|Ğ²Ğ¾ÑĞºÑ€ĞµÑĞµĞ½ÑŒĞµ)\b', False),
            (r'[ĞŸĞ¿]Ğ¾\s+(?:ÑƒÑ‚Ñ€Ğ¾|Ğ´ĞµĞ½ÑŒ|Ğ²ĞµÑ‡ĞµÑ€|Ğ½Ğ¾Ñ‡ÑŒ)\b', False),
            
            # Ñ‡ĞµÑ€ĞµĞ· + æ—¶é—´æ®µ
            (r'[Ğ§Ñ‡]ĞµÑ€ĞµĞ·\s+(?:Ğ½ĞµĞ´ĞµĞ»Ñ|Ğ´ĞµĞ½ÑŒ|Ğ¼ĞµÑÑÑ†|Ğ³Ğ¾Ğ´|Ñ‡Ğ°Ñ|Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñƒ|ÑĞµĞºÑƒĞ½Ğ´Ñƒ|Ğ¼Ğ³Ğ½Ğ¾Ğ²ĞµĞ½Ğ¸Ğµ)\b', False),
            (r'[Ğ§Ñ‡]ĞµÑ€ĞµĞ·\s+(?:(?:Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾|Ğ¿Ğ°Ñ€Ñƒ)\s+)?(?:[Ğ°-ÑÑ‘]+|\d+)\s+(?:Ğ½ĞµĞ´ĞµĞ»[Ğ¸ÑÑŒ]|Ğ´Ğ½[ÑĞµĞ¹]|Ğ¼ĞµÑÑÑ†[Ğ°Ğµ]?|Ğ³Ğ¾Ğ´[Ğ°]?|Ğ»ĞµÑ‚|Ñ‡Ğ°Ñ[Ğ°Ğ¾Ğ²]?|Ğ¼Ğ¸Ğ½ÑƒÑ‚[ÑƒÑ‹]?)\b', False),
            
            # Ğ·Ğ° + æ—¶é—´æ®µ
            (r'[Ğ—Ğ·]Ğ°\s+(?:Ğ½ĞµĞ´ĞµĞ»Ñ|Ğ´ĞµĞ½ÑŒ|Ğ¼ĞµÑÑÑ†|Ğ³Ğ¾Ğ´|Ñ‡Ğ°Ñ|Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñƒ|ÑĞµĞºÑƒĞ½Ğ´Ñƒ)\b', False),
            (r'[Ğ—Ğ·]Ğ°\s+(?:(?:Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾|Ğ¿Ğ°Ñ€Ñƒ)\s+)?(?:[Ğ°-ÑÑ‘]+|\d+)\s+(?:Ğ½ĞµĞ´ĞµĞ»[Ğ¸ÑÑŒ]|Ğ´Ğ½[ÑĞµĞ¹]|Ğ¼ĞµÑÑÑ†[Ğ°Ğµ]?|Ğ³Ğ¾Ğ´[Ğ°]?|Ğ»ĞµÑ‚|Ñ‡Ğ°Ñ[Ğ°Ğ¾Ğ²]?|Ğ¼Ğ¸Ğ½ÑƒÑ‚[ÑƒÑ‹]?)\b', False),
        ]
        
        found_expressions = []
        
        for pattern, needs_adj_validation in patterns_4th:
            for match in re.finditer(pattern, text, re.IGNORECASE):
                full_match = match.group(0).strip()
                
                if _time4_is_valid(full_match, needs_adj_validation):
                    found_expressions.append({
                        'text': full_match,
                        'position': match.start()
                    })
        
        unique = _time4_remove_overlapping(found_expressions)
        
        total_found = len(unique)
        found_display = [expr['text'] for expr in unique]
        
        if total_found == required_count:
            if required_count == 0:
                return 1, f"âœ… æ­£ç¡®ï¼šæœªæ‰¾åˆ°ç¬¬å››æ ¼æ—¶é—´è¡¨è¾¾ (è¦æ±‚=0ä¸ª)"
            else:
                return 1, f"âœ… æ‰¾åˆ° {total_found} ä¸ªç¬¬å››æ ¼æ—¶é—´è¡¨è¾¾ (è¦æ±‚={required_count}ä¸ª): {found_display}"
        else:
            if required_count == 0:
                return 0, f"âŒ æ‰¾åˆ° {total_found} ä¸ªç¬¬å››æ ¼æ—¶é—´è¡¨è¾¾ï¼Œä½†è¦æ±‚ 0 ä¸ª: {found_display}"
            else:
                return 0, f"âŒ æ‰¾åˆ° {total_found} ä¸ªç¬¬å››æ ¼æ—¶é—´è¡¨è¾¾ (è¦æ±‚={required_count}ä¸ª): {found_display}"

    except Exception as e:
        import traceback
        return 0, f"âŒ å‡½æ•°æ‰§è¡Œå¼‚å¸¸: {str(e)}\n{traceback.format_exc()}"


def _time4_is_valid(expression, needs_adj_validation):
    """éªŒè¯ç¬¬å››æ ¼è¡¨è¾¾å¼ï¼ˆä¿®å¤ç‰ˆ - æ”¯æŒ Ğ² + å½¢å®¹è¯ + Ğ²Ñ€ĞµĞ¼Ñï¼‰"""
    expr_lower = expression.lower()
    
    # ========== æ’é™¤è§„åˆ™ ==========
    
    # æ’é™¤ç¬¬å…­æ ¼ï¼ˆå¹´ä»½ï¼‰
    if 'Ğ³Ğ¾Ğ´Ñƒ' in expr_lower or 'Ğ³Ğ¾Ğ´Ğ°Ñ…' in expr_lower:
        return False
    if re.search(r'Ğ²\s+\d+\s+Ğ³Ğ¾Ğ´[Ğ°ÑƒĞµ]', expr_lower):
        return False
    if re.search(r'Ğ²\s+\d+-?(?:Ğ¾Ğ¼|Ğ¼)\s+Ğ²ĞµĞº[ĞµÑƒ]', expr_lower):
        return False
    
    # æ’é™¤æœˆä»½ç¬¬å…­æ ¼
    sixth_months = ['ÑĞ½Ğ²Ğ°Ñ€Ğµ', 'Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ğµ', 'Ğ¼Ğ°Ñ€Ñ‚Ğµ', 'Ğ°Ğ¿Ñ€ĞµĞ»Ğµ', 'Ğ¼Ğ°Ğµ', 'Ğ¸ÑĞ½Ğµ', 
                    'Ğ¸ÑĞ»Ğµ', 'Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğµ', 'ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ğµ', 'Ğ¾ĞºÑ‚ÑĞ±Ñ€Ğµ', 'Ğ½Ğ¾ÑĞ±Ñ€Ğµ', 'Ğ´ĞµĞºĞ°Ğ±Ñ€Ğµ']
    if any(m in expr_lower for m in sixth_months):
        return False
    
    # æ’é™¤å›ºå®šçŸ­è¯­ï¼ˆä½†ä¸æ’é™¤ "Ğ² Ñ‚Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ"ï¼‰
    invalid = ['Ğ² Ñ‚Ğ¾Ğ¼ Ñ‡Ğ¸ÑĞ»Ğµ', 'Ğ² ÑĞ²ÑĞ·Ğ¸', 'Ğ² ÑĞ»ÑƒÑ‡Ğ°Ğµ', 'Ğ² Ñ†ĞµĞ»Ğ¾Ğ¼', 'Ğ² Ğ¸Ñ‚Ğ¾Ğ³Ğµ',
               'Ğ² Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğµ', 'Ğ² Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ', 'Ğ² Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸', 'Ğ² ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğ¸',
               'Ğ½Ğ° ÑĞ°Ğ¼Ğ¾Ğ¼ Ğ´ĞµĞ»Ğµ', 'Ğ½Ğ° Ğ²ÑÑĞºĞ¸Ğ¹ ÑĞ»ÑƒÑ‡Ğ°Ğ¹', 'Ğ½Ğ° Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ Ğ²Ğ·Ğ³Ğ»ÑĞ´',
               'Ğ¿Ğ¾ ĞºÑ€Ğ°Ğ¹Ğ½ĞµĞ¹ Ğ¼ĞµÑ€Ğµ', 'Ğ¿Ğ¾ ÑÑƒÑ‚Ğ¸', 'Ğ¿Ğ¾ Ğ¼Ğ½ĞµĞ½Ğ¸Ñ', 'Ğ¿Ğ¾ Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğµ']
    if any(phrase in expr_lower for phrase in invalid):
        return False
    
    # æ’é™¤éæ—¶é—´åè¯
    non_time = ['Ğ²Ğ¾Ğ¿Ñ€Ğ¾Ñ', 'Ñ‚ĞµĞ¼Ñƒ', 'Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ', 'Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ', 'Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñƒ', 'Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°', 'ÑĞ¾ÑÑ‚Ğ°Ğ²', 'Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ', 'Ñ‡Ğ°ÑÑ‚ÑŒ', 'Ñ‡Ğ°ÑÑ‚Ğ¸']
    if any(re.search(r'\b' + noun + r'\b', expr_lower) for noun in non_time):
        return False
    
    # ========== éªŒè¯è§„åˆ™ ==========
    
    # ğŸ”¥ ä¸“é—¨å¤„ç† "Ğ² Ñ‚Ğ¾ (Ğ¶Ğµ) Ğ²Ñ€ĞµĞ¼Ñ" å›ºå®šçŸ­è¯­
    if re.search(r'[Ğ²Ğ’]\s+Ñ‚Ğ¾\s+(?:Ğ¶Ğµ\s+)?Ğ²Ñ€ĞµĞ¼[ÑÑ]', expr_lower):
        return True
    
    # å¤„ç† "Ğ² + å½¢å®¹è¯ + Ğ²Ñ€ĞµĞ¼Ñ" ç»“æ„
    if re.search(r'[Ğ²Ğ’]\s+[Ğ°-ÑÑ‘Ğ-Ğ¯Ğ]+(?:Ğ¾Ğµ|ĞµĞµ)\s+Ğ²Ñ€ĞµĞ¼[ÑÑ]', expr_lower):
        time_related_adjs = [
            'Ñ…Ğ¾Ğ»Ğ¾Ğ´Ğ½', 'Ñ‚ĞµĞ¿Ğ»', 'Ğ¶Ğ°Ñ€Ğº', 'Ğ¼Ğ¾Ñ€Ğ¾Ğ·Ğ½', 'ÑĞ¾Ğ»Ğ½ĞµÑ‡Ğ½', 'Ğ´Ğ¾Ğ¶Ğ´Ğ»Ğ¸Ğ²', 'ÑĞ½ĞµĞ¶Ğ½',
            'Ğ»ĞµÑ‚Ğ½', 'Ğ·Ğ¸Ğ¼Ğ½', 'Ğ²ĞµÑĞµĞ½Ğ½', 'Ğ¾ÑĞµĞ½Ğ½', 'Ñ€Ğ°Ğ½Ğ½', 'Ğ¿Ğ¾Ğ·Ğ´Ğ½', 'Ğ´Ğ¾Ğ»Ğ³', 'ĞºĞ¾Ñ€Ğ¾Ñ‚Ğº',
            'Ğ¿Ñ€Ğ¾ÑˆĞ»', 'Ğ±ÑƒĞ´ÑƒÑ‰', 'Ğ½Ğ°ÑÑ‚Ğ¾ÑÑ‰', 'Ğ¼Ğ¸Ğ½ÑƒĞ²Ñˆ', 'Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ½', 'Ğ´Ğ°Ğ²Ğ½', 'Ğ½ĞµĞ´Ğ°Ğ²Ğ½', 'Ğ±Ğ»Ğ¸Ğ¶Ğ°Ğ¹Ñˆ',
            'Ñ‚Ñ€ÑƒĞ´Ğ½', 'ÑĞ»Ğ¾Ğ¶Ğ½', 'Ñ‚ÑĞ¶ĞµĞ»', 'Ğ»ĞµĞ³Ğº', 'Ğ¿Ñ€Ğ¸ÑÑ‚', 'ÑÑ‡Ğ°ÑÑ‚Ğ»Ğ¸Ğ²', 'Ğ¾ÑĞ¾Ğ±', 'Ğ¾Ğ±Ñ‹Ñ‡Ğ½',
            'Ñ…Ğ¾Ñ€Ğ¾Ñˆ', 'Ğ¿Ğ»Ğ¾Ñ…', 'ÑĞ²ĞµÑ‚Ğ»', 'Ñ‚ĞµĞ¼Ğ½', 'Ñ‚Ğ¸Ñ…', 'ÑˆÑƒĞ¼Ğ½', 'ÑĞ»ĞµĞ´ÑƒÑÑ‰', 'Ğ´Ñ€ÑƒĞ³Ğ¾Ğ¹', 'Ğ¸Ğ½Ğ¾Ğ¹', 'Ğ½Ğ¾Ğ²', 'ÑÑ‚Ğ°Ñ€',
            'ÑĞ²Ğ¾Ğ±Ğ¾Ğ´Ğ½', 'Ñ€Ğ°Ğ±Ğ¾Ñ‡ĞµĞµ', 'Ğ²Ğ¾ĞµĞ½Ğ½', 'Ğ¼Ğ¸Ñ€Ğ½', 'Ñ‚Ğ°ĞºĞ¾Ğµ', 'ÑĞ°Ğ¼'
        ]
        
        time_pronouns = ['ÑÑ‚Ğ¾', 'Ğ²ÑÑ‘', 'ĞºĞ°ĞºĞ¾Ğµ', 'Ğ»ÑĞ±Ğ¾Ğµ', 'ĞºĞ°Ğ¶Ğ´Ğ¾Ğµ', 'Ğ´Ñ€ÑƒĞ³Ğ¾Ğµ']
        
        if any(adj in expr_lower for adj in time_related_adjs) or any(pron in expr_lower for pron in time_pronouns):
            return True
    
    # åŸæœ‰éªŒè¯é€»è¾‘ï¼ˆå½¢å®¹è¯ + å…¶ä»–æ—¶é—´åè¯ï¼‰
    if needs_adj_validation:
        explicit_time_nouns = [
            'Ğ´ĞµĞ½ÑŒ', 'Ğ½Ğ¾Ñ‡ÑŒ', 'Ğ½ĞµĞ´ĞµĞ»Ñ', 'Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñƒ', 'ÑĞµĞºÑƒĞ½Ğ´Ñƒ', 'Ğ¼Ğ¾Ğ¼ĞµĞ½Ñ‚', 'Ñ‡Ğ°Ñ', 'Ğ¿ĞµÑ€Ğ¸Ğ¾Ğ´', 'Ñ€Ğ°Ğ·',
            'Ğ¿Ğ¾Ğ³Ğ¾Ğ´Ñƒ', 'Ğ¿Ğ¾Ñ€Ñƒ', 'Ğ·Ğ¸Ğ¼Ñƒ', 'Ğ²ĞµÑĞ½Ñƒ', 'Ğ¾ÑĞµĞ½ÑŒ', 'Ğ»ĞµÑ‚Ğ¾', 'Ğ²Ñ€ĞµĞ¼[ÑÑ]',
            'Ğ´Ğ½Ğ¸', 'Ğ½Ğ¾Ñ‡Ğ¸', 'Ñ‡Ğ°ÑÑ‹', 'Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñ‹', 'ÑĞµĞºÑƒĞ½Ğ´Ñ‹', 'Ğ¼Ğ¾Ğ¼ĞµĞ½Ñ‚Ñ‹', 'Ğ½ĞµĞ´ĞµĞ»Ğ¸', 'Ğ¼ĞµÑÑÑ†Ñ‹', 'Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ°'
        ]
        
        has_time_noun = any(re.search(r'\b' + noun + r'\b', expr_lower) for noun in explicit_time_nouns)
        
        if has_time_noun:
            # æ£€æŸ¥æ˜¯å¦æœ‰å½¢å®¹è¯è¯å°¾ï¼ˆé˜³æ€§/ä¸­æ€§/é˜´æ€§/å¤æ•°ç¬¬å››æ ¼ï¼‰
            if re.search(r'\b[Ğ°-ÑÑ‘]+(?:Ñ‹Ğ¹|Ğ¸Ğ¹|Ğ¾Ğ¹|Ğ¾Ğµ|ĞµĞµ|ÑƒÑ|Ñ‹Ğµ|Ğ¸Ğµ)\b', expr_lower):
                return True
        
        # å…œåº•ï¼šæ£€æŸ¥æ˜¯å¦æœ‰æ—¶é—´ç›¸å…³å½¢å®¹è¯
        time_adjs = [
            'Ñ…Ğ¾Ğ»Ğ¾Ğ´Ğ½', 'Ñ‚ĞµĞ¿Ğ»', 'Ğ¶Ğ°Ñ€Ğº', 'Ğ¼Ğ¾Ñ€Ğ¾Ğ·Ğ½', 'ÑĞ¾Ğ»Ğ½ĞµÑ‡Ğ½', 'Ğ´Ğ¾Ğ¶Ğ´Ğ»Ğ¸Ğ²', 'ÑĞ½ĞµĞ¶Ğ½', 'ÑÑĞ½', 'Ğ¿Ğ°ÑĞ¼ÑƒÑ€Ğ½',
            'Ğ»ĞµÑ‚Ğ½', 'Ğ·Ğ¸Ğ¼Ğ½', 'Ğ²ĞµÑĞµĞ½Ğ½', 'Ğ¾ÑĞµĞ½Ğ½', 'Ñ€Ğ°Ğ½Ğ½', 'Ğ¿Ğ¾Ğ·Ğ´Ğ½', 'Ğ´Ğ¾Ğ»Ğ³', 'ĞºĞ¾Ñ€Ğ¾Ñ‚Ğº',
            'Ğ¿Ñ€Ğ¾ÑˆĞ»', 'Ğ±ÑƒĞ´ÑƒÑ‰', 'Ğ½Ğ°ÑÑ‚Ğ¾ÑÑ‰', 'Ğ¼Ğ¸Ğ½ÑƒĞ²Ñˆ', 'Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ½', 'Ğ´Ğ°Ğ²Ğ½', 'Ğ½ĞµĞ´Ğ°Ğ²Ğ½', 'Ğ±Ğ»Ğ¸Ğ¶Ğ°Ğ¹Ñˆ',
            'Ñ‚Ñ€ÑƒĞ´Ğ½', 'ÑĞ»Ğ¾Ğ¶Ğ½', 'Ñ‚ÑĞ¶ĞµĞ»', 'Ğ»ĞµĞ³Ğº', 'Ğ¿Ñ€Ğ¸ÑÑ‚', 'ÑÑ‡Ğ°ÑÑ‚Ğ»Ğ¸Ğ²', 'Ğ¾ÑĞ¾Ğ±', 'Ğ¾Ğ±Ñ‹Ñ‡Ğ½',
            'Ñ…Ğ¾Ñ€Ğ¾Ñˆ', 'Ğ¿Ğ»Ğ¾Ñ…', 'ÑĞ²ĞµÑ‚Ğ»', 'Ñ‚ĞµĞ¼Ğ½', 'Ñ‚Ğ¸Ñ…', 'ÑˆÑƒĞ¼Ğ½', 'ÑĞ»ĞµĞ´ÑƒÑÑ‰', 'Ğ´Ñ€ÑƒĞ³Ğ¾Ğ¹', 'Ğ¸Ğ½Ğ¾Ğ¹', 'Ğ½Ğ¾Ğ²', 'ÑÑ‚Ğ°Ñ€',
            'ÑÑ‚', 'Ñ‚Ğ¾Ñ‚', 'ĞºĞ°Ğ¶Ğ´', 'Ğ»ÑĞ±', 'Ğ²Ñ', 'ĞºĞ°Ğº', 'Ñ‚Ğ°Ğº', 'ÑĞ°Ğ¼'
        ]
        if any(adj in expr_lower for adj in time_adjs):
            return True
        
        return False
    
    return True


def _time4_remove_overlapping(expressions):
    """ç§»é™¤é‡å çš„è¡¨è¾¾å¼"""
    if not expressions:
        return []
    
    sorted_expr = sorted(expressions, key=lambda x: x['position'])
    unique = []
    
    for expr in sorted_expr:
        is_overlap = False
        for existing in unique:
            existing_start = existing['position']
            existing_end = existing_start + len(existing['text'])
            expr_start = expr['position']
            expr_end = expr_start + len(expr['text'])
            
            if not (expr_end <= existing_start or expr_start >= existing_end):
                is_overlap = True
                if len(expr['text']) > len(existing['text']):
                    unique.remove(existing)
                    unique.append(expr)
                break
        
        if not is_overlap:
            unique.append(expr)
    
    return unique




# ==================== è§„åˆ™ 4: ç¬¬å…­æ ¼æ—¶é—´è¡¨è¾¾æ£€æµ‹ ====================
def detect_russian_time_expression_6th_case(content_list, required_count):
    """
    æ£€æµ‹ä¿„è¯­ç¬¬å…­æ ¼æ—¶é—´è¡¨è¾¾æ ¼å¼
    
    Args:
        content_list: æ–‡æœ¬å†…å®¹åˆ—è¡¨
        required_count: è¦æ±‚çš„æ—¶é—´è¡¨è¾¾æ•°é‡
    
    Returns:
        tuple: (1/0, è¯´æ˜ä¿¡æ¯)
    """
    if content_list == "INVALID" or content_list is None:
        return 0, "âŒ è¾“å…¥æ–‡æœ¬æ— æ•ˆ"
    
    try:
        required_count = int(required_count)
    except (ValueError, TypeError):
        return 0, f"âŒ required_count å¿…é¡»æ˜¯æ•´æ•°: '{required_count}'"

    try:
        if isinstance(content_list, list):
            text = ' '.join(str(item) for item in content_list if item and str(item) != "INVALID")
        else:
            text = str(content_list)
        
        if not text.strip():
            if required_count == 0:
                return 1, "âœ… å†…å®¹ä¸ºç©ºï¼Œç¬¦åˆè¦æ±‚ 0 ä¸ªæ—¶é—´è¡¨è¾¾"
            else:
                return 0, "âŒ å†…å®¹ä¸ºç©ºï¼Œæ— æ³•æ£€æµ‹"
        
        patterns_6th = [
            r'Ğ²\s+(ÑÑ‚Ğ¾Ğ¼|Ğ¿Ñ€Ğ¾ÑˆĞ»Ğ¾Ğ¼|ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ¼|Ğ±ÑƒĞ´ÑƒÑ‰ĞµĞ¼)\s+Ğ³Ğ¾Ğ´Ñƒ',
            r'Ğ²\s+(\d{4})\s+Ğ³Ğ¾Ğ´Ñƒ',
            r'Ğ²\s+(Ğ´Ğ²Ğµ\s+Ñ‚Ñ‹ÑÑÑ‡Ğ¸\s+)?(\d+)-?Ğ¾Ğ¼\s+Ğ³Ğ¾Ğ´Ñƒ',
            r'Ğ²\s+(\d{4})-(\d{4})\s+Ğ³Ğ¾Ğ´Ğ°Ñ…',
            r'Ğ²\s+(\d\d)-Ñ…\s+Ğ³Ğ¾Ğ´Ğ°Ñ…',
            r'Ğ²\s+(ÑĞ½Ğ²Ğ°Ñ€Ğµ|Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ğµ|Ğ¼Ğ°Ñ€Ñ‚Ğµ|Ğ°Ğ¿Ñ€ĞµĞ»Ğµ|Ğ¼Ğ°Ğµ|Ğ¸ÑĞ½Ğµ|Ğ¸ÑĞ»Ğµ|Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğµ|ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ğµ|Ğ¾ĞºÑ‚ÑĞ±Ñ€Ğµ|Ğ½Ğ¾ÑĞ±Ñ€Ğµ|Ğ´ĞµĞºĞ°Ğ±Ñ€Ğµ)',
            r'Ğ²\s+(ÑÑ‚Ğ¾Ğ¼|Ğ¿Ñ€Ğ¾ÑˆĞ»Ğ¾Ğ¼|ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ¼)\s+Ğ¼ĞµÑÑÑ†Ğµ',
            r'Ğ²(?:Ğ¾)?\s+(Ğ¿ĞµÑ€Ğ²Ğ¾Ğ¼|Ğ²Ñ‚Ğ¾Ñ€Ğ¾Ğ¼|Ñ‚Ñ€ĞµÑ‚ÑŒĞµĞ¼|Ñ‡ĞµÑ‚Ğ²Ñ‘Ñ€Ñ‚Ğ¾Ğ¼|Ñ‡ĞµÑ‚Ğ²ĞµÑ€Ñ‚Ğ¾Ğ¼|Ğ¿ÑÑ‚Ğ¾Ğ¼|ÑˆĞµÑÑ‚Ğ¾Ğ¼|ÑĞµĞ´ÑŒĞ¼Ğ¾Ğ¼|Ğ²Ğ¾ÑÑŒĞ¼Ğ¾Ğ¼|Ğ´ĞµĞ²ÑÑ‚Ğ¾Ğ¼|Ğ´ĞµÑÑÑ‚Ğ¾Ğ¼|Ğ¾Ğ´Ğ¸Ğ½Ğ½Ğ°Ğ´Ñ†Ğ°Ñ‚Ğ¾Ğ¼|Ğ´Ğ²ĞµĞ½Ğ°Ğ´Ñ†Ğ°Ñ‚Ğ¾Ğ¼)\s+Ñ‡Ğ°Ñ[ÑƒĞµ](?:\s+(Ğ½Ğ¾Ñ‡Ğ¸|ÑƒÑ‚Ñ€Ğ°|Ğ´Ğ½Ñ|Ğ²ĞµÑ‡ĞµÑ€Ğ°))?',
            r'Ğ²\s+(Ğ¿Ñ€Ğ¾ÑˆĞ»Ğ¾Ğ¼|ÑÑ‚Ğ¾Ğ¼|ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ¼)\s+Ğ²ĞµĞºĞµ',
            r'Ğ²\s+(\d+)-?Ğ¾Ğ¼\s+Ğ²ĞµĞºĞµ',
            r'Ğ²\s+(\d+)-?Ñ‹Ñ…\s+Ğ³Ğ¾Ğ´Ğ°Ñ…',
            r'Ğ²\s+(X{0,3}(?:IX|IV|V?I{0,3}))[-Ğ¾Ğ¼]?\s+Ğ²ĞµĞºĞµ',
            r'Ğ²\s+(Ğ¿Ñ€Ğ¾ÑˆĞ»Ğ¾Ğ¼|Ğ½Ğ°ÑÑ‚Ğ¾ÑÑ‰ĞµĞ¼|Ğ±ÑƒĞ´ÑƒÑ‰ĞµĞ¼)(?!\s+Ğ³Ğ¾Ğ´Ñƒ)',
            r'Ğ²\s+(Ğ½Ğ°Ñ‡Ğ°Ğ»Ğµ|ĞºĞ¾Ğ½Ñ†Ğµ|ÑĞµÑ€ĞµĞ´Ğ¸Ğ½Ğµ)\s+([Ğ°-ÑÑ‘Ğ-Ğ¯Ğ]+(?:\s+[Ğ°-ÑÑ‘Ğ-Ğ¯Ğ]+)*)',
            r'Ğ²\s+(Ğ·Ğ¸Ğ¼Ñƒ|Ğ²ĞµÑĞ½Ñƒ|Ğ»ĞµÑ‚Ğ¾|Ğ¾ÑĞµĞ½ÑŒ)',
            r'Ğ²\s+(ÑƒÑ‚Ñ€Ğ¾|Ğ²ĞµÑ‡ĞµÑ€|Ğ½Ğ¾Ñ‡ÑŒ|Ğ¿Ğ¾Ğ»Ğ´ĞµĞ½ÑŒ|Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ñ‡ÑŒ)',
            r'Ğ²\s+(Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ½ĞµĞµ|Ğ±Ğ»Ğ¸Ğ¶Ğ°Ğ¹ÑˆĞµĞ¼|ÑĞºĞ¾Ñ€Ğ¾Ğ¼)\s+(Ğ²Ñ€ĞµĞ¼Ñ|Ğ±ÑƒĞ´ÑƒÑ‰ĞµĞ¼)',
            r'Ğ²\s+(Ğ¿ĞµÑ€Ğ¸Ğ¾Ğ´|ÑĞ¿Ğ¾Ñ…Ñƒ|Ñ€Ğ°Ğ¼ĞºĞ°Ñ…|Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ|Ñ…Ğ¾Ğ´Ğµ)\s+([Ğ°-ÑÑ‘Ğ-Ğ¯Ğ]+(?:\s+[Ğ°-ÑÑ‘Ğ-Ğ¯Ğ]+)*)',
            r'Ğ²\s+(Ğ´Ğ½Ğ¸|Ğ³Ğ¾Ğ´Ñ‹|ÑÑƒÑ‚ĞºĞ¸)(?:\s+([Ğ°-ÑÑ‘Ğ-Ğ¯Ğ]+(?:\s+[Ğ°-ÑÑ‘Ğ-Ğ¯Ğ]+)*))?',
            r'Ğ½Ğ°\s+(ÑÑ‚Ğ¾Ğ¹|Ğ¿Ñ€Ğ¾ÑˆĞ»Ğ¾Ğ¹|ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ¹|Ğ±ÑƒĞ´ÑƒÑ‰ĞµĞ¹)\s+Ğ½ĞµĞ´ĞµĞ»Ğµ',
            r'Ğ½Ğ°\s+Ğ¿Ñ€Ğ¾Ñ‚ÑĞ¶ĞµĞ½Ğ¸Ğ¸\s+([Ğ°-ÑÑ‘Ğ-Ğ¯Ğ]+(?:\s+[Ğ°-ÑÑ‘Ğ-Ğ¯Ğ]+)*)',
            r'Ğ²\s+ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ…\s+([Ğ°-ÑÑ‘Ğ-Ğ¯Ğ]+(?:\s+[Ğ°-ÑÑ‘Ğ-Ğ¯Ğ]+)*)',
        ]
        
        found = []
        
        for i, pattern in enumerate(patterns_6th):
            matches = re.finditer(pattern, text, re.IGNORECASE)
            for match in matches:
                full_match = match.group(0).strip()
                
                if _time6_is_valid(full_match):
                    found.append({
                        'text': full_match,
                        'position': match.start(),
                        'pattern_type': i
                    })
        
        unique = _time6_remove_overlapping(found)
        
        total = len(unique)
        display = [expr['text'] for expr in unique]
        
        if total == required_count:
            if required_count == 0:
                return 1, f"âœ… æ­£ç¡®ï¼šæœªæ‰¾åˆ°ç¬¬å…­æ ¼æ—¶é—´è¡¨è¾¾ (è¦æ±‚=0ä¸ª)"
            else:
                return 1, f"âœ… æ‰¾åˆ° {total} ä¸ªç¬¬å…­æ ¼æ—¶é—´è¡¨è¾¾ (è¦æ±‚={required_count}ä¸ª): {display}"
        else:
            if required_count == 0:
                return 0, f"âŒ æ‰¾åˆ° {total} ä¸ªç¬¬å…­æ ¼æ—¶é—´è¡¨è¾¾ï¼Œä½†è¦æ±‚ 0 ä¸ª: {display}"
            else:
                return 0, f"âŒ æ‰¾åˆ° {total} ä¸ªç¬¬å…­æ ¼æ—¶é—´è¡¨è¾¾ (è¦æ±‚={required_count}ä¸ª): {display}"

    except Exception as e:
        return 0, f"âŒ å‡½æ•°æ‰§è¡Œå¼‚å¸¸: {e}"


def _time6_is_valid(full_text):
    """éªŒè¯ç¬¬å…­æ ¼æ—¶é—´è¡¨è¾¾"""
    if len(full_text) < 2:
        return False
    
    excluded = [
        r'Ğ²\s+Ñ‚Ğ¾Ğ¼\s+Ñ‡Ğ¸ÑĞ»Ğµ', r'Ğ²\s+Ñ‡Ğ°ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸', r'Ğ²\s+Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğµ', r'Ğ²\s+ÑĞ»ÑƒÑ‡Ğ°Ğµ',
        r'Ğ²\s+ÑĞ²ÑĞ·Ğ¸', r'Ğ²\s+ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğ¸', r'Ğ²\s+Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸', r'Ğ²\s+Ñ‚ĞµÑ‡ĞµĞ½Ğ¸Ğµ',
        r'Ğ½Ğ°\s+ÑÑ‚Ğ¾Ğ»', r'Ğ½Ğ°\s+ÑƒĞ»Ğ¸Ñ†', r'Ğ²\s+ĞºĞ¾Ğ¼Ğ½Ğ°Ñ‚', r'Ğ²\s+Ğ·Ğ´Ğ°Ğ½Ğ¸',
    ]
    
    for pattern in excluded:
        if re.search(pattern, full_text, re.IGNORECASE):
            return False
    
    return True


def _time6_remove_overlapping(expressions):
    """å»é™¤é‡å çš„ç¬¬å…­æ ¼è¡¨è¾¾å¼"""
    if not expressions:
        return []
    
    expressions.sort(key=lambda x: x['position'])
    
    unique = []
    for expr in expressions:
        overlaps = False
        for existing in unique:
            if abs(expr['position'] - existing['position']) < 15:
                overlaps = True
                if len(expr['text']) > len(existing['text']):
                    unique.remove(existing)
                    unique.append(expr)
                break
        
        if not overlaps:
            unique.append(expr)
    
    return unique





try:
    import pyphen
    RUSSIAN_SYLLABIFIER = pyphen.Pyphen(lang='ru_RU')
    PYPHEN_AVAILABLE = True
except ImportError:
    RUSSIAN_SYLLABIFIER = None
    PYPHEN_AVAILABLE = False
    print("âš ï¸  è­¦å‘Š: pyphen åº“æœªå®‰è£…ï¼Œå°†ä½¿ç”¨å¤‡ç”¨éŸ³èŠ‚åˆ’åˆ†æ–¹æ³•ã€‚å»ºè®®è¿è¡Œ: pip install pyphen")


# ==================== LibraryManager ç±» ====================
class LibraryManager:
    """ç®¡ç†å¤–éƒ¨åº“çš„åŠ è½½"""
    _stresser = None
    _stresser_available = None
    
    @classmethod
    def get_stresser(cls):
        """è¿”å› (stresserå¯¹è±¡, æ˜¯å¦å¯ç”¨)"""
        if cls._stresser_available is not None:
            return cls._stresser, cls._stresser_available
        
        try:
            import russtress
            cls._stresser = russtress.Accent()
            cls._stresser_available = True
        except ImportError:
            cls._stresser = None
            cls._stresser_available = False
        
        return cls._stresser, cls._stresser_available


def create_logger(debug):
    """åˆ›å»ºæ—¥å¿—å‡½æ•°"""
    def log(msg):
        if debug:
            print(msg)
    return log


# ==================== è§„åˆ™ 5: ä¿„è¯­æ ¼å¾‹æ£€æµ‹ ====================
def detect_russian_single_meter(content_list, expected_meter, debug=False):
    """
    æ£€æµ‹ä¿„è¯­è¯—æ­Œæ˜¯å¦ç¬¦åˆæŒ‡å®šçš„æ ¼å¾‹ç±»å‹ï¼ˆç®€åŒ–è¾“å‡ºç‰ˆï¼‰
    
    Args:
        content_list: è¯—æ­Œå†…å®¹åˆ—è¡¨
        expected_meter: æœŸæœ›çš„æ ¼å¾‹ç±»å‹ï¼ˆĞ¥Ğ¾Ñ€ĞµĞ¹/Ğ¯Ğ¼Ğ±/Ğ”Ğ°ĞºÑ‚Ğ¸Ğ»ÑŒ/ĞĞ¼Ñ„Ğ¸Ğ±Ñ€Ğ°Ñ…Ğ¸Ğ¹/ĞĞ½Ğ°Ğ¿ĞµÑÑ‚ï¼‰
        debug: æ˜¯å¦è¾“å‡ºè°ƒè¯•ä¿¡æ¯
    
    Returns:
        tuple: (1/0, è¯´æ˜ä¿¡æ¯)
    """
    log = create_logger(debug)
    
    if not isinstance(content_list, list) or not content_list:
        return 0, "âŒ è¾“å…¥è¯—æ­Œæ— æ•ˆ"
    
    poem = str(content_list[0]).strip()
    if not poem:
        return 0, "âŒ æ²¡æœ‰æ‰¾åˆ°è¯—æ­Œå†…å®¹"
    
    log(f"[DEBUG] æœŸæœ›æ ¼å¾‹: {expected_meter}")

    try:
        detected_meter, analysis = _meter_analyze_poem(poem, log)
        
        meter_names = {
            'Ğ¥Ğ¾Ñ€ĞµĞ¹': 'æ‰¬æŠ‘æ ¼(Ğ¥Ğ¾Ñ€ĞµĞ¹)', 
            'Ğ¯Ğ¼Ğ±': 'æŠ‘æ‰¬æ ¼(Ğ¯Ğ¼Ğ±)', 
            'Ğ”Ğ°ĞºÑ‚Ğ¸Ğ»ÑŒ': 'æ‰¬æŠ‘æŠ‘æ ¼(Ğ”Ğ°ĞºÑ‚Ğ¸Ğ»ÑŒ)', 
            'ĞĞ¼Ñ„Ğ¸Ğ±Ñ€Ğ°Ñ…Ğ¸Ğ¹': 'æŠ‘æ‰¬æŠ‘æ ¼(ĞĞ¼Ñ„Ğ¸Ğ±Ñ€Ğ°Ñ…Ğ¸Ğ¹)', 
            'ĞĞ½Ğ°Ğ¿ĞµÑÑ‚': 'æŠ‘æŠ‘æ‰¬æ ¼(ĞĞ½Ğ°Ğ¿ĞµÑÑ‚)', 
            'Unknown': 'æœªçŸ¥æˆ–æ··åˆæ ¼å¾‹'
        }
        
        detected_name = meter_names.get(detected_meter, 'æœªçŸ¥æ ¼å¾‹')
        expected_name = meter_names.get(expected_meter, expected_meter)
        
        # âœ… æ·»åŠ è¦†ç›–ç‡æ£€æŸ¥
        matched_lines = analysis.get('matched_lines', 0)
        total_lines = analysis.get('total_lines', 0)
        coverage_rate = matched_lines / total_lines if total_lines > 0 else 0
        
        if detected_meter == expected_meter:
            # âœ… å³ä½¿æ ¼å¾‹åŒ¹é…ï¼Œä¹Ÿè¦æ£€æŸ¥è¦†ç›–ç‡
            if coverage_rate < 0.75:
                return 0, f"âŒ æ ¼å¾‹ä¸€è‡´æ€§ä¸è¶³: è™½ç„¶æ£€æµ‹åˆ°{detected_name}ï¼Œä½†ç¬¦åˆåº¦ä¸å¤Ÿï¼ˆéœ€â‰¥75%ï¼‰"
            return 1, f"âœ… è¯—æ­Œæ ¼å¾‹ç¬¦åˆè¦æ±‚: æ£€æµ‹åˆ°{detected_name}ï¼Œç¬¦åˆæœŸæœ›çš„{expected_name}"
        else:
            return 0, f"âŒ è¯—æ­Œæ ¼å¾‹ä¸ç¬¦åˆè¦æ±‚: æ£€æµ‹åˆ°{detected_name}ï¼ŒæœŸæœ›{expected_name}"
    
    except Exception as e:
        log(f"[ERROR] å¼‚å¸¸: {e}")
        if debug:
            import traceback
            traceback.print_exc()
        return 0, f"âŒ å‡½æ•°æ‰§è¡Œå¼‚å¸¸: {e}"


def _meter_analyze_poem(poem, log):
    """åˆ†æè¯—æ­Œæ ¼å¾‹"""
    lines = [line.strip() for line in poem.split('\n') if line.strip()]
    if not lines:
        return 'Unknown', {'method': 'error', 'details': 'æ— æœ‰æ•ˆè¯—è¡Œ', 'matched_lines': 0, 'total_lines': 0}
    
    log(f"[DEBUG] è¯—æ­Œå…± {len(lines)} è¡Œ")
    line_analyses = [_meter_analyze_line(line, i + 1, log) for i, line in enumerate(lines)]
    return _meter_determine(line_analyses, log)


def _meter_analyze_line(line, line_num, log):
    """åˆ†æå•è¡Œè¯—æ­Œçš„æ ¼å¾‹"""
    clean_line = re.sub(r'[â€”,.:;!?Â»Â«""\(\)]+', ' ', line)
    words = re.findall(r'[Ğ°-ÑÑ‘Ğ-Ğ¯Ğ]+', clean_line)
    
    log(f"[DEBUG] ç¬¬{line_num}è¡Œå•è¯: {words}")
    
    all_patterns = []
    
    for word in words:
        stressed_word = _meter_get_stress(word, log)
        word_info = _meter_word_stress(word, stressed_word, log)
        
        # ç›´æ¥æ·»åŠ æ¯ä¸ªè¯çš„é‡éŸ³æ¨¡å¼
        stress_pattern = word_info.get('stress_pattern', [])
        all_patterns.extend(stress_pattern)
        
        log(f"[DEBUG]   å•è¯: {word} -> éŸ³èŠ‚: {word_info.get('syllables', [])} -> æ¨¡å¼: {stress_pattern}")
    
    log(f"[DEBUG] ç¬¬{line_num}è¡Œå®Œæ•´æ¨¡å¼: {all_patterns}")
    
    matches = _meter_match_pattern(all_patterns, log)
    
    return {
        'line_num': line_num,
        'line_text': line,
        'stress_pattern': all_patterns,
        'meter_matches': matches
    }


def _meter_split_syllables(word):
    """ä½¿ç”¨ pyphen è¿›è¡Œå‡†ç¡®çš„éŸ³èŠ‚åˆ’åˆ†"""
    if PYPHEN_AVAILABLE and RUSSIAN_SYLLABIFIER:
        try:
            syllables_str = RUSSIAN_SYLLABIFIER.inserted(word.lower(), hyphen='|')
            syllables = syllables_str.split('|')
            if syllables and all(syllables):
                return syllables
        except:
            pass
    
    # å¤‡ç”¨æ–¹æ¡ˆï¼šåŸºäºå…ƒéŸ³çš„ç®€å•åˆ’åˆ†
    vowels = "Ğ°ĞµÑ‘Ğ¸Ğ¾ÑƒÑ‹ÑÑÑ"
    word_lower = word.lower()
    syllables = []
    temp = ""
    
    for char in word_lower:
        temp += char
        if char in vowels:
            syllables.append(temp)
            temp = ""
    
    if temp and syllables:
        syllables[-1] += temp
    elif temp:
        syllables.append(temp)
    
    return syllables if syllables else [word]


def _meter_word_stress(original, stressed, log):
    """è·å–å•è¯çš„é‡éŸ³åˆ†æ"""
    vowels_lower = "Ğ°ĞµÑ‘Ğ¸Ğ¾ÑƒÑ‹ÑÑÑ"
    word_lower = original.lower()
    has_vowels = any(c in vowels_lower for c in word_lower)
    
    # åŠŸèƒ½è¯åˆ—è¡¨
    function_words = {
        'Ğ²', 'Ğº', 'Ñ', 'Ğ½Ğ°', 'Ğ·Ğ°', 'Ğ¿Ğ¾Ğ´', 'Ğ½Ğ°Ğ´', 'Ğ¾Ñ‚', 'Ğ´Ğ¾', 'Ğ¿Ğ¾', 'Ğ¿Ñ€Ğ¸', 'Ğ¿Ñ€Ğ¾',
        'Ğ±ĞµĞ·', 'Ğ´Ğ»Ñ', 'Ñ‡ĞµÑ€ĞµĞ·', 'Ğ¼ĞµĞ¶Ğ´Ñƒ', 'Ñƒ', 'Ğ¾', 'Ğ¾Ğ±', 'Ğ¸Ğ·', 'ÑĞ¾', 'ĞºĞ¾', 'Ğ²Ğ¾',
        'Ğ¸', 'Ğ°', 'Ğ½Ğ¾', 'Ğ¸Ğ»Ğ¸', 'Ğ´Ğ°', 'Ğ½Ğ¸',
        'Ğ½Ğµ', 'Ğ¶Ğµ', 'Ğ»Ğ¸', 'Ğ±Ñ‹',
        'Ñ‡Ñ‚Ğ¾', 'ĞºĞ°Ğº', 'Ğ³Ğ´Ğµ', 'ĞºÑƒĞ´Ğ°', 'ĞºĞ¾Ğ³Ğ´Ğ°', 'ĞºÑ‚Ğ¾', 'Ñ‡ĞµĞ¹', 'Ñ‡ĞµĞ¼', 'ĞºĞµĞ¼'
    }
    
    if word_lower in function_words:
        if has_vowels:
            return {
                'word': original,
                'syllables': [original],
                'stress_pattern': ['è½»'],
                'is_syllabic': True,
                'is_function_word': True
            }
        else:
            return {
                'word': original,
                'syllables': [],
                'stress_pattern': [],
                'is_syllabic': False,
                'is_function_word': True
            }

    if not has_vowels:
        return {
            'word': original,
            'syllables': [],
            'stress_pattern': [],
            'is_syllabic': False
        }

    syllables = _meter_split_syllables(original)
    syllable_count = len(syllables)
    
    log(f"[DEBUG]     {original} çš„éŸ³èŠ‚åˆ’åˆ†: {syllables} (å…±{syllable_count}ä¸ª)")
    
    stress_pattern = ['è½»'] * syllable_count
    stressed_syllable_index = -1
    
    if 'Ì' in stressed:
        vowel_count = 0
        for i, char in enumerate(stressed):
            if char.lower() in vowels_lower:
                vowel_count += 1
            if char == 'Ì':
                stressed_syllable_index = vowel_count - 1
                log(f"[DEBUG]     æ‰¾åˆ°é‡éŸ³ç¬¦å·ï¼Œä½ç½®åœ¨ç¬¬ {stressed_syllable_index + 1} ä¸ªéŸ³èŠ‚")
                break
    
    if stressed_syllable_index == -1 and 'Ñ‘' in word_lower:
        vowel_count = 0
        for char in word_lower:
            if char in vowels_lower:
                vowel_count += 1
                if char == 'Ñ‘':
                    stressed_syllable_index = vowel_count - 1
                    log(f"[DEBUG]     æ‰¾åˆ° Ñ‘ï¼Œä½ç½®åœ¨ç¬¬ {stressed_syllable_index + 1} ä¸ªéŸ³èŠ‚")
                    break
    
    if stressed_syllable_index != -1 and 0 <= stressed_syllable_index < syllable_count:
        stress_pattern[stressed_syllable_index] = 'é‡'
        log(f"[DEBUG]     æ ‡è®°ç¬¬ {stressed_syllable_index + 1} ä¸ªéŸ³èŠ‚ä¸ºé‡éŸ³")
    elif syllable_count > 0:
        default_stress = -2 if syllable_count >= 2 else 0
        stress_pattern[default_stress] = 'é‡'
        log(f"[DEBUG]     ä½¿ç”¨é»˜è®¤é‡éŸ³ä½ç½®: ç¬¬ {syllable_count + default_stress + 1} ä¸ªéŸ³èŠ‚")

    return {
        'word': original,
        'syllables': syllables,
        'stress_pattern': stress_pattern,
        'is_syllabic': True,
        'syllable_count': syllable_count
    }


@lru_cache(maxsize=2000)
def _meter_get_stress(word, log):
    """è·å–å•è¯é‡éŸ³"""
    stresser, available = LibraryManager.get_stresser()
    
    word_lower = word.lower().strip('.,!?;:â€”Â»Â«""')
    if not word_lower:
        return ""
    
    stress_dict = {
        'Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑ': 'Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÌÑÑ', 'Ñ‚ĞµÑ…Ğ½Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ': 'Ñ‚ĞµÑ…Ğ½Ğ¾Ğ»Ğ¾ÌĞ³Ğ¸Ñ', 'Ñ‚ĞµÑ…Ğ½Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¹': 'Ñ‚ĞµÑ…Ğ½Ğ¾Ğ»Ğ¾ÌĞ³Ğ¸Ğ¹',
        'Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ñ': 'Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°ÌÑ†Ğ¸Ñ', 'Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ñ': 'Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°ÌÑ†Ğ¸Ñ', 'Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ğµ': 'Ñ€Ğ°Ğ·Ğ²Ğ¸ÌÑ‚Ğ¸Ğµ',
        'Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ğ¸': 'Ñ€Ğ°Ğ·Ğ²Ğ¸ÌÑ‚Ğ¸Ğ¸', 'Ğ±ÑƒĞ´ÑƒÑ‰ĞµĞµ': 'Ğ±ÑƒÌĞ´ÑƒÑ‰ĞµĞµ', 'Ğ¼Ğ¾Ğ´ĞµÑ€Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ': 'Ğ¼Ğ¾Ğ´ĞµÑ€Ğ½Ğ¸Ğ·Ğ°ÌÑ†Ğ¸Ñ',
        'Ğ¼Ğ¾Ğ´ĞµÑ€Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸': 'Ğ¼Ğ¾Ğ´ĞµÑ€Ğ½Ğ¸Ğ·Ğ°ÌÑ†Ğ¸Ğ¸', 'ÑÑ‚Ñ€Ğ¾Ğ¸Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ¾': 'ÑÑ‚Ñ€Ğ¾Ğ¸ÌÑ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ¾',
        'Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ': 'Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµÌĞ½Ğ¸Ğµ', 'Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ': 'Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°ÌĞ½Ğ¸Ğµ',
        'Ğ¸Ğ½Ğ´ÑƒÑÑ‚Ñ€Ğ¸Ñ': 'Ğ¸Ğ½Ğ´ÑƒÌÑÑ‚Ñ€Ğ¸Ñ', 'ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸ĞºĞ°': 'ÑĞºĞ¾Ğ½Ğ¾ÌĞ¼Ğ¸ĞºĞ°', 'Ñ†Ğ¸Ñ„Ñ€Ñ‹': 'Ñ†Ğ¸ÌÑ„Ñ€Ñ‹',
        'Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ': 'Ğ¿Ñ€Ğ¾Ñ†ĞµÌÑÑ', 'Ğ´Ğ²Ğ¸Ğ¶ĞµÑ‚': 'Ğ´Ğ²Ğ¸ÌĞ¶ĞµÑ‚', 'Ğ´Ğ²Ğ¸Ğ¶ĞµĞ¼ÑÑ': 'Ğ´Ğ²Ğ¸ÌĞ¶ĞµĞ¼ÑÑ',
        'Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞµ': 'Ğ¿Ğ¾Ñ‚Ğ¾ÌĞºĞµ', 'Ğ¿Ñ€Ğ°Ğ²Ğ¸Ñ‚': 'Ğ¿Ñ€Ğ°ÌĞ²Ğ¸Ñ‚', 'ÑĞ²ĞµÑ‚Ğ¾Ğ¼': 'ÑĞ²ĞµÌÑ‚Ğ¾Ğ¼',
        'Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ°': 'Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÌÑÑĞ°', 'Ğ²Ğ²Ñ‹ÑÑŒ': 'Ğ²Ğ²Ñ‹ÑÑŒ', 'Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ñ‚': 'Ğ¾ÑÑ‚Ğ°ÌĞ²Ğ¸Ñ‚',
        'ÑˆĞ°Ğ½ÑĞ°': 'ÑˆĞ°ÌĞ½ÑĞ°', 'ÑĞ¾Ğ¼Ğ½ĞµĞ½ÑŒÑĞ¼': 'ÑĞ¾Ğ¼Ğ½ĞµÌĞ½ÑŒÑĞ¼', 'Ğ²Ğ¿ĞµÑ€ĞµĞ´': 'Ğ²Ğ¿ĞµÑ€Ñ‘Ğ´',
        'ÑÑ‚Ñ€ĞµĞ¼Ğ¸ÑÑŒ': 'ÑÑ‚Ñ€ĞµĞ¼Ğ¸ÌÑÑŒ', 'Ñ‚Ğ²Ğ¾Ñ€Ñ‡ĞµÑÑ‚Ğ²Ğ¾': 'Ñ‚Ğ²Ğ¾ÌÑ€Ñ‡ĞµÑÑ‚Ğ²Ğ¾', 'Ğ½Ğ°Ğ´ĞµĞ¶Ğ´Ñ‹': 'Ğ½Ğ°Ğ´ĞµÌĞ¶Ğ´Ñ‹',
        'ÑĞ¸ÑĞµÑ‚': 'ÑĞ¸ÑÌĞµÑ‚', 'Ğ¾Ğ±Ğ»Ğ°ĞºĞ°Ñ…': 'Ğ¾Ğ±Ğ»Ğ°ĞºĞ°ÌÑ…', 'ÑˆĞ°Ğ³Ğ°ĞµĞ¼': 'ÑˆĞ°Ğ³Ğ°ÌĞµĞ¼',
        'Ğ±Ğ¾ÑÑÑŒ': 'Ğ±Ğ¾ÑÌÑÑŒ', 'ÑĞ»Ğ°Ğ²Ğ¸Ñ‚': 'ÑĞ»Ğ°ÌĞ²Ğ¸Ñ‚', 'Ğ·Ğ°Ğ¼Ğ¾Ğº': 'Ğ·Ğ°ÌĞ¼Ğ¾Ğº',
        'Ğ¼ÑƒĞºĞ°': 'Ğ¼ÑƒÌĞºĞ°', 'Ğ¾Ñ€Ğ³Ğ°Ğ½': 'Ğ¾ÌÑ€Ğ³Ğ°Ğ½', 'Ğ°Ñ‚Ğ»Ğ°Ñ': 'Ğ°ÌÑ‚Ğ»Ğ°Ñ',
        'Ñ…Ğ»Ğ¾Ğ¿Ğ¾Ğº': 'Ñ…Ğ»Ğ¾ÌĞ¿Ğ¾Ğº', 'Ğ¿Ğ¸Ğ»Ğ¸': 'Ğ¿Ğ¸ÌĞ»Ğ¸', 'Ğ¿Ğ°Ñ€Ğ¸Ñ‚ÑŒ': 'Ğ¿Ğ°ÌÑ€Ğ¸Ñ‚ÑŒ',
        'Ğ²ĞµÑ‚ĞµÑ€': 'Ğ²ĞµÌÑ‚ĞµÑ€', 'ÑĞ²ĞµĞ¶Ğ¸Ğ¹': 'ÑĞ²ĞµÌĞ¶Ğ¸Ğ¹', 'ÑĞ²ĞµÑ‚Ğ¸Ñ‚': 'ÑĞ²ĞµÌÑ‚Ğ¸Ñ‚',
        'ÑĞ²ĞµÑ‚Ğ»Ñ‹Ğ¹': 'ÑĞ²ĞµÌÑ‚Ğ»Ñ‹Ğ¹', 'ÑĞ¾Ğ»Ğ½Ñ†Ğµ': 'ÑĞ¾ÌĞ»Ğ½Ñ†Ğµ', 'Ğ¼Ğ¸Ñ€': 'Ğ¼Ğ¸Ñ€',
        'Ğ¼Ğ¸Ñ€Ğµ': 'Ğ¼Ğ¸ÌÑ€Ğµ', 'Ğ¸Ğ´ĞµĞ¹': 'Ğ¸Ğ´ĞµÌĞ¹', 'Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ': 'Ğ¼ĞµĞ½ÑÌĞµÑ‚ÑÑ',
        'Ñ‚Ğ°Ğ½Ñ†Ğµ': 'Ñ‚Ğ°ÌĞ½Ñ†Ğµ', 'Ğ½Ğ¾Ğ²Ñ‹Ğ¹': 'Ğ½Ğ¾ÌĞ²Ñ‹Ğ¹', 'Ğ´ĞµĞ½ÑŒ': 'Ğ´ĞµĞ½ÑŒ',
        'ÑÑ‡Ğ°ÑÑ‚ÑŒĞµ': 'ÑÑ‡Ğ°ÌÑÑ‚ÑŒĞµ', 'Ğ½ĞµÑÑ‘Ñ‚': 'Ğ½ĞµÑÑ‘Ñ‚', 'Ğ¸Ğ´Ñ‘Ğ¼': 'Ğ¸Ğ´Ñ‘Ğ¼',
        'Ğ½Ğ°Ğ¹Ğ´Ñ‘Ğ¼': 'Ğ½Ğ°Ğ¹Ğ´Ñ‘Ğ¼', 'Ñ€Ğ°ÑÑ‚Ñ‘Ñ‚': 'Ñ€Ğ°ÑÑ‚Ñ‘Ñ‚', 'Ğ¾Ğ¶Ğ¸Ğ²Ñ‘Ñ‚': 'Ğ¾Ğ¶Ğ¸Ğ²Ñ‘Ñ‚',
        'Ğ¿Ğ¾Ğ»Ñ‘Ñ‚': 'Ğ¿Ğ¾Ğ»Ñ‘Ñ‚', 'Ğ¸Ğ´Ñ‘Ñ‚': 'Ğ¸Ğ´Ñ‘Ñ‚', 'Ğ¼Ğ¾Ñ€Ğµ': 'Ğ¼Ğ¾ÌÑ€Ğµ',
        'ÑĞ²ĞµÑ‚': 'ÑĞ²ĞµÑ‚', 'Ğ¿ÑƒÑ‚ÑŒ': 'Ğ¿ÑƒÑ‚ÑŒ', 'Ğ·ĞµĞ¼Ğ»Ñ': 'Ğ·ĞµĞ¼Ğ»ÑÌ',
        'Ğ²Ğ¾Ğ´Ğ°': 'Ğ²Ğ¾Ğ´Ğ°Ì', 'Ğ¾Ğ³Ğ¾Ğ½ÑŒ': 'Ğ¾Ğ³Ğ¾ÌĞ½ÑŒ', 'Ğ½ĞµĞ±Ğ¾': 'Ğ½ĞµÌĞ±Ğ¾',
        'Ñ€ĞµĞºĞ°': 'Ñ€ĞµĞºĞ°Ì', 'Ğ³Ğ¾Ñ€Ğ°': 'Ğ³Ğ¾Ñ€Ğ°Ì', 'ÑĞ¼ĞµĞ»Ğ¾': 'ÑĞ¼ĞµÌĞ»Ğ¾',
        'Ğ²Ğ¼ĞµÑÑ‚Ğµ': 'Ğ²Ğ¼ĞµÌÑÑ‚Ğµ', 'Ğ²Ñ€ĞµĞ¼Ñ': 'Ğ²Ñ€ĞµÌĞ¼Ñ', 'ÑĞµÑ€Ğ´Ñ†Ğµ': 'ÑĞµÌÑ€Ğ´Ñ†Ğµ',
        'Ğ¼ĞµÑ‡Ñ‚Ñ‹': 'Ğ¼ĞµÑ‡Ñ‚Ñ‹Ì', 'Ğ½Ğ°ÑˆĞ°': 'Ğ½Ğ°ÌÑˆĞ°', 'Ğ²ĞµÑĞ½Ğ°': 'Ğ²ĞµÑĞ½Ğ°Ì', 'Ğ¾Ğ½Ğ°': 'Ğ¾Ğ½Ğ°Ì',
        'ÑÑĞ½Ğ¾': 'ÑÌÑĞ½Ğ¾', 'Ğ³Ğ»ÑĞ´ÑŒ': 'Ğ³Ğ»ÑĞ´ÑŒ', 'Ñ€Ğ°Ğ·Ğ²Ğ¸Ğ²Ğ°ĞµĞ¼ÑÑ': 'Ñ€Ğ°Ğ·Ğ²Ğ¸Ğ²Ğ°ÌĞµĞ¼ÑÑ',
        'Ğ¿Ñ€ĞµĞºÑ€Ğ°ÑĞ½Ğ¾': 'Ğ¿Ñ€ĞµĞºÑ€Ğ°ÌÑĞ½Ğ¾'
    }
    
    if word_lower in stress_dict:
        return stress_dict[word_lower]
    
    if available and stresser:
        try:
            stressed = stresser.put_stress(word_lower, stress_symbol='Ì')
            if stressed and ('Ì' in stressed or 'Ñ‘' in stressed):
                return stressed
        except:
            pass
    
    if 'Ñ‘' in word_lower:
        return word_lower
    
    vowels_lower = "Ğ°ĞµĞ¸Ğ¾ÑƒÑ‹ÑÑÑ"
    vowel_count = sum(1 for c in word_lower if c in vowels_lower)
    
    if vowel_count == 1:
        for i, c in enumerate(word_lower):
            if c in vowels_lower:
                return word_lower[:i+1] + 'Ì' + word_lower[i+1:]
        return word_lower
    
    if word_lower.endswith(('Ğ¾Ğ¹', 'Ğ°Ñ', 'Ğ¾Ğµ', 'Ñ‹Ğµ', 'Ğ¸Ğ¹', 'ÑÑ', 'ĞµĞµ', 'Ğ¸Ğµ')):
        vowel_positions = [i for i, c in enumerate(word_lower) if c in vowels_lower]
        if vowel_positions:
            pos = vowel_positions[-1]
            return word_lower[:pos+1] + 'Ì' + word_lower[pos+1:]
    
    if word_lower.endswith(('ĞµĞ½Ğ¸Ğµ', 'Ğ°Ğ½Ğ¸Ğµ', 'ÑÑ‚Ğ²Ğ¾', 'Ğ°Ñ†Ğ¸Ñ', 'ÑÑ†Ğ¸Ñ')):
        vowel_positions = [i for i, c in enumerate(word_lower) if c in vowels_lower]
        if len(vowel_positions) >= 3:
            pos = vowel_positions[-3]
            return word_lower[:pos+1] + 'Ì' + word_lower[pos+1:]
    
    vowel_positions = [i for i, c in enumerate(word_lower) if c in vowels_lower]
    if vowel_positions:
        pos = vowel_positions[-2] if len(vowel_positions) >= 2 else vowel_positions[0]
        return word_lower[:pos+1] + 'Ì' + word_lower[pos+1:]
    
    return word_lower


def _meter_has_pattern_violations(stress_pattern):
    """æ£€æµ‹æ˜¯å¦æœ‰æ ¼å¾‹è¿è§„"""
    pattern_str = ''.join(stress_pattern)
    
    if 'é‡é‡' in pattern_str:
        return True
    
    if 'è½»è½»è½»è½»' in pattern_str:
        return True
    
    total = len(stress_pattern)
    heavy_count = stress_pattern.count('é‡')
    
    if total > 0:
        heavy_ratio = heavy_count / total
        if heavy_ratio < 0.25 or heavy_ratio > 0.60:
            return True
    
    return False


def _meter_match_pattern(stress_pattern, log):
    """åŒ¹é…æ ¼å¾‹æ¨¡å¼"""
    if len(stress_pattern) < 2:
        return []
    
    if _meter_has_pattern_violations(stress_pattern):
        log(f"[DEBUG] æ£€æµ‹åˆ°æ ¼å¾‹è¿è§„: {stress_pattern}")
        return []
    
    meter_patterns = {
        'Ğ¥Ğ¾Ñ€ĞµĞ¹': ['é‡', 'è½»'],
        'Ğ¯Ğ¼Ğ±': ['è½»', 'é‡'],
        'Ğ”Ğ°ĞºÑ‚Ğ¸Ğ»ÑŒ': ['é‡', 'è½»', 'è½»'],
        'ĞĞ¼Ñ„Ğ¸Ğ±Ñ€Ğ°Ñ…Ğ¸Ğ¹': ['è½»', 'é‡', 'è½»'],
        'ĞĞ½Ğ°Ğ¿ĞµÑÑ‚': ['è½»', 'è½»', 'é‡']
    }
    
    matches = []
    for name, pattern in meter_patterns.items():
        score = _meter_calc_score(stress_pattern, pattern)
        if score >= 0.80:
            matches.append({'meter': name, 'confidence': score})
            log(f"[DEBUG] æ ¼å¾‹ {name} åŒ¹é…åˆ†æ•°: {score:.1%}")
    
    return sorted(matches, key=lambda x: x['confidence'], reverse=True)


def _meter_calc_score(stress_pattern, meter_pattern):
    """è®¡ç®—æ ¼å¾‹åŒ¹é…åˆ†æ•°"""
    total = len(stress_pattern)
    if total == 0:
        return 0.0
    
    match = 0
    p_len = len(meter_pattern)
    penalty = 0
    
    for i, actual in enumerate(stress_pattern):
        expected = meter_pattern[i % p_len]
        
        if actual == expected:
            match += 1.0
        else:
            if i == 0:
                match += 0.5
            elif i == total - 1:
                match += 0.4
            else:
                match += 0.2
                penalty += 0.3
    
    base_score = match / total
    final_score = base_score - (penalty / total)
    
    return max(0.0, final_score)


def _meter_determine(line_analyses, log):
    """åˆ¤æ–­è¯—æ­Œçš„ä¸»å¯¼æ ¼å¾‹"""
    if not line_analyses:
        return 'Unknown', {'matched_lines': 0, 'total_lines': 0}
    
    meter_votes = defaultdict(list)
    total_lines = len(line_analyses)
    
    for analysis in line_analyses:
        matches = analysis.get('meter_matches', [])
        if matches:
            best = matches[0]
            if best['confidence'] >= 0.80:
                meter_votes[best['meter']].append(best['confidence'])
    
    if not meter_votes:
        return 'Unknown', {
            'method': 'no_matches',
            'matched_lines': 0,
            'total_lines': total_lines
        }
    
    best_meter = None
    best_score = 0
    
    for meter, confidences in meter_votes.items():
        line_count = len(confidences)
        avg_conf = sum(confidences) / line_count
        coverage = line_count / total_lines
        score = coverage * avg_conf
        
        if coverage >= 0.80 and avg_conf >= 0.80 and score > best_score:
            best_score = score
            best_meter = meter
    
    if not best_meter:
        for meter, confidences in meter_votes.items():
            line_count = len(confidences)
            avg_conf = sum(confidences) / line_count
            coverage = line_count / total_lines
            
            if coverage >= 0.75 and avg_conf >= 0.75:
                return meter, {
                    'method': 'partial_match',
                    'matched_lines': line_count,
                    'total_lines': total_lines
                }
        
        return 'Unknown', {
            'method': 'insufficient_match',
            'matched_lines': 0,
            'total_lines': total_lines
        }
    
    confidences = meter_votes[best_meter]
    return best_meter, {
        'method': 'strict_analysis',
        'matched_lines': len(confidences),
        'total_lines': total_lines
    }





# ==================== è§„åˆ™ 6: å•å¤æ•°è¯­ä¹‰å·®å¼‚å¯¹æ£€æµ‹ ====================
def detect_russian_singular_plural_semantic_pairs(content_list, required_pairs, debug=False):
    """
    åŠ¨æ€æ£€æµ‹ä¿„è¯­æ–‡æœ¬ä¸­å•å¤æ•°è¯­ä¹‰å·®å¼‚åè¯å¯¹
    
    æ”¹è¿›ç‰ˆæœ¬ï¼šæ›´ä¸¥æ ¼åœ°åŒºåˆ†çœŸæ­£çš„è¯­ä¹‰å·®å¼‚å’Œæ™®é€šè¯­æ³•å˜åŒ–
    
    Args:
        content_list: æ–‡æœ¬å†…å®¹åˆ—è¡¨
        required_pairs: è¦æ±‚çš„å•å¤æ•°è¯­ä¹‰å·®å¼‚å¯¹æ•°é‡
        debug: æ˜¯å¦è¾“å‡ºè°ƒè¯•ä¿¡æ¯
    
    Returns:
        tuple: (1/0, è¯´æ˜ä¿¡æ¯)
    """
    log = create_logger(debug)
    log(f"[DEBUG] å¼€å§‹åŠ¨æ€æ£€æµ‹å•å¤æ•°è¯­ä¹‰å·®å¼‚å¯¹, required_pairs={required_pairs}")
    
    # è¾“å…¥éªŒè¯
    if content_list == "INVALID" or content_list is None:
        return 0, "âŒ è¾“å…¥æ–‡æœ¬æ— æ•ˆ"
    
    try:
        required_pairs = int(required_pairs)
    except (ValueError, TypeError):
        return 0, f"âŒ required_pairs å¿…é¡»æ˜¯æ•´æ•°: '{required_pairs}'"

    try:
        # æ–‡æœ¬é¢„å¤„ç†
        if isinstance(content_list, list):
            text = ' '.join(str(item) for item in content_list if item and str(item) != "INVALID")
        else:
            text = str(content_list)
        
        if not text.strip():
            return 1 if required_pairs == 0 else 0, "âœ… å†…å®¹ä¸ºç©º" if required_pairs == 0 else "âŒ å†…å®¹ä¸ºç©º"
        
        log(f"[DEBUG] å¤„ç†æ–‡æœ¬é•¿åº¦: {len(text)}")
        
        # æ­¥éª¤1: æ£€æŸ¥å·²çŸ¥çš„è¯­ä¹‰å·®å¼‚å¯¹
        known_pairs = _sp_check_known_pairs(text, log)
        log(f"[DEBUG] å‘ç° {len(known_pairs)} ä¸ªå·²çŸ¥è¯­ä¹‰å·®å¼‚å¯¹")
        
        # æ­¥éª¤2: æå–æ½œåœ¨åè¯
        potential_nouns = _sp_extract_nouns(text, log)
        log(f"[DEBUG] æå–åˆ° {len(potential_nouns)} ä¸ªæ½œåœ¨åè¯")
        
        # æ­¥éª¤3: è¯†åˆ«å•å¤æ•°å…³ç³»
        sg_pl_pairs = _sp_identify_relationships(potential_nouns, text, log)
        log(f"[DEBUG] è¯†åˆ«åˆ° {len(sg_pl_pairs)} ä¸ªå•å¤æ•°å…³ç³»")
        
        # æ­¥éª¤4: ä¸¥æ ¼åˆ†æè¯­ä¹‰å·®å¼‚
        dynamic_pairs = _sp_analyze_semantics(sg_pl_pairs, text, log)
        log(f"[DEBUG] åŠ¨æ€å‘ç° {len(dynamic_pairs)} ä¸ªè¯­ä¹‰å·®å¼‚å¯¹")
        
        # æ­¥éª¤5: åˆå¹¶ç»“æœ
        all_pairs = _sp_merge_deduplicate(known_pairs, dynamic_pairs, log)
        total_found = len(all_pairs)
        
        # æ„å»ºç»“æœè¯´æ˜
        if all_pairs:
            descriptions = []
            for pair in all_pairs:
                sg_info = f"{pair['singular']}({pair['singular_context']})"
                pl_info = f"{pair['plural']}({pair['plural_context']})"
                conf = f"ç½®ä¿¡åº¦:{pair['confidence']:.2f}"
                descriptions.append(f"{sg_info} vs {pl_info} [{conf}]")
            
            pairs_text = "; ".join(descriptions[:3])
            if len(descriptions) > 3:
                pairs_text += f" ç­‰{total_found}ç»„"
        else:
            pairs_text = "æ— "
        
        # åˆ¤æ–­ç»“æœ
        if total_found == required_pairs:
            return 1, f"âœ… æ‰¾åˆ°æ°å¥½ {total_found} ç»„å•å¤æ•°è¯­ä¹‰å·®å¼‚å¯¹ (è¦æ±‚={required_pairs}ç»„): {pairs_text}"
        else:
            return 0, f"âŒ æ‰¾åˆ° {total_found} ç»„å•å¤æ•°è¯­ä¹‰å·®å¼‚å¯¹ (è¦æ±‚={required_pairs}ç»„): {pairs_text}"

    except Exception as e:
        log(f"[DEBUG] å¼‚å¸¸: {e}")
        if debug:
            import traceback
            traceback.print_exc()
        return 0, f"âŒ å‡½æ•°æ‰§è¡Œå¼‚å¸¸: {e}"


def _sp_check_known_pairs(text, log):
    """æ£€æŸ¥å·²çŸ¥çš„è¯­ä¹‰å·®å¼‚å¯¹"""
    known = {
        ('ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ', 'ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸'): {
            'singular_meaning': 'æŠ½è±¡èƒ½åŠ›',
            'plural_meaning': 'å…·ä½“æ‰èƒ½',
            'confidence': 0.95
        },
        ('Ğ²Ğ»Ğ°ÑÑ‚ÑŒ', 'Ğ²Ğ»Ğ°ÑÑ‚Ğ¸'): {
            'singular_meaning': 'æƒåŠ›æ¦‚å¿µ',
            'plural_meaning': 'å½“å±€æœºæ„',
            'confidence': 0.95
        },
        ('Ğ±ÑƒĞ¼Ğ°Ğ³Ğ°', 'Ğ±ÑƒĞ¼Ğ°Ğ³Ğ¸'): {
            'singular_meaning': 'çº¸å¼ ææ–™',
            'plural_meaning': 'æ–‡ä»¶èµ„æ–™',
            'confidence': 0.90
        },
        ('Ğ²Ñ€ĞµĞ¼Ñ', 'Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ°'): {
            'singular_meaning': 'æ—¶é—´æ¦‚å¿µ',
            'plural_meaning': 'æ—¶ä»£æ—¶æœŸ',
            'confidence': 0.90
        },
        ('Ğ´ĞµĞ»Ğ¾', 'Ğ´ĞµĞ»Ğ°'): {
            'singular_meaning': 'äº‹æƒ…æ¦‚å¿µ',
            'plural_meaning': 'å…·ä½“äº‹åŠ¡',
            'confidence': 0.85
        },
    }
    
    found = []
    text_lower = text.lower()
    
    for (singular, plural), info in known.items():
        if re.search(r'\b' + re.escape(singular) + r'\b', text_lower) and \
           re.search(r'\b' + re.escape(plural) + r'\b', text_lower):
            
            sg_contexts = _sp_get_contexts(text_lower, singular)
            pl_contexts = _sp_get_contexts(text_lower, plural)
            
            if _sp_validate_difference(sg_contexts, pl_contexts, singular, plural, log):
                if (singular, plural) == ('Ğ²Ğ»Ğ°ÑÑ‚ÑŒ', 'Ğ²Ğ»Ğ°ÑÑ‚Ğ¸'):
                    if _sp_check_contextual_diff(text_lower, singular, plural, 
                                                  ['Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ°', 'ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ', 'Ğ³Ğ¾ÑÑƒĞ´Ğ°Ñ€ÑÑ‚Ğ²Ğ¾']):
                        found.append({
                            'singular': singular,
                            'plural': plural,
                            'singular_context': info['singular_meaning'],
                            'plural_context': info['plural_meaning'],
                            'confidence': info['confidence'],
                            'evidence': 'å·²çŸ¥è¯­ä¹‰å·®å¼‚å¯¹'
                        })
                        log(f"[DEBUG] å‘ç°å·²çŸ¥è¯­ä¹‰å·®å¼‚å¯¹: {singular} vs {plural}")
                else:
                    found.append({
                        'singular': singular,
                        'plural': plural,
                        'singular_context': info['singular_meaning'],
                        'plural_context': info['plural_meaning'],
                        'confidence': info['confidence'],
                        'evidence': 'å·²çŸ¥è¯­ä¹‰å·®å¼‚å¯¹'
                    })
                    log(f"[DEBUG] å‘ç°å·²çŸ¥è¯­ä¹‰å·®å¼‚å¯¹: {singular} vs {plural}")
    
    return found


def _sp_extract_nouns(text, log):
    """æå–æ½œåœ¨åè¯"""
    patterns = [
        r'\b[Ğ°-ÑÑ‘Ğ-Ğ¯Ğ]+(?:Ğ¾ÑÑ‚ÑŒ|Ğ½Ğ¸Ğµ|Ñ†Ğ¸Ñ|ÑĞ¸Ñ|Ñ‚Ğ¸Ğµ|ÑÑ‚Ğ²Ğ¾|ĞµĞ½Ğ¸Ğµ|Ğ°Ğ½Ğ¸Ğµ)\b',
        r'\b[Ğ°-ÑÑ‘Ğ-Ğ¯Ğ]+(?:Ñ‚ĞµĞ»ÑŒ|Ğ½Ğ¸Ğº|Ñ‰Ğ¸Ğº|Ñ‡Ğ¸Ğº|Ğ°Ñ€ÑŒ|Ñ‹Ñ€ÑŒ)\b',
        r'\b[Ğ°-ÑÑ‘Ğ-Ğ¯Ğ]+(?:ĞºĞ°|Ğ³Ğ°|Ğ±Ğ°|Ğ¿Ğ°|Ñ‚Ğ°|Ğ´Ğ°|Ğ·Ğ°|ÑĞ°|Ñ€Ğ°|Ğ»Ğ°|Ğ½Ğ°|Ğ¼Ğ°)\b',
        r'\b[Ğ°-ÑÑ‘Ğ-Ğ¯Ğ]+[Ğ°-ÑÑ‘]{3,}\b'
    ]
    
    nouns = set()
    text_lower = text.lower()
    
    for pattern in patterns:
        nouns.update(re.findall(pattern, text_lower))
    
    # æ’é™¤è¯
    excluded = {
        'ÑÑ‚Ğ¾', 'Ñ‡Ñ‚Ğ¾', 'ĞºĞ°Ğº', 'Ğ³Ğ´Ğµ', 'ĞºĞ¾Ğ³Ğ´Ğ°', 'Ğ¿Ğ¾Ñ‡ĞµĞ¼Ñƒ', 'Ğ·Ğ°Ñ‡ĞµĞ¼', 'Ğ¾Ñ‚ĞºÑƒĞ´Ğ°', 'ĞºÑƒĞ´Ğ°',
        'ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹', 'ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ', 'ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğµ', 'ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ', 'Ñ‚Ğ°ĞºĞ¾Ğ¹', 'Ñ‚Ğ°ĞºĞ°Ñ', 'Ñ‚Ğ°ĞºĞ¾Ğµ', 'Ñ‚Ğ°ĞºĞ¸Ğµ',
        'ÑÑ‚Ğ¾Ñ‚', 'ÑÑ‚Ğ°', 'ÑÑ‚Ğ¾', 'ÑÑ‚Ğ¸', 'Ñ‚Ğ¾Ñ‚', 'Ñ‚Ğ°', 'Ñ‚Ğ¾', 'Ñ‚Ğµ', 'Ğ¼ĞµÑÑ‚Ğ¾', 'Ğ¼ĞµÑÑ‚Ğ°',
        'Ğ¼Ğ¾Ğ¹', 'Ğ¼Ğ¾Ñ', 'Ğ¼Ğ¾Ñ‘', 'Ğ¼Ğ¾Ğ¸', 'Ñ‚Ğ²Ğ¾Ğ¹', 'Ñ‚Ğ²Ğ¾Ñ', 'Ñ‚Ğ²Ğ¾Ñ‘', 'Ñ‚Ğ²Ğ¾Ğ¸',
        'ĞµĞ³Ğ¾', 'ĞµÑ‘', 'Ğ¸Ñ…', 'Ğ½Ğ°Ñˆ', 'Ğ½Ğ°ÑˆĞ°', 'Ğ½Ğ°ÑˆĞµ', 'Ğ½Ğ°ÑˆĞ¸', 'Ğ²Ğ°Ñˆ', 'Ğ²Ğ°ÑˆĞ°', 'Ğ²Ğ°ÑˆĞµ', 'Ğ²Ğ°ÑˆĞ¸',
        'Ğ¶Ğ¸Ğ·Ğ½ÑŒ', 'Ğ¶Ğ¸Ğ·Ğ½Ğ¸', 'Ğ¶Ğ¸Ğ·Ğ½ÑŒÑ', 'Ğ¶Ğ¸Ğ·Ğ½ÑĞ¼'
    }
    
    function_words = {
        'Ğ²', 'Ğº', 'Ñ', 'Ğ½Ğ°', 'Ğ·Ğ°', 'Ğ¿Ğ¾Ğ´', 'Ğ½Ğ°Ğ´', 'Ğ¾Ñ‚', 'Ğ´Ğ¾', 'Ğ¿Ğ¾', 'Ğ¿Ñ€Ğ¸', 'Ğ¿Ñ€Ğ¾',
        'Ğ±ĞµĞ·', 'Ğ´Ğ»Ñ', 'Ñ‡ĞµÑ€ĞµĞ·', 'Ğ¼ĞµĞ¶Ğ´Ñƒ', 'Ğ¸', 'Ğ°', 'Ğ½Ğ¾', 'Ğ¸Ğ»Ğ¸', 'Ğ½Ğµ', 'Ğ¶Ğµ', 'Ğ»Ğ¸', 'Ğ±Ñ‹'
    }
    
    filtered = [n for n in nouns if n not in excluded and len(n) > 3 and n not in function_words]
    
    return filtered


def _sp_identify_relationships(nouns, text, log):
    """è¯†åˆ«å•å¤æ•°å…³ç³»"""
    pairs = []
    text_lower = text.lower()
    
    # å¤æ•°è§„åˆ™
    plural_rules = [
        (r'([Ğ°-ÑÑ‘]+)Ğ¾ÑÑ‚ÑŒ$', r'\1Ğ¾ÑÑ‚Ğ¸'),
        (r'([Ğ°-ÑÑ‘]+)Ğ½Ğ¸Ğµ$', r'\1Ğ½Ğ¸Ñ'),
        (r'([Ğ°-ÑÑ‘]*[Ğ±Ğ²Ğ³Ğ´Ğ¶Ğ·ĞºĞ»Ğ¼Ğ½Ğ¿Ñ€ÑÑ‚Ñ„Ñ…Ñ†Ñ‡ÑˆÑ‰])$', r'\1Ñ‹'),
        (r'([Ğ°-ÑÑ‘]*[Ğ¶Ñ‡ÑˆÑ‰])$', r'\1Ğ¸'),
        (r'([Ğ°-ÑÑ‘]+)ÑŒ$', r'\1Ğ¸'),
        (r'([Ğ°-ÑÑ‘]+)Ğ¾$', r'\1Ğ°'),
        (r'([Ğ°-ÑÑ‘]+)Ğµ$', r'\1Ñ'),
        (r'([Ğ°-ÑÑ‘]+)Ğ°$', r'\1Ñ‹'),
        (r'([Ğ°-ÑÑ‘]+)Ñ$', r'\1Ğ¸'),
    ]
    
    noun_set = set(re.findall(r'\b[Ğ°-ÑÑ‘]+\b', text_lower))
    
    for noun in nouns:
        for sg_pattern, pl_replacement in plural_rules:
            if re.match(sg_pattern, noun):
                potential_plural = re.sub(sg_pattern, pl_replacement, noun)
                if potential_plural in noun_set and potential_plural != noun:
                    if _sp_check_contextual_difference(text_lower, noun, potential_plural):
                        pairs.append({
                            'singular': noun,
                            'plural': potential_plural,
                            'confidence': 0.75
                        })
                        log(f"[DEBUG] è¯†åˆ«åˆ°å•å¤æ•°å…³ç³»: {noun} -> {potential_plural}")
                    break
    
    return pairs


def _sp_analyze_semantics(pairs, text, log):
    """ä¸¥æ ¼åˆ†æè¯­ä¹‰å·®å¼‚"""
    semantic_pairs = []
    text_lower = text.lower()
    
    for pair in pairs:
        singular = pair['singular']
        plural = pair['plural']
        
        log(f"[DEBUG] ä¸¥æ ¼åˆ†æè¯­ä¹‰å·®å¼‚: {singular} vs {plural}")
        
        sg_contexts = _sp_get_contexts(text_lower, singular)
        pl_contexts = _sp_get_contexts(text_lower, plural)
        
        if not sg_contexts or not pl_contexts:
            continue
        
        analysis = _sp_compare_contexts(sg_contexts, pl_contexts, singular, plural)
        
        if analysis['has_semantic_difference']:
            semantic_pairs.append({
                'singular': singular,
                'plural': plural,
                'singular_context': analysis['singular_meaning'],
                'plural_context': analysis['plural_meaning'],
                'confidence': analysis['confidence'],
                'evidence': analysis['evidence']
            })
            
            log(f"[DEBUG] å‘ç°è¯­ä¹‰å·®å¼‚: {singular}({analysis['singular_meaning']}) vs {plural}({analysis['plural_meaning']})")
    
    return semantic_pairs


def _sp_get_contexts(text, word, window=5):
    """è·å–è¯æ±‡ä¸Šä¸‹æ–‡"""
    contexts = []
    words = re.split(r'\s+', text)
    
    for i, w in enumerate(words):
        if w == word:
            start = max(0, i - window)
            end = min(len(words), i + window + 1)
            context = ' '.join(words[start:end])
            contexts.append(context.strip())
    
    return contexts


def _sp_validate_difference(sg_contexts, pl_contexts, sg_word, pl_word, log):
    """éªŒè¯ä¸Šä¸‹æ–‡å·®å¼‚"""
    # ç‰¹åˆ«å¤„ç†"Ğ¼ĞµÑÑ‚Ğ¾/Ğ¼ĞµÑÑ‚Ğ°"
    if sg_word == 'Ğ¼ĞµÑÑ‚Ğ¾' and pl_word == 'Ğ¼ĞµÑÑ‚Ğ°':
        if _sp_check_same_context(sg_contexts, pl_contexts):
            log(f"[DEBUG] æ’é™¤è¯¯åˆ¤: {sg_word} vs {pl_word} å‡ºç°åœ¨ç›¸åŒä¸Šä¸‹æ–‡ä¸­")
            return False
    
    # æ£€æŸ¥æ‰€æœ‰æ ¼ç»“æ„
    if _sp_check_possessives(sg_contexts, pl_contexts):
        log(f"[DEBUG] æ’é™¤è¯¯åˆ¤: {sg_word} vs {pl_word} å‡ºç°åœ¨æ‰€æœ‰æ ¼ç»“æ„ä¸­")
        return False
    
    return True


def _sp_check_same_context(sg_contexts, pl_contexts):
    """æ£€æŸ¥æ˜¯å¦åœ¨ç›¸åŒä¸Šä¸‹æ–‡"""
    for sg_ctx in sg_contexts:
        for pl_ctx in pl_contexts:
            if _sp_context_similarity(sg_ctx, pl_ctx) > 0.8:
                return True
    return False


def _sp_context_similarity(ctx1, ctx2):
    """è®¡ç®—ä¸Šä¸‹æ–‡ç›¸ä¼¼åº¦"""
    words1 = set(re.findall(r'\w+', ctx1))
    words2 = set(re.findall(r'\w+', ctx2))
    
    if not words1 or not words2:
        return 0.0
    
    common = words1.intersection(words2)
    total = words1.union(words2)
    
    return len(common) / len(total) if total else 0.0


def _sp_check_possessives(sg_contexts, pl_contexts):
    """æ£€æŸ¥æ‰€æœ‰æ ¼ç»“æ„"""
    possessive_patterns = [
        r'\b(?:Ğ¼Ğ¾Ğ¹|Ğ¼Ğ¾Ñ|Ğ¼Ğ¾Ñ‘|Ğ¼Ğ¾Ğ¸|Ñ‚Ğ²Ğ¾Ğ¹|Ñ‚Ğ²Ğ¾Ñ|Ñ‚Ğ²Ğ¾Ñ‘|Ñ‚Ğ²Ğ¾Ğ¸|ĞµĞ³Ğ¾|ĞµÑ‘|Ğ¸Ñ…|Ğ½Ğ°Ñˆ|Ğ½Ğ°ÑˆĞ°|Ğ½Ğ°ÑˆĞµ|Ğ½Ğ°ÑˆĞ¸|Ğ²Ğ°Ñˆ|Ğ²Ğ°ÑˆĞ°|Ğ²Ğ°ÑˆĞµ|Ğ²Ğ°ÑˆĞ¸)\b',
        r'\b(?:ÑÑ‚Ğ¾Ñ‚|ÑÑ‚Ğ°|ÑÑ‚Ğ¾|ÑÑ‚Ğ¸|Ñ‚Ğ¾Ñ‚|Ñ‚Ğ°|Ñ‚Ğ¾|Ñ‚Ğµ)\b'
    ]
    
    pattern = '|'.join(possessive_patterns)
    sg_has = any(re.search(pattern, ctx) for ctx in sg_contexts)
    pl_has = any(re.search(pattern, ctx) for ctx in pl_contexts)
    
    return sg_has and pl_has


def _sp_check_contextual_difference(text, singular, plural):
    """æ£€æŸ¥å•å¤æ•°æ˜¯å¦å‡ºç°åœ¨ä¸åŒè¯­ä¹‰ä¸Šä¸‹æ–‡"""
    sg_contexts = _sp_get_contexts(text, singular)
    pl_contexts = _sp_get_contexts(text, plural)
    
    for sg_ctx in sg_contexts:
        for pl_ctx in pl_contexts:
            if _sp_context_similarity(sg_ctx, pl_ctx) < 0.4:
                return True
    
    return False


def _sp_check_contextual_diff(text, singular, plural, context_keywords):
    """æ£€æŸ¥æ˜¯å¦å‡ºç°åœ¨ä¸åŒè¯­ä¹‰ä¸Šä¸‹æ–‡ä¸­"""
    pattern = '|'.join(re.escape(k) for k in context_keywords)
    sg_proximity = bool(re.search(rf'\b{re.escape(singular)}\b.{{0,50}}(?:{pattern})', text))
    pl_proximity = bool(re.search(rf'\b{re.escape(plural)}\b.{{0,50}}(?:{pattern})', text))
    
    return sg_proximity and pl_proximity


def _sp_compare_contexts(sg_contexts, pl_contexts, sg_word, pl_word):
    """æ¯”è¾ƒè¯­ä¹‰ä¸Šä¸‹æ–‡"""
    indicators = {
        'abstract_concepts': ['Ğ¿Ğ¾Ğ½ÑÑ‚Ğ¸Ğµ', 'Ğ¸Ğ´ĞµÑ', 'ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ñ', 'Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿', 'Ñ‚ĞµĞ¾Ñ€Ğ¸Ñ', 'Ñ„Ğ¸Ğ»Ğ¾ÑĞ¾Ñ„Ğ¸Ñ', 'Ğ¼Ñ‹ÑĞ»ÑŒ'],
        'concrete_objects': ['Ğ¿Ñ€ĞµĞ´Ğ¼ĞµÑ‚', 'Ğ²ĞµÑ‰ÑŒ', 'Ğ¾Ğ±ÑŠĞµĞºÑ‚', 'Ğ¼Ğ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ»', 'Ğ¸Ğ·Ğ´ĞµĞ»Ğ¸Ğµ', 'Ñ‚Ğ¾Ğ²Ğ°Ñ€', 'Ğ¿Ñ€Ğ¾Ğ´ÑƒĞºÑ‚'],
        'actions': ['Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ', 'Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ', 'Ğ´ĞµÑÑ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ', 'Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°', 'Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ñ', 'Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ'],
        'qualities': ['ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾', 'ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ²Ğ¾', 'Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ¸ÑÑ‚Ğ¸ĞºĞ°', 'Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ', 'Ñ‡ĞµÑ€Ñ‚Ğ°'],
        'institutions': ['Ğ¾Ñ€Ğ³Ğ°Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ', 'ÑƒÑ‡Ñ€ĞµĞ¶Ğ´ĞµĞ½Ğ¸Ğµ', 'Ğ¸Ğ½ÑÑ‚Ğ¸Ñ‚ÑƒÑ‚', 'Ğ¾Ñ€Ğ³Ğ°Ğ½', 'ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ°', 'ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°'],
        'collections': ['Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğ¾', 'ÑĞ¾Ğ²Ğ¾ĞºÑƒĞ¿Ğ½Ğ¾ÑÑ‚ÑŒ', 'Ğ³Ñ€ÑƒĞ¿Ğ¿Ğ°', 'Ğ½Ğ°Ğ±Ğ¾Ñ€', 'ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑ', 'Ñ€ÑĞ´'],
        'skills': ['ÑƒĞ¼ĞµĞ½Ğ¸Ğµ', 'Ğ½Ğ°Ğ²Ñ‹Ğº', 'Ğ¼Ğ°ÑÑ‚ĞµÑ€ÑÑ‚Ğ²Ğ¾', 'Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²Ğ¾', 'Ñ‚Ğ°Ğ»Ğ°Ğ½Ñ‚', 'Ğ´Ğ°Ñ€'],
        'documents': ['Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚', 'ÑĞ¿Ñ€Ğ°Ğ²ĞºĞ°', 'Ğ±ÑƒĞ¼Ğ°Ğ³Ğ°', 'Ğ·Ğ°ÑĞ²Ğ»ĞµĞ½Ğ¸Ğµ', 'Ğ¾Ñ‚Ñ‡ĞµÑ‚', 'Ğ°ĞºÑ‚'],
        'temporal': ['Ğ²Ñ€ĞµĞ¼Ñ', 'Ğ¿ĞµÑ€Ğ¸Ğ¾Ğ´', 'ÑĞ¿Ğ¾Ñ…Ğ°', 'Ğ¼Ğ¾Ğ¼ĞµĞ½Ñ‚', 'Ñ‡Ğ°Ñ', 'Ğ´ĞµĞ½ÑŒ'],
        'spatial': ['Ğ¼ĞµÑÑ‚Ğ¾', 'Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾', 'Ñ‚ĞµÑ€Ñ€Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ñ', 'Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑŒ', 'Ğ·Ğ¾Ğ½Ğ°']
    }
    
    sg_scores = defaultdict(float)
    pl_scores = defaultdict(float)
    
    for ctx in sg_contexts:
        for category, keywords in indicators.items():
            for kw in keywords:
                if kw in ctx:
                    sg_scores[category] += 1
    
    for ctx in pl_contexts:
        for category, keywords in indicators.items():
            for kw in keywords:
                if kw in ctx:
                    pl_scores[category] += 1
    
    semantic_diff = _sp_calc_semantic_diff(sg_scores, pl_scores)
    has_diff = semantic_diff > 0.7
    confidence = min(semantic_diff * 1.2, 0.95)
    
    # ç‰¹åˆ«å¤„ç†
    if (sg_word, pl_word) in [('Ğ²Ğ»Ğ°ÑÑ‚ÑŒ', 'Ğ²Ğ»Ğ°ÑÑ‚Ğ¸'), ('Ğ±ÑƒĞ¼Ğ°Ğ³Ğ°', 'Ğ±ÑƒĞ¼Ğ°Ğ³Ğ¸'), ('Ğ²Ñ€ĞµĞ¼Ñ', 'Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ°')]:
        if _sp_check_same_context(sg_contexts, pl_contexts):
            has_diff = False
            confidence = 0.0
    
    sg_meaning = _sp_infer_meaning(sg_contexts, sg_word, sg_scores)
    pl_meaning = _sp_infer_meaning(pl_contexts, pl_word, pl_scores)
    
    if has_diff:
        if sg_meaning == pl_meaning or 'å«ä¹‰ä¸æ˜' in [sg_meaning, pl_meaning]:
            has_diff = False
            confidence = 0.0
    
    return {
        'has_semantic_difference': has_diff,
        'confidence': confidence,
        'singular_meaning': sg_meaning,
        'plural_meaning': pl_meaning,
        'evidence': f'è¯­ä¹‰å·®å¼‚åº¦: {semantic_diff:.2f}'
    }


def _sp_calc_semantic_diff(scores1, scores2):
    """è®¡ç®—è¯­ä¹‰å·®å¼‚åº¦"""
    all_categories = set(scores1.keys()) | set(scores2.keys())
    
    if not all_categories:
        return 0.0
    
    diffs = []
    for category in all_categories:
        s1 = scores1.get(category, 0)
        s2 = scores2.get(category, 0)
        total = s1 + s2
        
        if total > 0:
            diff = abs(s1 - s2) / total
            diffs.append(diff)
    
    return sum(diffs) / len(diffs) if diffs else 0.0


def _sp_infer_meaning(contexts, word, scores):
    """æ¨æ–­å«ä¹‰"""
    if not scores:
        return "å«ä¹‰ä¸æ˜"
    
    top_category = max(scores.items(), key=lambda x: x[1])
    if top_category[1] == 0:
        return "å«ä¹‰ä¸æ˜"
    
    meaning_map = {
        'abstract_concepts': 'æŠ½è±¡æ¦‚å¿µ',
        'concrete_objects': 'å…·ä½“ç‰©å“',
        'actions': 'è¡Œä¸ºåŠ¨ä½œ',
        'qualities': 'å“è´¨ç‰¹å¾',
        'institutions': 'æœºæ„ç»„ç»‡',
        'collections': 'é›†åˆæ¦‚å¿µ',
        'skills': 'æŠ€èƒ½æ‰èƒ½',
        'documents': 'æ–‡ä»¶èµ„æ–™',
        'temporal': 'æ—¶é—´æ¦‚å¿µ',
        'spatial': 'ç©ºé—´æ¦‚å¿µ'
    }
    
    return meaning_map.get(top_category[0], 'ä¸€èˆ¬æ¦‚å¿µ')


def _sp_merge_deduplicate(known, dynamic, log):
    """åˆå¹¶å¹¶å»é‡"""
    all_pairs = []
    seen = set()
    
    for pair in known:
        key = (pair['singular'], pair['plural'])
        if key not in seen:
            seen.add(key)
            all_pairs.append(pair)
    
    for pair in dynamic:
        key = (pair['singular'], pair['plural'])
        if key not in seen:
            if key == ('Ğ¼ĞµÑÑ‚Ğ¾', 'Ğ¼ĞµÑÑ‚Ğ°'):
                log("[DEBUG] æ’é™¤è¯¯åˆ¤: Ğ¼ĞµÑÑ‚Ğ¾/Ğ¼ĞµÑÑ‚Ğ°")
                continue
            seen.add(key)
            all_pairs.append(pair)
    
    return all_pairs


# ==================== è§„åˆ™ 7: å¤šç§å¤æ•°å½¢å¼æ£€æµ‹ ====================
def detect_russian_multiple_plural_forms_enhanced(content_list, required_pairs, debug=False):
    """
    æ£€æµ‹ä¸€ä¸ªåè¯çš„ä¸¤ç§ä¸åŒå«ä¹‰çš„å¤æ•°å½¢å¼
    ç»“åˆ"å·²çŸ¥çŸ¥è¯†åº“"å’Œ"åŠ¨æ€å‘ç°"ä¸¤ç§ç­–ç•¥
    
    Args:
        content_list: æ–‡æœ¬å†…å®¹åˆ—è¡¨
        required_pairs: è¦æ±‚çš„å¤æ•°å½¢å¼å¯¹æ•°é‡
        debug: æ˜¯å¦è¾“å‡ºè°ƒè¯•ä¿¡æ¯
    
    Returns:
        tuple: (1/0, è¯´æ˜ä¿¡æ¯)
    """
    log = create_logger(debug)
    log(f"[DEBUG] å¼€å§‹æ£€æµ‹, required_pairs={required_pairs}")

    # è¾“å…¥éªŒè¯
    if not content_list or content_list == "INVALID":
        return 0, "âŒ è¾“å…¥æ–‡æœ¬æ— æ•ˆ"
    
    try:
        required_pairs = int(required_pairs)
    except (ValueError, TypeError):
        return 0, "âŒ required_pairs å¿…é¡»æ˜¯æ•´æ•°"

    text = ' '.join(str(item) for item in content_list if item).lower()
    if not text.strip():
        return 1 if required_pairs == 0 else 0, "âœ… å†…å®¹ä¸ºç©º"

    # ç­–ç•¥1: æ£€æŸ¥å·²çŸ¥çŸ¥è¯†åº“
    known_pairs = _mpl_check_known(text, log)
    log(f"[DEBUG] ç­–ç•¥1(çŸ¥è¯†åº“)å‘ç° {len(known_pairs)} å¯¹")

    # ç­–ç•¥2: åŠ¨æ€å‘ç°
    dynamic_pairs = []
    morph, available = LibraryManager.get_morph()
    
    if available:
        log("[INFO] Pymorphy2 åº“åŠ è½½æˆåŠŸï¼Œå°†ç”¨äºåŠ¨æ€åˆ†æ")
        dynamic_pairs = _mpl_discover_dynamic(text, morph, log)
        log(f"[DEBUG] ç­–ç•¥2(åŠ¨æ€å‘ç°)å‘ç° {len(dynamic_pairs)} å¯¹")
    else:
        if debug:
            log("[WARNING] Pymorphy2 åº“æœªå®‰è£…ï¼ŒåŠ¨æ€å‘ç°åŠŸèƒ½å°†å—é™")

    # åˆå¹¶å»é‡
    all_found = known_pairs
    seen_bases = {p['base_noun'] for p in known_pairs}
    for d_pair in dynamic_pairs:
        if d_pair['base_noun'] not in seen_bases:
            all_found.append(d_pair)
            seen_bases.add(d_pair['base_noun'])
    
    # ç”Ÿæˆç»“æœ
    total = len(all_found)
    if all_found:
        descriptions = [f"'{p['form1']}'({p['meaning1']}) vs '{p['form2']}'({p['meaning2']}) [æ¥æº: {p['source']}]" 
                       for p in all_found]
        pairs_text = "; ".join(descriptions)
    else:
        pairs_text = "æ— "

    if total == required_pairs:
        return 1, f"âœ… æ‰¾åˆ°æ°å¥½ {total} ç»„å¤šç§å¤æ•°å½¢å¼å¯¹ (è¦æ±‚={required_pairs}): {pairs_text}"
    else:
        return 0, f"âŒ æ‰¾åˆ° {total} ç»„å¤šç§å¤æ•°å½¢å¼å¯¹ (è¦æ±‚={required_pairs}): {pairs_text}"


def _mpl_check_known(text, log):
    """ç­–ç•¥1ï¼šæ£€æŸ¥çŸ¥è¯†åº“"""
    knowledge_base = {
        'Ğ·ÑƒĞ±': {
            'plural1': {'form': 'Ğ·ÑƒĞ±Ñ‹', 'meaning': 'ç‰™é½¿(ç”Ÿç‰©)', 'context': ['ÑÑ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¾Ğ»Ğ¾Ğ³', 'Ğ²Ñ€Ğ°Ñ‡', 'Ñ‡Ğ¸ÑÑ‚Ğ¸Ñ‚ÑŒ', 'Ğ±Ğ¾Ğ»Ğ¸Ñ‚', 'Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞº', 'Ñ‡ĞµĞ»ÑÑÑ‚ÑŒ']},
            'plural2': {'form': 'Ğ·ÑƒĞ±ÑŒÑ', 'meaning': 'é”¯é½¿(å·¥å…·)', 'context': ['Ğ¿Ğ¸Ğ»Ñ‹', 'ÑˆĞµÑÑ‚ĞµÑ€Ğ½Ğ¸', 'Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼', 'Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚', 'Ğ³Ñ€ĞµĞ±Ğ½Ñ']}
        },
        'Ğ»Ğ¸ÑÑ‚': {
            'plural1': {'form': 'Ğ»Ğ¸ÑÑ‚Ñ‹', 'meaning': 'çº¸å¼ /æ¿æ', 'context': ['Ğ±ÑƒĞ¼Ğ°Ğ³Ğ¸', 'Ğ¼ĞµÑ‚Ğ°Ğ»Ğ»Ğ°', 'ĞºĞ½Ğ¸Ğ³Ğ¸', 'Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚', 'ÑÑ‚Ğ°Ğ»Ğ¸']},
            'plural2': {'form': 'Ğ»Ğ¸ÑÑ‚ÑŒÑ', 'meaning': 'æ ‘å¶', 'context': ['Ğ´ĞµÑ€ĞµĞ²Ğ°', 'Ğ¾ÑĞµĞ½ÑŒ', 'Ğ·ĞµĞ»Ñ‘Ğ½Ñ‹Ğµ', 'Ğ¶Ñ‘Ğ»Ñ‚Ñ‹Ğµ', 'Ñ€Ğ°ÑÑ‚ĞµĞ½Ğ¸Ğµ']}
        },
    }
    
    found = []
    for base_noun, info in knowledge_base.items():
        p1 = info['plural1']
        p2 = info['plural2']
        if re.search(r'\b' + p1['form'] + r'\b', text) and re.search(r'\b' + p2['form'] + r'\b', text):
            contexts1 = _mpl_get_contexts(text, p1['form'])
            contexts2 = _mpl_get_contexts(text, p2['form'])
            if any(c in ctx for ctx in contexts1 for c in p1['context']) and \
               any(c in ctx for ctx in contexts2 for c in p2['context']):
                found.append({
                    'base_noun': base_noun,
                    'form1': p1['form'],
                    'form2': p2['form'],
                    'meaning1': p1['meaning'],
                    'meaning2': p2['meaning'],
                    'source': 'çŸ¥è¯†åº“'
                })
                log(f"[DEBUG] çŸ¥è¯†åº“å‘ç°: {base_noun} -> {p1['form']} vs {p2['form']}")
    return found


def _mpl_discover_dynamic(text, morph, log):
    """ç­–ç•¥2ï¼šåŠ¨æ€å‘ç°"""
    words = set(re.findall(r'\b[Ğ°-ÑÑ‘-]{3,}\b', text))
    lemmas = defaultdict(list)

    # è¯å½¢è¿˜åŸ
    for word in words:
        parses = morph.parse(word)
        if parses:
            p = parses[0]
            if 'NOUN' in p.tag:
                lemmas[p.normal_form].append(word)

    found = []
    # æŸ¥æ‰¾å¤šä¸ªå¤æ•°å½¢å¼
    for base_noun, forms in lemmas.items():
        if len(forms) > 1:
            plurals = [f for f in forms if 'plur' in morph.parse(f)[0].tag]
            if len(plurals) > 1:
                form1, form2 = plurals[0], plurals[1]
                
                # ä½¿ç”¨ russtress è·å–é‡éŸ³
                stresser, available = LibraryManager.get_stresser()
                if available and stresser:
                    try:
                        stressed1 = stresser.stress(form1)
                        stressed2 = stresser.stress(form2)
                        
                        if stressed1 != stressed2 and 'Ì' in stressed1 and 'Ì' in stressed2:
                            log(f"[DEBUG] åŠ¨æ€å‘ç°: {base_noun} -> {form1}({stressed1}) vs {form2}({stressed2})")
                            found.append({
                                'base_noun': base_noun,
                                'form1': form1,
                                'form2': form2,
                                'meaning1': f"å¤æ•°å½¢å¼1({stressed1})",
                                'meaning2': f"å¤æ•°å½¢å¼2({stressed2})",
                                'source': 'åŠ¨æ€åˆ†æ'
                            })
                    except:
                        pass
    return found


def _mpl_get_contexts(text, word, window=5):
    """è·å–ä¸Šä¸‹æ–‡"""
    contexts = []
    for match in re.finditer(r'\b' + re.escape(word) + r'\b', text):
        pre = text[:match.start()].split()
        post = text[match.end():].split()
        start_words = pre[-window:]
        end_words = post[:window]
        contexts.append(' '.join(start_words + [word] + end_words))
    return contexts


# ==================== è§„åˆ™ 8: æ´¾ç”Ÿè¯æ£€æµ‹ ====================
def check_russian_derived_words(content_list, base_verb, required_count, debug=False):
    """
    æ£€æµ‹ç”±åŸºç¡€åŠ¨è¯æ´¾ç”Ÿå‡ºçš„ã€å…·æœ‰ä¸åŒå‰ç¼€çš„æ–°åŠ¨è¯
    
    Args:
        content_list: æ–‡æœ¬å†…å®¹åˆ—è¡¨
        base_verb: åŸºç¡€åŠ¨è¯ï¼ˆä¸å®šå¼ï¼‰
        required_count: è¦æ±‚çš„æ´¾ç”Ÿè¯æ•°é‡
        debug: æ˜¯å¦è¾“å‡ºè°ƒè¯•ä¿¡æ¯
    
    Returns:
        tuple: (1/0, è¯´æ˜ä¿¡æ¯)
    """
    log = create_logger(debug)
    
    # å‰ç½®æ£€æŸ¥
    morph, available = LibraryManager.get_morph()
    
    if not available:
        if debug:
            log("[WARNING] Pymorphy2 åº“æœªå®‰è£…ï¼Œæ­¤è§„åˆ™ä¸å¯ç”¨")
        return 0, "âŒ è§„åˆ™è¯„ä¼°å¤±è´¥: Pymorphy2 åº“æœªå®‰è£…ï¼Œæ— æ³•æ‰§è¡ŒåŠ¨è¯æ´¾ç”Ÿåˆ†æ"
    else:
        log("[INFO] Pymorphy2 åº“åŠ è½½æˆåŠŸ")
    
    if not content_list or content_list == "INVALID":
        return 0, "âŒ è¾“å…¥æ–‡æœ¬æ— æ•ˆ"
    
    try:
        required_count = int(required_count)
    except (ValueError, TypeError):
        return 0, f"âŒ 'required_count' å¿…é¡»æ˜¯æœ‰æ•ˆçš„æ•´æ•°ï¼Œä½†æ”¶åˆ°äº† '{required_count}'"

    text = ' '.join(str(item) for item in content_list if item).lower()
    if not text.strip():
        return 1 if required_count == 0 else 0, f"âŒ å†…å®¹ä¸ºç©º (è¦æ±‚æ‰¾åˆ° {required_count} ä¸ªæ´¾ç”Ÿè¯)"

    # å‡†å¤‡åŸºç¡€åŠ¨è¯çš„è¯æ ¹
    base_verb_lower = base_verb.lower()
    
    # è¯æ ¹æ˜ å°„è¡¨
    verb_root_map = {
        "Ğ¸Ğ´Ñ‚Ğ¸": ["Ñ…Ğ¾Ğ´", "Ğ¹Ğ´", "ÑˆĞµĞ´"],
        "ĞµÑ…Ğ°Ñ‚ÑŒ": ["ĞµĞ·Ğ¶", "ĞµÑ…"],
        "Ğ±Ñ€Ğ°Ñ‚ÑŒ": ["Ğ±ĞµÑ€", "Ğ±Ğ¸Ñ€"],
        "ÑĞ»Ğ°Ñ‚ÑŒ": ["ÑÑ‹Ğ»", "ÑĞ»"],
    }
    
    if base_verb_lower in verb_root_map:
        possible_roots = verb_root_map[base_verb_lower]
        log(f"[DEBUG] ä»è¯æ ¹æ˜ å°„è¡¨æ‰¾åˆ°åŸºç¡€åŠ¨è¯ '{base_verb}' çš„è¯æ ¹: {possible_roots}")
    else:
        root = _deriv_get_root(base_verb_lower, morph)
        if not root:
            return 0, f"âŒ æ— æ³•ä»åŸºç¡€åŠ¨è¯ '{base_verb}' ä¸­æå–æœ‰æ•ˆçš„è¯æ ¹"
        possible_roots = [root]
        log(f"[DEBUG] æå–åŸºç¡€åŠ¨è¯ '{base_verb}' çš„è¯æ ¹: {possible_roots}")

    # æå–å•è¯
    words = set(re.findall(r'\b[Ğ°-ÑÑ‘-]{3,}\b', text))
    log(f"[DEBUG] ä»æ–‡æœ¬ä¸­æå–åˆ° {len(words)} ä¸ªå•è¯")
    
    found = set()

    # åŠ¨è¯å‰ç¼€
    prefixes = {
        'Ğ²', 'Ğ²Ğ¾', 'Ğ²Ğ·', 'Ğ²Ğ·Ğ¾', 'Ğ²Ğ¾Ğ·', 'Ğ²Ğ¾Ğ·Ğ¾', 'Ğ²Ñ‹', 'Ğ´Ğ¾', 'Ğ·Ğ°', 'Ğ¸Ğ·', 'Ğ¸Ğ·Ğ¾',
        'Ğ½Ğ°', 'Ğ½Ğ°Ğ´', 'Ğ½Ğ°Ğ´Ğ¾', 'Ğ½Ğµ', 'Ğ½Ğ¸Ğ·', 'Ğ½Ğ¸Ğ·Ğ¾', 'Ğ¾', 'Ğ¾Ğ±', 'Ğ¾Ğ±Ğ¾', 'Ğ¾Ñ‚', 'Ğ¾Ñ‚Ğ¾',
        'Ğ¿ĞµÑ€Ğµ', 'Ğ¿Ğ¾', 'Ğ¿Ğ¾Ğ´', 'Ğ¿Ğ¾Ğ´Ğ¾', 'Ğ¿Ñ€Ğ°', 'Ğ¿Ñ€ĞµĞ´', 'Ğ¿Ñ€Ğµ', 'Ğ¿Ñ€Ğ¾', 'Ñ€Ğ°Ğ·', 'Ñ€Ğ°Ğ·Ğ¾',
        'Ñ', 'ÑĞ¾', 'ÑÑƒ', 'Ñƒ'
    }

    # éå†åˆ†æ
    for word in words:
        parses = morph.parse(word)
        if not parses:
            continue

        lemma = parses[0].normal_form
        
        if 'VERB' not in parses[0].tag and 'INFN' not in parses[0].tag:
            continue

        # åˆ¤æ–­æ˜¯å¦ä¸ºæ´¾ç”Ÿè¯
        is_base = (lemma == base_verb_lower) or \
                  (base_verb_lower == "Ğ¸Ğ´Ñ‚Ğ¸" and lemma == "Ñ…Ğ¾Ğ´Ğ¸Ñ‚ÑŒ") or \
                  (base_verb_lower == "ĞµÑ…Ğ°Ñ‚ÑŒ" and lemma == "ĞµĞ·Ğ´Ğ¸Ñ‚ÑŒ")
        
        if not is_base:
            for root in possible_roots:
                if root in lemma:
                    is_derived = False
                    for prefix in prefixes:
                        if lemma.startswith(prefix) and root in lemma.replace(prefix, '', 1):
                            is_derived = True
                            log(f"[DEBUG] æ‰¾åˆ°æ´¾ç”Ÿè¯: {word} -> {lemma} (å‰ç¼€: {prefix}, è¯æ ¹: {root})")
                            break
                    
                    if is_derived:
                        found.add(lemma)
                        break

    # ç»“æœåˆ¤æ–­
    found_count = len(found)
    found_str = ", ".join(sorted(list(found))) if found else "æ— "

    log(f"[DEBUG] æ€»å…±æ‰¾åˆ° {found_count} ä¸ªæ´¾ç”Ÿè¯")

    if found_count >= required_count:
        return 1, f"âœ… æˆåŠŸæ‰¾åˆ° {found_count} ä¸ªæ´¾ç”Ÿè¯ (è¦æ±‚â‰¥{required_count}): {found_str}"
    else:
        return 0, f"âŒ æ‰¾åˆ° {found_count} ä¸ªæ´¾ç”Ÿè¯ (è¦æ±‚â‰¥{required_count}): {found_str}"


def _deriv_get_root(verb_str, morph):
    """æå–åŠ¨è¯è¯æ ¹"""
    if morph is None:
        endings = ['Ğ°Ñ‚ÑŒ', 'ĞµÑ‚ÑŒ', 'Ğ¸Ñ‚ÑŒ', 'Ñ‚Ğ¸', 'Ñ‡ÑŒ']
        for end in endings:
            if verb_str.endswith(end):
                return verb_str[:-len(end)]
        return verb_str
    
    p = morph.parse(verb_str)
    if not p:
        return verb_str

    infinitive = p[0].normal_form
    if infinitive.endswith(('Ğ°Ñ‚ÑŒ', 'ÑÑ‚ÑŒ')):
        return infinitive[:-3]
    if infinitive.endswith(('Ğ¸Ñ‚ÑŒ', 'ĞµÑ‚ÑŒ')):
        return infinitive[:-3]
    if infinitive.endswith('Ñ‚Ğ¸'):
        return infinitive[:-2]
    if infinitive.endswith('Ñ‡ÑŒ'):
        return infinitive[:-2]
    return infinitive


# ==================== è§„åˆ™ 9: å‰¯åŠ¨è¯ä½¿ç”¨æ£€æµ‹ ====================
def check_russian_participle_usage(content_list, *args, debug=False):
    """
    æ£€æŸ¥å¥å­ä¸­çš„å‰¯åŠ¨è¯ä½¿ç”¨æ˜¯å¦æ­£ç¡®
    
    Args:
        content_list: æ–‡æœ¬å†…å®¹åˆ—è¡¨
        *args: å…³é”®è¯åˆ—è¡¨
        debug: æ˜¯å¦è¾“å‡ºè°ƒè¯•ä¿¡æ¯
    
    Returns:
        tuple: (1/0, è¯´æ˜ä¿¡æ¯)
    """
    log = create_logger(debug)
    
    morph, available = LibraryManager.get_morph()
    
    if not available:
        if debug:
            log("[WARNING] Pymorphy2 åº“æœªå®‰è£…")
        return 0, "âŒ è§„åˆ™è¯„ä¼°å¤±è´¥: Pymorphy2 åº“æœªå®‰è£…"
    else:
        log("[INFO] Pymorphy2 åº“åŠ è½½æˆåŠŸ")
    
    if not content_list or len(content_list) == 0 or not content_list[0]:
        return 0, "âŒ è¾“å…¥å†…å®¹ä¸ºç©º"
    if not args or len(args) < 2:
        return 0, "âŒ è‡³å°‘éœ€è¦ä¸¤ä¸ªå…³é”®è¯"
    
    sentence = content_list[0]
    keywords = list(args)
    
    log(f"\n[DEBUG] æ£€æŸ¥å¥å­: {sentence}")
    log(f"[DEBUG] å…³é”®è¯: {keywords}")
    
    try:
        # è·å–å…³é”®è¯çš„ä½“å¯¹
        all_target_lemmas = {}
        for keyword in keywords:
            aspect_variants = _part_get_aspect_pair(keyword)
            target_lemmas = set()
            for variant in aspect_variants:
                keyword_parse = morph.parse(variant)
                if keyword_parse:
                    target_lemmas.update(p.normal_form for p in keyword_parse)
            all_target_lemmas[keyword] = target_lemmas
            log(f"[DEBUG] å…³é”®è¯ '{keyword}' çš„ä½“å¯¹æ ‡å‡†å½¢å¼: {target_lemmas}")
        
        words = re.findall(r'\b[Ğ°-ÑÑ‘Ğ-Ğ¯Ğ-]+\b', sentence)
        parses = [morph.parse(w)[0] for w in words]
        
        # æŸ¥æ‰¾æ‰€æœ‰å‰¯åŠ¨è¯
        all_gerunds = []
        for word, parse in zip(words, parses):
            if parse.tag.POS == 'GRND':
                word_lemma = parse.normal_form
                aspect = 'perf' if 'perf' in parse.tag else ('impf' if 'impf' in parse.tag else None)
                all_gerunds.append({
                    'word': word,
                    'lemma': word_lemma,
                    'aspect': aspect
                })
                log(f"[DEBUG] å‘ç°å‰¯åŠ¨è¯: {word} (åŸå½¢: {word_lemma}, ä½“: {aspect})")
        
        if not all_gerunds:
            log(f"[DEBUG] å¥å­ä¸­æ²¡æœ‰æ‰¾åˆ°ä»»ä½•å‰¯åŠ¨è¯")
        else:
            log(f"[DEBUG] å¥å­ä¸­å…±æ‰¾åˆ° {len(all_gerunds)} ä¸ªå‰¯åŠ¨è¯")
        
        # æŸ¥æ‰¾å…³é”®è¯çš„å„ç§å½¢å¼
        found_forms = {keyword: {
            'finite_verbs': [],
            'gerunds': [],
            'infinitives': [],
            'other_forms': []
        } for keyword in keywords}
        
        for word, parse in zip(words, parses):
            word_lemma = parse.normal_form
            pos = parse.tag.POS
            aspect = 'perf' if 'perf' in parse.tag else ('impf' if 'impf' in parse.tag else None)
            
            for keyword in keywords:
                if word_lemma in all_target_lemmas[keyword]:
                    if pos == 'GRND':
                        found_forms[keyword]['gerunds'].append({
                            'word': word, 'lemma': word_lemma, 'aspect': aspect
                        })
                        log(f"[DEBUG] âœ“ æ‰¾åˆ°å‰¯åŠ¨è¯(GRND): {word}")
                    elif pos == 'INFN':
                        found_forms[keyword]['infinitives'].append({
                            'word': word, 'lemma': word_lemma, 'aspect': aspect
                        })
                        log(f"[DEBUG] âš  æ‰¾åˆ°ä¸å®šå¼(INFN): {word}")
                    elif pos == 'VERB':
                        found_forms[keyword]['finite_verbs'].append({
                            'word': word, 'lemma': word_lemma, 'aspect': aspect
                        })
                        log(f"[DEBUG] âœ“ æ‰¾åˆ°é™å®šåŠ¨è¯(VERB): {word}")
                    else:
                        found_forms[keyword]['other_forms'].append({
                            'word': word, 'lemma': word_lemma, 'aspect': aspect, 'pos': pos
                        })
        
        # æ£€æŸ¥ç¼ºå¤±çš„å…³é”®è¯
        missing = []
        for keyword in keywords:
            forms = found_forms[keyword]
            total_found = sum(len(forms[k]) for k in forms.keys())
            if total_found == 0:
                missing.append(keyword)
        
        if missing:
            if all_gerunds:
                gerund_info = ', '.join([f"{g['word']}({g['lemma']})" for g in all_gerunds])
                return 0, f"âŒ å…³é”®è¯ {missing} æœªåœ¨å¥ä¸­æ‰¾åˆ°ã€‚\n\n   å¥ä¸­å‰¯åŠ¨è¯ï¼š{gerund_info}ï¼ˆä¸æ¥è‡ªè¦æ±‚çš„å…³é”®è¯ï¼‰"
            return 0, f"âŒ å…³é”®è¯ {missing} æœªåœ¨å¥ä¸­æ‰¾åˆ°"
        
        # æ„å»ºæŠ¥å‘Š
        forms_report = []
        for keyword, forms in found_forms.items():
            parts = []
            if forms['finite_verbs']:
                parts.append(f"é™å®šåŠ¨è¯: {', '.join([f['word'] for f in forms['finite_verbs']])}")
            if forms['gerunds']:
                parts.append(f"å‰¯åŠ¨è¯: {', '.join([f['word'] for f in forms['gerunds']])}")
            if forms['infinitives']:
                parts.append(f"ä¸å®šå¼: {', '.join([f['word'] for f in forms['infinitives']])}")
            if forms['other_forms']:
                # âœ… ä¿®å¤ï¼šæ‹†åˆ†è¡¨è¾¾å¼
                other_forms_str = ', '.join([f"{f['word']}({f['pos']})" for f in forms['other_forms']])
                parts.append(f"å…¶ä»–: {other_forms_str}")
            
            if parts:
                forms_report.append(f"'{keyword}': {', '.join(parts)}")

        
        forms_report_str = '\n   '.join(forms_report) if forms_report else "æœªæ‰¾åˆ°ä»»ä½•å½¢å¼"
        
        # æ£€æŸ¥æ˜¯å¦æœ‰å‰¯åŠ¨è¯
        if len(all_gerunds) == 0:
            return 0, f"âŒ å¥å­ä¸­æœªæ‰¾åˆ°å‰¯åŠ¨è¯ã€‚\n\n   å…³é”®è¯ä½¿ç”¨æƒ…å†µï¼š\n   {forms_report_str}"
        
        # æ£€æŸ¥å…³é”®è¯æ˜¯å¦ä»¥ç‹¬ç«‹å½¢å¼ä½¿ç”¨
        keywords_without_main = []
        keywords_only_inf = []
        
        for keyword, forms in found_forms.items():
            has_finite = len(forms['finite_verbs']) > 0
            has_gerund = len(forms['gerunds']) > 0
            has_infinitive = len(forms['infinitives']) > 0
            
            if not has_finite and not has_gerund:
                keywords_without_main.append(keyword)
                if has_infinitive:
                    keywords_only_inf.append({
                        'keyword': keyword,
                        'infinitive_word': forms['infinitives'][0]['word']
                    })
        
        if keywords_without_main:
            error_msg = f"âŒ å…³é”®è¯ {keywords_without_main} æœªä»¥ç‹¬ç«‹å½¢å¼ä½¿ç”¨ã€‚\n\n"
            
            if keywords_only_inf:
                inf_examples = ', '.join([f"'{item['infinitive_word']}'" for item in keywords_only_inf])
                error_msg += f"   ä¸å®šå¼ {inf_examples} ä¾é™„äºå…¶ä»–åŠ¨è¯ï¼Œä¸æ˜¯ç‹¬ç«‹åŠ¨ä½œã€‚\n\n"
            
            error_msg += f"   å…³é”®è¯ä½¿ç”¨æƒ…å†µï¼š\n"
            for keyword, forms in found_forms.items():
                if forms['finite_verbs']:
                    error_msg += f"   âœ“ '{keyword}': é™å®šåŠ¨è¯ {', '.join([f['word'] for f in forms['finite_verbs']])}\n"
                elif forms['gerunds']:
                    error_msg += f"   âœ“ '{keyword}': å‰¯åŠ¨è¯ {', '.join([f['word'] for f in forms['gerunds']])}\n"
                elif forms['infinitives']:
                    error_msg += f"   âœ— '{keyword}': ä¸å®šå¼ {forms['infinitives'][0]['word']}\n"
            
            if all_gerunds:
                gerund_info = ', '.join([f"{g['word']}({g['lemma']})" for g in all_gerunds])
                error_msg += f"\n   å¥ä¸­å‰¯åŠ¨è¯ï¼š{gerund_info}ï¼ˆä¸æ¥è‡ªå…³é”®è¯ï¼‰"
            
            return 0, error_msg
        
        # æˆåŠŸ
        gerund_info = ', '.join([f"{g['word']}({g['lemma']}, {g['aspect']})" for g in all_gerunds])
        
        keyword_main_forms = []
        for keyword, forms in found_forms.items():
            for fv in forms['finite_verbs']:
                keyword_main_forms.append(f"{fv['word']}(é™å®š-{keyword})")
            for gv in forms['gerunds']:
                keyword_main_forms.append(f"{gv['word']}(å‰¯åŠ¨-{keyword})")
        
        keyword_forms_info = ', '.join(keyword_main_forms)
        
        return 1, f"âœ… å‰¯åŠ¨è¯ä½¿ç”¨æ­£ç¡®ã€‚\n\n   å…³é”®è¯ï¼š{keyword_forms_info}\n   å¥ä¸­å‰¯åŠ¨è¯ï¼š{gerund_info}"
    
    except Exception as e:
        log(f"[ERROR] å¼‚å¸¸: {e}")
        if debug:
            import traceback
            traceback.print_exc()
        return 0, f"âŒ å‡½æ•°æ‰§è¡Œå¼‚å¸¸: {e}"


def _part_get_aspect_pair(verb):
    """è·å–åŠ¨è¯ä½“å¯¹"""
    pairs = {
        'Ğ¿Ğ¸Ñ‚ÑŒ': {'Ğ¿Ğ¸Ñ‚ÑŒ', 'Ğ²Ñ‹Ğ¿Ğ¸Ñ‚ÑŒ', 'Ğ²Ñ‹Ğ¿Ğ¸Ğ²Ğ°Ñ‚ÑŒ', 'Ğ¿Ğ¾Ğ¿Ğ¸Ñ‚ÑŒ'},
        'Ğ²Ñ‹Ğ¿Ğ¸Ñ‚ÑŒ': {'Ğ¿Ğ¸Ñ‚ÑŒ', 'Ğ²Ñ‹Ğ¿Ğ¸Ñ‚ÑŒ', 'Ğ²Ñ‹Ğ¿Ğ¸Ğ²Ğ°Ñ‚ÑŒ'},
        'Ğ²Ñ‹Ğ¿Ğ¸Ğ²Ğ°Ñ‚ÑŒ': {'Ğ¿Ğ¸Ñ‚ÑŒ', 'Ğ²Ñ‹Ğ¿Ğ¸Ñ‚ÑŒ', 'Ğ²Ñ‹Ğ¿Ğ¸Ğ²Ğ°Ñ‚ÑŒ'},
        'Ğ¿Ğ¾Ğ¿Ğ¸Ñ‚ÑŒ': {'Ğ¿Ğ¸Ñ‚ÑŒ', 'Ğ¿Ğ¾Ğ¿Ğ¸Ñ‚ÑŒ'},
        'Ñ‡Ğ¸Ñ‚Ğ°Ñ‚ÑŒ': {'Ñ‡Ğ¸Ñ‚Ğ°Ñ‚ÑŒ', 'Ğ¿Ñ€Ğ¾Ñ‡Ğ¸Ñ‚Ğ°Ñ‚ÑŒ', 'Ğ¿Ğ¾Ñ‡Ğ¸Ñ‚Ğ°Ñ‚ÑŒ'},
        'Ğ¿Ñ€Ğ¾Ñ‡Ğ¸Ñ‚Ğ°Ñ‚ÑŒ': {'Ñ‡Ğ¸Ñ‚Ğ°Ñ‚ÑŒ', 'Ğ¿Ñ€Ğ¾Ñ‡Ğ¸Ñ‚Ğ°Ñ‚ÑŒ'},
        'Ğ¿Ğ¾Ñ‡Ğ¸Ñ‚Ğ°Ñ‚ÑŒ': {'Ñ‡Ğ¸Ñ‚Ğ°Ñ‚ÑŒ', 'Ğ¿Ğ¾Ñ‡Ğ¸Ñ‚Ğ°Ñ‚ÑŒ'},
        'Ğ´ĞµĞ»Ğ°Ñ‚ÑŒ': {'Ğ´ĞµĞ»Ğ°Ñ‚ÑŒ', 'ÑĞ´ĞµĞ»Ğ°Ñ‚ÑŒ'},
        'ÑĞ´ĞµĞ»Ğ°Ñ‚ÑŒ': {'Ğ´ĞµĞ»Ğ°Ñ‚ÑŒ', 'ÑĞ´ĞµĞ»Ğ°Ñ‚ÑŒ'},
        'Ğ¾Ñ‚Ğ´Ñ‹Ñ…Ğ°Ñ‚ÑŒ': {'Ğ¾Ñ‚Ğ´Ñ‹Ñ…Ğ°Ñ‚ÑŒ', 'Ğ¾Ñ‚Ğ´Ğ¾Ñ…Ğ½ÑƒÑ‚ÑŒ'},
        'Ğ¾Ñ‚Ğ´Ğ¾Ñ…Ğ½ÑƒÑ‚ÑŒ': {'Ğ¾Ñ‚Ğ´Ñ‹Ñ…Ğ°Ñ‚ÑŒ', 'Ğ¾Ñ‚Ğ´Ğ¾Ñ…Ğ½ÑƒÑ‚ÑŒ'},
        'ÑĞ»ÑƒÑˆĞ°Ñ‚ÑŒ': {'ÑĞ»ÑƒÑˆĞ°Ñ‚ÑŒ', 'Ğ¿Ğ¾ÑĞ»ÑƒÑˆĞ°Ñ‚ÑŒ'},
        'Ğ¿Ğ¾ÑĞ»ÑƒÑˆĞ°Ñ‚ÑŒ': {'ÑĞ»ÑƒÑˆĞ°Ñ‚ÑŒ', 'Ğ¿Ğ¾ÑĞ»ÑƒÑˆĞ°Ñ‚ÑŒ'},
        'Ğ³Ğ¾Ğ²Ğ¾Ñ€Ğ¸Ñ‚ÑŒ': {'Ğ³Ğ¾Ğ²Ğ¾Ñ€Ğ¸Ñ‚ÑŒ', 'ÑĞºĞ°Ğ·Ğ°Ñ‚ÑŒ', 'Ğ¿Ğ¾Ğ³Ğ¾Ğ²Ğ¾Ñ€Ğ¸Ñ‚ÑŒ'},
        'ÑĞºĞ°Ğ·Ğ°Ñ‚ÑŒ': {'Ğ³Ğ¾Ğ²Ğ¾Ñ€Ğ¸Ñ‚ÑŒ', 'ÑĞºĞ°Ğ·Ğ°Ñ‚ÑŒ'},
        'Ğ¿Ğ¾Ğ³Ğ¾Ğ²Ğ¾Ñ€Ğ¸Ñ‚ÑŒ': {'Ğ³Ğ¾Ğ²Ğ¾Ñ€Ğ¸Ñ‚ÑŒ', 'Ğ¿Ğ¾Ğ³Ğ¾Ğ²Ğ¾Ñ€Ğ¸Ñ‚ÑŒ'},
        'Ğ³ÑƒĞ»ÑÑ‚ÑŒ': {'Ğ³ÑƒĞ»ÑÑ‚ÑŒ', 'Ğ¿Ğ¾Ğ³ÑƒĞ»ÑÑ‚ÑŒ'},
        'Ğ¿Ğ¾Ğ³ÑƒĞ»ÑÑ‚ÑŒ': {'Ğ³ÑƒĞ»ÑÑ‚ÑŒ', 'Ğ¿Ğ¾Ğ³ÑƒĞ»ÑÑ‚ÑŒ'},
        'ÑĞ¼Ğ¾Ñ‚Ñ€ĞµÑ‚ÑŒ': {'ÑĞ¼Ğ¾Ñ‚Ñ€ĞµÑ‚ÑŒ', 'Ğ¿Ğ¾ÑĞ¼Ğ¾Ñ‚Ñ€ĞµÑ‚ÑŒ'},
        'Ğ¿Ğ¾ÑĞ¼Ğ¾Ñ‚Ñ€ĞµÑ‚ÑŒ': {'ÑĞ¼Ğ¾Ñ‚Ñ€ĞµÑ‚ÑŒ', 'Ğ¿Ğ¾ÑĞ¼Ğ¾Ñ‚Ñ€ĞµÑ‚ÑŒ'},
        'ÑƒĞ¶Ğ¸Ğ½Ğ°Ñ‚ÑŒ': {'ÑƒĞ¶Ğ¸Ğ½Ğ°Ñ‚ÑŒ', 'Ğ¿Ğ¾ÑƒĞ¶Ğ¸Ğ½Ğ°Ñ‚ÑŒ'},
        'Ğ¿Ğ¾ÑƒĞ¶Ğ¸Ğ½Ğ°Ñ‚ÑŒ': {'ÑƒĞ¶Ğ¸Ğ½Ğ°Ñ‚ÑŒ', 'Ğ¿Ğ¾ÑƒĞ¶Ğ¸Ğ½Ğ°Ñ‚ÑŒ'},
        'Ğ²Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰Ğ°Ñ‚ÑŒÑÑ': {'Ğ²Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰Ğ°Ñ‚ÑŒÑÑ', 'Ğ²ĞµÑ€Ğ½ÑƒÑ‚ÑŒÑÑ'},
        'Ğ²ĞµÑ€Ğ½ÑƒÑ‚ÑŒÑÑ': {'Ğ²Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰Ğ°Ñ‚ÑŒÑÑ', 'Ğ²ĞµÑ€Ğ½ÑƒÑ‚ÑŒÑÑ'},
        'Ğ½Ğ°Ñ‡Ğ¸Ğ½Ğ°Ñ‚ÑŒ': {'Ğ½Ğ°Ñ‡Ğ¸Ğ½Ğ°Ñ‚ÑŒ', 'Ğ½Ğ°Ñ‡Ğ°Ñ‚ÑŒ'},
        'Ğ½Ğ°Ñ‡Ğ°Ñ‚ÑŒ': {'Ğ½Ğ°Ñ‡Ğ¸Ğ½Ğ°Ñ‚ÑŒ', 'Ğ½Ğ°Ñ‡Ğ°Ñ‚ÑŒ'},
        'Ğ»Ğ¾Ğ¶Ğ¸Ñ‚ÑŒÑÑ': {'Ğ»Ğ¾Ğ¶Ğ¸Ñ‚ÑŒÑÑ', 'Ğ»ĞµÑ‡ÑŒ'},
        'Ğ»ĞµÑ‡ÑŒ': {'Ğ»Ğ¾Ğ¶Ğ¸Ñ‚ÑŒÑÑ', 'Ğ»ĞµÑ‡ÑŒ'},
        'Ñ€ĞµÑˆĞ°Ñ‚ÑŒ': {'Ñ€ĞµÑˆĞ°Ñ‚ÑŒ', 'Ñ€ĞµÑˆĞ¸Ñ‚ÑŒ'},
        'Ñ€ĞµÑˆĞ¸Ñ‚ÑŒ': {'Ñ€ĞµÑˆĞ°Ñ‚ÑŒ', 'Ñ€ĞµÑˆĞ¸Ñ‚ÑŒ'},
        'ÑĞ°Ğ´Ğ¸Ñ‚ÑŒÑÑ': {'ÑĞ°Ğ´Ğ¸Ñ‚ÑŒÑÑ', 'ÑĞµÑÑ‚ÑŒ'},
        'ÑĞµÑÑ‚ÑŒ': {'ÑĞ°Ğ´Ğ¸Ñ‚ÑŒÑÑ', 'ÑĞµÑÑ‚ÑŒ'},
        'Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ‚ÑŒ': {'Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ‚ÑŒ', 'Ğ²ĞºĞ»ÑÑ‡Ğ¸Ñ‚ÑŒ'},
        'Ğ²ĞºĞ»ÑÑ‡Ğ¸Ñ‚ÑŒ': {'Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ‚ÑŒ', 'Ğ²ĞºĞ»ÑÑ‡Ğ¸Ñ‚ÑŒ'},
        'Ğ·Ğ°ĞºĞ°Ğ½Ñ‡Ğ¸Ğ²Ğ°Ñ‚ÑŒ': {'Ğ·Ğ°ĞºĞ°Ğ½Ñ‡Ğ¸Ğ²Ğ°Ñ‚ÑŒ', 'Ğ·Ğ°ĞºĞ¾Ğ½Ñ‡Ğ¸Ñ‚ÑŒ'},
        'Ğ·Ğ°ĞºĞ¾Ğ½Ñ‡Ğ¸Ñ‚ÑŒ': {'Ğ·Ğ°ĞºĞ°Ğ½Ñ‡Ğ¸Ğ²Ğ°Ñ‚ÑŒ', 'Ğ·Ğ°ĞºĞ¾Ğ½Ñ‡Ğ¸Ñ‚ÑŒ'},
        'Ğ¿Ğ¸ÑĞ°Ñ‚ÑŒ': {'Ğ¿Ğ¸ÑĞ°Ñ‚ÑŒ', 'Ğ½Ğ°Ğ¿Ğ¸ÑĞ°Ñ‚ÑŒ'},
        'Ğ½Ğ°Ğ¿Ğ¸ÑĞ°Ñ‚ÑŒ': {'Ğ¿Ğ¸ÑĞ°Ñ‚ÑŒ', 'Ğ½Ğ°Ğ¿Ğ¸ÑĞ°Ñ‚ÑŒ'},
        'Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ‚ÑŒ': {'Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ‚ÑŒ', 'Ğ¿Ğ¾Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ‚ÑŒ'},
        'Ğ¿Ğ¾Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ‚ÑŒ': {'Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ‚ÑŒ', 'Ğ¿Ğ¾Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ‚ÑŒ'},
        'ÑƒÑ‡Ğ¸Ñ‚ÑŒ': {'ÑƒÑ‡Ğ¸Ñ‚ÑŒ', 'Ğ²Ñ‹ÑƒÑ‡Ğ¸Ñ‚ÑŒ', 'Ğ¸Ğ·ÑƒÑ‡Ğ°Ñ‚ÑŒ', 'Ğ¸Ğ·ÑƒÑ‡Ğ¸Ñ‚ÑŒ'},
        'Ğ²Ñ‹ÑƒÑ‡Ğ¸Ñ‚ÑŒ': {'ÑƒÑ‡Ğ¸Ñ‚ÑŒ', 'Ğ²Ñ‹ÑƒÑ‡Ğ¸Ñ‚ÑŒ'},
        'Ğ¸Ğ·ÑƒÑ‡Ğ°Ñ‚ÑŒ': {'Ğ¸Ğ·ÑƒÑ‡Ğ°Ñ‚ÑŒ', 'Ğ¸Ğ·ÑƒÑ‡Ğ¸Ñ‚ÑŒ'},
        'Ğ¸Ğ·ÑƒÑ‡Ğ¸Ñ‚ÑŒ': {'Ğ¸Ğ·ÑƒÑ‡Ğ°Ñ‚ÑŒ', 'Ğ¸Ğ·ÑƒÑ‡Ğ¸Ñ‚ÑŒ'},
        'Ğ´ÑƒĞ¼Ğ°Ñ‚ÑŒ': {'Ğ´ÑƒĞ¼Ğ°Ñ‚ÑŒ', 'Ğ¿Ğ¾Ğ´ÑƒĞ¼Ğ°Ñ‚ÑŒ'},
        'Ğ¿Ğ¾Ğ´ÑƒĞ¼Ğ°Ñ‚ÑŒ': {'Ğ´ÑƒĞ¼Ğ°Ñ‚ÑŒ', 'Ğ¿Ğ¾Ğ´ÑƒĞ¼Ğ°Ñ‚ÑŒ'},
        'ÑĞ¸Ğ´ĞµÑ‚ÑŒ': {'ÑĞ¸Ğ´ĞµÑ‚ÑŒ', 'Ğ¿Ğ¾ÑĞ¸Ğ´ĞµÑ‚ÑŒ'},
        'Ğ¿Ğ¾ÑĞ¸Ğ´ĞµÑ‚ÑŒ': {'ÑĞ¸Ğ´ĞµÑ‚ÑŒ', 'Ğ¿Ğ¾ÑĞ¸Ğ´ĞµÑ‚ÑŒ'},
        'ĞµÑÑ‚ÑŒ': {'ĞµÑÑ‚ÑŒ', 'ÑÑŠĞµÑÑ‚ÑŒ', 'Ğ¿Ğ¾ĞµÑÑ‚ÑŒ'},
        'ÑÑŠĞµÑÑ‚ÑŒ': {'ĞµÑÑ‚ÑŒ', 'ÑÑŠĞµÑÑ‚ÑŒ'},
        'Ğ¿Ğ¾ĞµÑÑ‚ÑŒ': {'ĞµÑÑ‚ÑŒ', 'Ğ¿Ğ¾ĞµÑÑ‚ÑŒ'},
        'Ğ¶Ğ¸Ñ‚ÑŒ': {'Ğ¶Ğ¸Ñ‚ÑŒ', 'Ğ¿Ñ€Ğ¾Ğ¶Ğ¸Ñ‚ÑŒ', 'Ğ¿Ğ¾Ğ¶Ğ¸Ñ‚ÑŒ'},
        'Ğ¿Ñ€Ğ¾Ğ¶Ğ¸Ñ‚ÑŒ': {'Ğ¶Ğ¸Ñ‚ÑŒ', 'Ğ¿Ñ€Ğ¾Ğ¶Ğ¸Ñ‚ÑŒ'},
        'Ğ¿Ğ¾Ğ¶Ğ¸Ñ‚ÑŒ': {'Ğ¶Ğ¸Ñ‚ÑŒ', 'Ğ¿Ğ¾Ğ¶Ğ¸Ñ‚ÑŒ'},
        'ÑĞ¿Ğ°Ñ‚ÑŒ': {'ÑĞ¿Ğ°Ñ‚ÑŒ', 'Ğ¿Ğ¾ÑĞ¿Ğ°Ñ‚ÑŒ'},
        'Ğ¿Ğ¾ÑĞ¿Ğ°Ñ‚ÑŒ': {'ÑĞ¿Ğ°Ñ‚ÑŒ', 'Ğ¿Ğ¾ÑĞ¿Ğ°Ñ‚ÑŒ'},
        'Ğ²ÑÑ‚Ğ°Ğ²Ğ°Ñ‚ÑŒ': {'Ğ²ÑÑ‚Ğ°Ğ²Ğ°Ñ‚ÑŒ', 'Ğ²ÑÑ‚Ğ°Ñ‚ÑŒ'},
        'Ğ²ÑÑ‚Ğ°Ñ‚ÑŒ': {'Ğ²ÑÑ‚Ğ°Ğ²Ğ°Ñ‚ÑŒ', 'Ğ²ÑÑ‚Ğ°Ñ‚ÑŒ'},
        'Ğ¸Ğ´Ñ‚Ğ¸': {'Ğ¸Ğ´Ñ‚Ğ¸', 'Ğ¿Ğ¾Ğ¹Ñ‚Ğ¸'},
        'Ğ¿Ğ¾Ğ¹Ñ‚Ğ¸': {'Ğ¸Ğ´Ñ‚Ğ¸', 'Ğ¿Ğ¾Ğ¹Ñ‚Ğ¸'},
        'Ñ…Ğ¾Ğ´Ğ¸Ñ‚ÑŒ': {'Ñ…Ğ¾Ğ´Ğ¸Ñ‚ÑŒ', 'ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ÑŒ', 'Ğ¿Ğ¾Ñ…Ğ¾Ğ´Ğ¸Ñ‚ÑŒ'},
        'ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ÑŒ': {'Ñ…Ğ¾Ğ´Ğ¸Ñ‚ÑŒ', 'ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ÑŒ'},
        'Ğ¿Ğ¾Ñ…Ğ¾Ğ´Ğ¸Ñ‚ÑŒ': {'Ñ…Ğ¾Ğ´Ğ¸Ñ‚ÑŒ', 'Ğ¿Ğ¾Ñ…Ğ¾Ğ´Ğ¸Ñ‚ÑŒ'},
        'ĞµÑ…Ğ°Ñ‚ÑŒ': {'ĞµÑ…Ğ°Ñ‚ÑŒ', 'Ğ¿Ğ¾ĞµÑ…Ğ°Ñ‚ÑŒ'},
        'Ğ¿Ğ¾ĞµÑ…Ğ°Ñ‚ÑŒ': {'ĞµÑ…Ğ°Ñ‚ÑŒ', 'Ğ¿Ğ¾ĞµÑ…Ğ°Ñ‚ÑŒ'},
        'Ğ±ĞµĞ¶Ğ°Ñ‚ÑŒ': {'Ğ±ĞµĞ¶Ğ°Ñ‚ÑŒ', 'Ğ¿Ğ¾Ğ±ĞµĞ¶Ğ°Ñ‚ÑŒ'},
        'Ğ¿Ğ¾Ğ±ĞµĞ¶Ğ°Ñ‚ÑŒ': {'Ğ±ĞµĞ¶Ğ°Ñ‚ÑŒ', 'Ğ¿Ğ¾Ğ±ĞµĞ¶Ğ°Ñ‚ÑŒ'},
        'Ğ±Ñ€Ğ°Ñ‚ÑŒ': {'Ğ±Ñ€Ğ°Ñ‚ÑŒ', 'Ğ²Ğ·ÑÑ‚ÑŒ'},
        'Ğ²Ğ·ÑÑ‚ÑŒ': {'Ğ±Ñ€Ğ°Ñ‚ÑŒ', 'Ğ²Ğ·ÑÑ‚ÑŒ'},
        'Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ': {'Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ', 'Ğ´Ğ°Ñ‚ÑŒ'},
        'Ğ´Ğ°Ñ‚ÑŒ': {'Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ', 'Ğ´Ğ°Ñ‚ÑŒ'},
        'Ğ¿Ğ¾ĞºÑƒĞ¿Ğ°Ñ‚ÑŒ': {'Ğ¿Ğ¾ĞºÑƒĞ¿Ğ°Ñ‚ÑŒ', 'ĞºÑƒĞ¿Ğ¸Ñ‚ÑŒ'},
        'ĞºÑƒĞ¿Ğ¸Ñ‚ÑŒ': {'Ğ¿Ğ¾ĞºÑƒĞ¿Ğ°Ñ‚ÑŒ', 'ĞºÑƒĞ¿Ğ¸Ñ‚ÑŒ'},
        'Ğ¿Ñ€Ğ¾Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ': {'Ğ¿Ñ€Ğ¾Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ', 'Ğ¿Ñ€Ğ¾Ğ´Ğ°Ñ‚ÑŒ'},
        'Ğ¿Ñ€Ğ¾Ğ´Ğ°Ñ‚ÑŒ': {'Ğ¿Ñ€Ğ¾Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ', 'Ğ¿Ñ€Ğ¾Ğ´Ğ°Ñ‚ÑŒ'},
    }
    verb_lower = verb.lower().strip()
    return pairs.get(verb_lower, {verb_lower})


# ==================== è§„åˆ™ 10: å…³é”®è¯å˜å½¢æ£€æµ‹ï¼ˆæ¯æ¡å†…å®¹ï¼‰ ====================
def check_keyword_inflections_each(content_list, keywords, debug=False):
    """
    æ£€æŸ¥æ¯ä¸€æ¡å†…å®¹æ˜¯å¦éƒ½åŒ…å«æŒ‡å®šå…³é”®è¯çš„ä»»ä½•å˜å½¢
    
    Args:
        content_list: æ–‡æœ¬å†…å®¹åˆ—è¡¨
        keywords: å…³é”®è¯åˆ—è¡¨
        debug: æ˜¯å¦è¾“å‡ºè°ƒè¯•ä¿¡æ¯
    
    Returns:
        tuple: (1/0, è¯´æ˜ä¿¡æ¯)
    """
    log = create_logger(debug)
    
    morph, available = LibraryManager.get_morph()
    
    if not available:
        if debug:
            log("[WARNING] Pymorphy2 åº“æœªå®‰è£…")
        return 0, "âŒ pymorphy2 åº“æœªå®‰è£…ï¼Œæ— æ³•æ‰§è¡Œæ­¤è§„åˆ™"
    else:
        log("[INFO] Pymorphy2 åº“åŠ è½½æˆåŠŸ")
    
    if not content_list or content_list == "INVALID":
        return 0, "âŒ è¾“å…¥æ–‡æœ¬æ— æ•ˆ"
    if not keywords:
        return 0, "âŒ å…³é”®è¯ä¸èƒ½ä¸ºç©º"
        
    if not isinstance(content_list, list):
        content_list = [str(content_list)]
    
    keywords = parse_keywords(keywords)
    if not keywords:
        return 0, "âŒ å…³é”®è¯è§£æå¤±è´¥æˆ–ä¸ºç©º"
    
    log(f"[DEBUG] è§£æåçš„å…³é”®è¯åˆ—è¡¨: {keywords}")
    
    try:
        all_target_lemmas = {}
        
        for keyword in keywords:
            aspect_variants = _part_get_aspect_pair(keyword)
            target_lemmas = set()
            for variant in aspect_variants:
                keyword_parse = morph.parse(variant)
                if keyword_parse:
                    target_lemmas.update(p.normal_form for p in keyword_parse)
            all_target_lemmas[keyword] = target_lemmas
            log(f"[DEBUG] å…³é”®è¯ '{keyword}' åŠå…¶ä½“å¯¹çš„æ ‡å‡†å½¢å¼: {target_lemmas}")

        failing_items = []

        for i, item_text in enumerate(content_list):
            if not item_text or not str(item_text).strip():
                failing_items.append(f"ç¬¬ {i+1} æ¡å†…å®¹ä¸ºç©º")
                continue

            item_text_str = str(item_text).lower()
            words_in_item = set(re.findall(r'\b[Ğ°-ÑÑ‘-]+\b', item_text_str))
            
            missing_keywords = []
            
            for keyword in keywords:
                found_keyword = False
                target_lemmas = all_target_lemmas[keyword]
                
                for word in words_in_item:
                    parses = morph.parse(word)
                    if parses:
                        word_lemma = parses[0].normal_form
                        if word_lemma in target_lemmas:
                            found_keyword = True
                            log(f"[DEBUG] ç¬¬ {i+1} æ¡æ‰¾åˆ° '{keyword}': {word} â†’ {word_lemma}")
                            break
                
                if not found_keyword:
                    missing_keywords.append(keyword)
            
            if missing_keywords:
                failing_items.append(f"ç¬¬ {i+1} æ¡å†…å®¹æœªæ‰¾åˆ°å…³é”®è¯ {missing_keywords}")

        if not failing_items:
            keywords_str = "', '".join(keywords)
            return 1, f"âœ… æ‰€æœ‰ {len(content_list)} æ¡å†…å®¹éƒ½åŒ…å«äº†å…³é”®è¯ '{keywords_str}'"
        else:
            return 0, f"âŒ æœ‰ {len(failing_items)}/{len(content_list)} æ¡å†…å®¹ä¸æ»¡è¶³è¦æ±‚: {'; '.join(failing_items)}"

    except Exception as e:
        log(f"[ERROR] å¼‚å¸¸: {e}")
        if debug:
            import traceback
            traceback.print_exc()
        return 0, f"âŒ å‡½æ•°æ‰§è¡Œå¼‚å¸¸: {e}"


# ==================== è§„åˆ™ 11: è¿å­—ç¬¦å•è¯æ£€æµ‹ ====================
def check_hyphenated_words_count(content_list, min_count=1, debug=False):
    """
    æ£€æŸ¥æ–‡æœ¬ä¸­å¸¦è¿å­—ç¬¦çš„ä¿„è¯­å•è¯æ•°é‡
    
    Args:
        content_list: æ–‡æœ¬å†…å®¹åˆ—è¡¨
        min_count: æœ€å°æ•°é‡ï¼ˆå¯ä»¥æ˜¯å­—ç¬¦ä¸²æˆ–æ•´æ•°ï¼‰
        debug: æ˜¯å¦è¾“å‡ºè°ƒè¯•ä¿¡æ¯
    
    Returns:
        tuple: (1/0, è¯´æ˜ä¿¡æ¯)
    """
    log = create_logger(debug)
    
    
    try:
        min_count = int(min_count)
    except (ValueError, TypeError):
        return 0, f"âŒ æœŸæœ›æ•°é‡å¿…é¡»æ˜¯æ•´æ•°ï¼Œå®é™…ä¸º '{min_count}'"
    
    if min_count < 0:
        return 0, f"âŒ æœŸæœ›æ•°é‡ä¸èƒ½ä¸ºè´Ÿæ•°ï¼Œå®é™…ä¸º {min_count}"
    
    if not content_list or not content_list[0]:
        return 0, "âŒ è¾“å…¥å†…å®¹ä¸ºç©º"
    
    text = content_list[0] if isinstance(content_list, list) else str(content_list)
    
    pattern = r'\b[Ğ°-ÑÑ‘Ğ-Ğ¯Ğ]+-[Ğ°-ÑÑ‘Ğ-Ğ¯Ğ]+(?:-[Ğ°-ÑÑ‘Ğ-Ğ¯Ğ]+)*\b'
    matches = re.findall(pattern, text)
    
    count = len(matches)
    
    log(f"[DEBUG] æ‰¾åˆ° {count} ä¸ªå¸¦è¿å­—ç¬¦çš„å•è¯: {matches}")
    
    if count >= min_count:
        return 1, f"âœ… æ‰¾åˆ° {count} ä¸ªå¸¦è¿å­—ç¬¦çš„å•è¯ï¼ˆâ‰¥{min_count}ä¸ªï¼‰: {', '.join(matches)}"
    else:
        return 0, f"âŒ åªæ‰¾åˆ° {count} ä¸ªå¸¦è¿å­—ç¬¦çš„å•è¯ï¼Œè¦æ±‚è‡³å°‘ {min_count} ä¸ªã€‚æ‰¾åˆ°çš„: {', '.join(matches) if matches else 'æ— '}"


# ==================== è§„åˆ™ 12: æ€§åˆ«ä¸€è‡´æ€§æ£€æµ‹ ====================
def check_russian_gender_agreement(content_list, keyword, required_count, debug=False):
    """
    æ£€æŸ¥ä¸å…³é”®è¯æ­é…çš„åŠ¨è¯ã€å½¢å®¹è¯ç­‰çš„"æ€§"æ˜¯å¦ä¸€è‡´
    
    Args:
        content_list: æ–‡æœ¬å†…å®¹åˆ—è¡¨
        keyword: å…³é”®è¯ï¼ˆåè¯ï¼‰
        required_count: æ¯æ¡è¯„è®ºè¦æ±‚çš„æœ€å°æ­é…æ•°é‡
        debug: æ˜¯å¦è¾“å‡ºè°ƒè¯•ä¿¡æ¯
    
    Returns:
        tuple: (1/0, è¯´æ˜ä¿¡æ¯)
    """
    log = create_logger(debug)
    
    morph, available = LibraryManager.get_morph()
    
    if not available:
        if debug:
            log("[WARNING] Pymorphy2 åº“æœªå®‰è£…")
        return 0, "âŒ è§„åˆ™è¯„ä¼°å¤±è´¥: Pymorphy2 åº“æœªå®‰è£…"
    else:
        log("[INFO] Pymorphy2 åº“åŠ è½½æˆåŠŸ")
    
    if not content_list or content_list == "INVALID":
        return 0, "âŒ è¾“å…¥æ–‡æœ¬æ— æ•ˆ"
    
    if not isinstance(content_list, list):
        content_list = [str(content_list)]
        
    try:
        min_agreements = int(required_count)
        if min_agreements <= 0:
            return 1, "âœ… è¦æ±‚æ‰¾åˆ°0ä¸ªæˆ–æ›´å°‘æ­é…ï¼Œè‡ªåŠ¨é€šè¿‡"
    except (ValueError, TypeError):
        return 0, f"âŒ 'required_count' å¿…é¡»æ˜¯æœ‰æ•ˆçš„æ•´æ•°ï¼Œä½†æ”¶åˆ°äº† '{required_count}'"

    keyword_lower = keyword.lower()
    try:
        keyword_parses = morph.parse(keyword_lower)
        if not keyword_parses:
            return 0, f"âŒ è¯æ€§æ­é…ä¸ç¬¦åˆè¦æ±‚"
        
        noun_parse = None
        for parse in keyword_parses:
            if 'NOUN' in parse.tag:
                noun_parse = parse
                break
        
        if not noun_parse:
            return 0, f"âŒ è¯æ€§æ­é…ä¸ç¬¦åˆè¦æ±‚"
        
        expected_gender = noun_parse.tag.gender
        if not expected_gender:
            return 0, f"âŒ è¯æ€§æ­é…ä¸ç¬¦åˆè¦æ±‚"
        
        gender_map = {"masc": "é˜³æ€§", "femn": "é˜´æ€§", "neut": "ä¸­æ€§"}
        expected_gender_text = gender_map.get(expected_gender, "æœªçŸ¥")
        log(f"[DEBUG] å…³é”®è¯ '{keyword}' æ€§åˆ«: {expected_gender_text}")
        
    except Exception as e:
        return 0, f"âŒ è¯æ€§æ­é…ä¸ç¬¦åˆè¦æ±‚"

    failing_count = 0
    total_items = len(content_list)

    for i, item_text in enumerate(content_list):
        item_text_str = str(item_text).strip()
        
        # ğŸ”¥ ä¿®æ”¹ï¼šä¸æ£€æŸ¥æ˜¯å¦åŒ…å«å…³é”®è¯ï¼Œåªæ£€æŸ¥æ­é…
        agreement_details = set()
        text_lower = item_text_str.lower()
        
        try:
            words = re.findall(r'\b[Ğ°-ÑÑ‘]+\b', text_lower)
            if not words:
                failing_count += 1
                continue
            
            log(f"[DEBUG] ç¬¬ {i+1} æ¡è¯„è®ºæ‰¾åˆ°çš„è¯æ±‡: {words}")
            
            word_analyses = {}
            for word in words:
                try:
                    parses = morph.parse(word)
                    if parses:
                        word_analyses[word] = parses[0]
                except:
                    continue
            
            keyword_found = False
            keyword_positions = []
            
            # æŸ¥æ‰¾å…³é”®è¯ä½ç½®
            for idx, word in enumerate(words):
                if word == keyword_lower:
                    keyword_found = True
                    keyword_positions.append(idx)
                elif word in word_analyses:
                    parse = word_analyses[word]
                    if parse.normal_form == keyword_lower:
                        keyword_found = True
                        keyword_positions.append(idx)
            
            # ğŸ”¥ ä¿®æ”¹ï¼šå¦‚æœæ²¡æœ‰å…³é”®è¯ï¼Œå½“å‰è¯„è®ºä¸åˆæ ¼ï¼Œä½†ä¸è¯´æ˜æ˜¯å› ä¸ºç¼ºå°‘å…³é”®è¯
            if not keyword_found:
                failing_count += 1
                log(f"[DEBUG] ç¬¬ {i+1} æ¡è¯„è®ºæœªæ‰¾åˆ°å…³é”®è¯")
                continue
            
            log(f"[DEBUG] å…³é”®è¯ä½ç½®: {keyword_positions}")
            
            # æœç´¢ä¸å…³é”®è¯æ­é…çš„è¯
            for kw_pos in keyword_positions:
                search_start = max(0, kw_pos - 5)
                search_end = min(len(words), kw_pos + 8)
                
                for word_idx in range(search_start, search_end):
                    if word_idx == kw_pos:
                        continue
                    
                    word = words[word_idx]
                    if word not in word_analyses:
                        continue
                    
                    parse = word_analyses[word]
                    
                    try:
                        # æ£€æŸ¥åŠ¨è¯
                        if 'VERB' in parse.tag:
                            if 'past' in parse.tag and 'sing' in parse.tag:
                                if hasattr(parse.tag, 'gender') and parse.tag.gender == expected_gender:
                                    agreement_details.add(f"åŠ¨è¯'{word}'")
                                    log(f"[DEBUG] æ‰¾åˆ°åŒ¹é…: åŠ¨è¯'{word}'")
                                    continue
                            
                            if hasattr(parse.tag, 'gender') and parse.tag.gender == expected_gender:
                                if 'past' in parse.tag or 'pres' in parse.tag:
                                    agreement_details.add(f"åŠ¨è¯'{word}'")
                                    log(f"[DEBUG] æ‰¾åˆ°åŒ¹é…: åŠ¨è¯'{word}'")
                        
                        # æ£€æŸ¥å½¢å®¹è¯
                        if 'ADJF' in parse.tag or 'ADJS' in parse.tag:
                            if 'nomn' in parse.tag and hasattr(parse.tag, 'gender'):
                                if parse.tag.gender == expected_gender:
                                    agreement_details.add(f"å½¢å®¹è¯'{word}'")
                                    log(f"[DEBUG] æ‰¾åˆ°åŒ¹é…: å½¢å®¹è¯'{word}'")
                        
                        # æ£€æŸ¥ç³»åŠ¨è¯
                        if parse.normal_form == 'Ğ±Ñ‹Ñ‚ÑŒ':
                            if 'past' in parse.tag and hasattr(parse.tag, 'gender'):
                                if parse.tag.gender == expected_gender:
                                    agreement_details.add(f"ç³»åŠ¨è¯'{word}'")
                                    log(f"[DEBUG] æ‰¾åˆ°åŒ¹é…: ç³»åŠ¨è¯'{word}'")
                        
                        # æ£€æŸ¥åˆ†è¯
                        if 'PRTF' in parse.tag or 'PRTS' in parse.tag:
                            if hasattr(parse.tag, 'gender') and parse.tag.gender == expected_gender:
                                agreement_details.add(f"åˆ†è¯'{word}'")
                                log(f"[DEBUG] æ‰¾åˆ°åŒ¹é…: åˆ†è¯'{word}'")
                                
                    except Exception as e:
                        log(f"[DEBUG] è§£æè¯ '{word}' æ—¶å‡ºé”™: {e}")
                        continue
            
            # ç‰¹æ®Šå¤„ç†å·²çŸ¥åŠ¨è¯
            special_verbs = ['Ğ²Ğ¿ĞµÑ‡Ğ°Ñ‚Ğ»Ğ¸Ğ»Ğ¾', 'Ğ²Ğ´Ğ¾Ñ…Ğ½Ğ¾Ğ²Ğ¸Ğ»Ğ¾', 'Ğ¿Ğ¾Ğ½Ñ€Ğ°Ğ²Ğ¸Ğ»Ğ¾ÑÑŒ', 'Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ¸Ğ»Ğ¾ÑÑŒ', 'Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾ÑÑŒ']
            for verb in special_verbs:
                if verb in text_lower:
                    try:
                        verb_parse = morph.parse(verb)[0]
                        if ('VERB' in verb_parse.tag and 
                            'past' in verb_parse.tag and 
                            'sing' in verb_parse.tag and 
                            hasattr(verb_parse.tag, 'gender') and
                            verb_parse.tag.gender == expected_gender):
                            agreement_details.add(f"åŠ¨è¯'{verb}'")
                            log(f"[DEBUG] ç‰¹æ®Šå¤„ç†æ‰¾åˆ°åŒ¹é…: åŠ¨è¯'{verb}'")
                    except:
                        pass
                        
        except Exception as e:
            log(f"[DEBUG] æ–‡æœ¬è§£æå‡ºé”™: {e}")
            if debug:
                import traceback
                traceback.print_exc()
            failing_count += 1
            continue
        
        found_count = len(agreement_details)
        log(f"[DEBUG] ç¬¬ {i+1} æ¡è¯„è®ºæ‰¾åˆ° {found_count} ä¸ªæ­é…")
        
        # ğŸ”¥ ä¿®æ”¹ï¼šå¦‚æœæ­é…æ•°é‡ä¸è¶³ï¼Œç®—ä½œä¸åˆæ ¼
        if found_count < min_agreements:
            failing_count += 1

    # ğŸ”¥ ä¿®æ”¹ï¼šç®€åŒ–è¾“å‡ºä¿¡æ¯
    if failing_count == 0:
        return 1, f"âœ… è¯æ€§æ­é…ç¬¦åˆè¦æ±‚\n   æ‰€æœ‰ {total_items} æ¡æ–‡æ¡ˆçš„åŠ¨è¯éƒ½ä¸å…³é”®è¯ '{keyword}' æ€§åˆ«ä¸€è‡´"
    else:
        return 0, f"âŒ è¯æ€§æ­é…ä¸ç¬¦åˆè¦æ±‚\n   æœ‰ {failing_count}/{total_items} æ¡æ–‡æ¡ˆçš„åŠ¨è¯ä¸å…³é”®è¯ '{keyword}' æ€§åˆ«ä¸ä¸€è‡´æˆ–æ­é…ä¸è¶³"



# ==================== è§„åˆ™ 13: åŠ¨è¯æ—¶é—´å…³ç³»æ£€æµ‹ ====================
def check_russian_verb_temporal_relation(content_list, expected_relation, debug=False):
    """
    æ£€æŸ¥ä¿„è¯­å¥å­ä¸­å‰¯åŠ¨è¯å’Œè°“è¯­åŠ¨è¯çš„æ—¶é—´å…³ç³»
    
    Args:
        content_list: æ–‡æœ¬å†…å®¹åˆ—è¡¨
        expected_relation: æœŸæœ›çš„æ—¶é—´å…³ç³»
        debug: æ˜¯å¦è¾“å‡ºè°ƒè¯•ä¿¡æ¯
    
    Returns:
        tuple: (1/0, è¯´æ˜ä¿¡æ¯)
    """
    log = create_logger(debug)
    
    morph, available = LibraryManager.get_morph()
    
    if not available:
        if debug:
            log("[WARNING] Pymorphy2 åº“æœªå®‰è£…")
        return 0, "âŒ è§„åˆ™è¯„ä¼°å¤±è´¥: Pymorphy2 åº“æœªå®‰è£…"
    else:
        log("[INFO] Pymorphy2 åº“åŠ è½½æˆåŠŸ")
    
    if not content_list or len(content_list) == 0 or not content_list[0]:
        return 0, "âŒ è¾“å…¥å†…å®¹ä¸ºç©º"
    if not expected_relation:
        return 0, "âŒ æœŸæœ›çš„æ—¶é—´å…³ç³»å‚æ•°ä¸ºç©º"
    
    sentence = content_list[0]
    expected_relation = expected_relation.strip()
    
    log(f"\n[DEBUG] æ£€æŸ¥å¥å­: {sentence}")
    log(f"[DEBUG] æœŸæœ›çš„æ—¶é—´å…³ç³»: {expected_relation}")

    try:
        # æ—¶é—´é¡ºåºæ ‡å¿—è¯
        temporal_markers = [
            'Ğ¿Ğ¾ÑĞ»Ğµ Ñ‚Ğ¾Ğ³Ğ¾ ĞºĞ°Ğº', 'Ğ¿Ğ¾ÑĞ»Ğµ', 'Ğ¿Ğ¾Ñ‚Ğ¾Ğ¼', 'Ğ·Ğ°Ñ‚ĞµĞ¼', 'ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ°', 
            'ÑĞ¿ĞµÑ€Ğ²Ğ°', 'Ğ¿Ñ€ĞµĞ¶Ğ´Ğµ Ñ‡ĞµĞ¼', 'Ğ¿ĞµÑ€ĞµĞ´ Ñ‚ĞµĞ¼ ĞºĞ°Ğº'
        ]
        
        # åŒæ—¶å…³ç³»æ ‡å¿—è¯
        simultaneous_markers = [
            'Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾', 'Ğ² Ñ‚Ğ¾ Ğ¶Ğµ Ğ²Ñ€ĞµĞ¼Ñ', 'Ğ² ÑÑ‚Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ', 'Ñ‚ĞµĞ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½ĞµĞ¼'
        ]
        
        sentence_lower = sentence.lower()
        has_temporal = any(m in sentence_lower for m in temporal_markers)
        has_simultaneous = any(m in sentence_lower for m in simultaneous_markers)
        
        log(f"[DEBUG] æ—¶é—´é¡ºåºæ ‡å¿—è¯: {has_temporal}")
        log(f"[DEBUG] åŒæ—¶å…³ç³»æ ‡å¿—è¯: {has_simultaneous}")
        
        words = re.findall(r'\b[Ğ°-ÑÑ‘Ğ-Ğ¯Ğ-]+\b', sentence)
        parses = [morph.parse(w)[0] for w in words]
        
        main_verbs = []
        subordinate_verbs = []
        participles = []
        all_verbs = []
        
        subordinate_markers = ['Ğ¿Ğ¾ÑĞ»Ğµ Ñ‚Ğ¾Ğ³Ğ¾ ĞºĞ°Ğº', 'ĞºĞ¾Ğ³Ğ´Ğ°', 'ĞµÑĞ»Ğ¸', 'ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹', 'ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ', 'ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğµ', 'ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ']
        
        for word_index, (word, parse) in enumerate(zip(words, parses)):
            pos = parse.tag.POS
            aspect = 'perf' if 'perf' in parse.tag else ('impf' if 'impf' in parse.tag else None)
            
            word_position = sentence.find(word)
            in_subordinate = False
            
            for marker in subordinate_markers:
                if marker in sentence_lower:
                    marker_pos = sentence_lower.find(marker)
                    comma_after = sentence.find(',', marker_pos)
                    if comma_after > 0:
                        if marker_pos < word_position < comma_after:
                            in_subordinate = True
                            break
            
            if pos == 'GRND':
                participles.append({
                    'word': word,
                    'lemma': parse.normal_form,
                    'aspect': aspect,
                    'parse': parse,
                    'position': word_index
                })
                log(f"[DEBUG] æ‰¾åˆ°å‰¯åŠ¨è¯: {word} (lemma: {parse.normal_form}, aspect: {aspect})")
            
            elif pos == 'VERB':
                verb_info = {
                    'word': word,
                    'lemma': parse.normal_form,
                    'aspect': aspect,
                    'parse': parse,
                    'position': word_index,
                    'in_subordinate': in_subordinate
                }
                all_verbs.append(verb_info)
                
                if in_subordinate:
                    subordinate_verbs.append(verb_info)
                    log(f"[DEBUG] æ‰¾åˆ°ä»å¥åŠ¨è¯: {word}")
                else:
                    main_verbs.append(verb_info)
                    log(f"[DEBUG] æ‰¾åˆ°ä¸»å¥åŠ¨è¯: {word}")
        
        if not all_verbs:
            return 0, "âŒ å¥å­ä¸­æœªæ‰¾åˆ°ä»»ä½•åŠ¨è¯ï¼Œæ— æ³•åˆ¤æ–­æ—¶é—´å…³ç³»"
        
        actual_relation = None
        explanation = []
        
        # åˆ¤æ–­æ—¶é—´å…³ç³»
        if has_simultaneous:
            actual_relation = "ĞĞ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ñ"
            explanation.append(f"å¥å­ä½¿ç”¨äº†åŒæ—¶å…³ç³»æ ‡å¿—è¯")
        
        if not actual_relation and has_temporal:
            perf_verbs = [v for v in all_verbs if v['aspect'] == 'perf']
            if len(perf_verbs) >= 2:
                actual_relation = "Ğ¥Ñ€Ğ¾Ğ½Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ"
                explanation.append(f"å¥å­ä½¿ç”¨äº†æ—¶é—´é¡ºåºæ ‡å¿—è¯ï¼Œå¹¶ä¸”æœ‰å¤šä¸ªå®Œæˆä½“åŠ¨è¯")
            elif perf_verbs:
                actual_relation = "Ğ¥Ñ€Ğ¾Ğ½Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ"
                explanation.append(f"å¥å­ä½¿ç”¨äº†æ—¶é—´é¡ºåºæ ‡å¿—è¯")
        
        if not actual_relation and participles:
            target_main = main_verbs if main_verbs else all_verbs[:1]
            
            if target_main:
                participle = participles[0]
                main_verb = target_main[0]
                
                main_aspect = main_verb['aspect']
                part_aspect = participle['aspect']
                
                log(f"[DEBUG] å‰¯åŠ¨è¯ä½“: {part_aspect}, ä¸»å¥åŠ¨è¯ä½“: {main_aspect}")
                
                if main_aspect == 'impf' and part_aspect == 'impf':
                    actual_relation = "ĞĞ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ñ"
                    explanation.append(f"å‰¯åŠ¨è¯å’Œè°“è¯­åŠ¨è¯éƒ½æ˜¯æœªå®Œæˆä½“")
                elif part_aspect == 'perf':
                    actual_relation = "Ğ¥Ñ€Ğ¾Ğ½Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ"
                    explanation.append(f"å‰¯åŠ¨è¯æ˜¯å®Œæˆä½“")
                elif part_aspect == 'impf' and main_aspect == 'perf':
                    actual_relation = "ĞĞ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ñ"
                    explanation.append(f"èƒŒæ™¯-äº‹ä»¶å…³ç³»")
                elif part_aspect == 'perf' and main_aspect == 'impf':
                    actual_relation = "Ğ¥Ñ€Ğ¾Ğ½Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ"
                    explanation.append(f"å‰¯åŠ¨è¯å®Œæˆä½“ + è°“è¯­æœªå®Œæˆä½“")
        
        if not actual_relation and not participles:
            perf_verbs = [v for v in all_verbs if v['aspect'] == 'perf']
            if len(perf_verbs) >= 2 and has_temporal:
                actual_relation = "Ğ¥Ñ€Ğ¾Ğ½Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ"
                explanation.append(f"è¿ç»­å®Œæˆä½“åŠ¨è¯ + æ—¶é—´é¡ºåºæ ‡å¿—è¯")
        
        if not actual_relation:
            return 0, "âŒ æ— æ³•æ˜ç¡®åˆ¤æ–­æ—¶é—´å…³ç³»"
        
        explanation_text = ' '.join(explanation)
        log(f"[DEBUG] å®é™…æ—¶é—´å…³ç³»: {actual_relation}")
        
        if actual_relation == expected_relation:
            return 1, f"âœ… æ—¶é—´å…³ç³»æ­£ç¡®: {actual_relation}ã€‚{explanation_text}"
        else:
            return 0, f"âŒ æ—¶é—´å…³ç³»ä¸ç¬¦: æœŸæœ› '{expected_relation}'ï¼Œå®é™…ä¸º '{actual_relation}'ã€‚{explanation_text}"
    
    except Exception as e:
        log(f"[ERROR] å¼‚å¸¸: {e}")
        if debug:
            import traceback
            traceback.print_exc()
        return 0, f"âŒ å‡½æ•°æ‰§è¡Œå¼‚å¸¸: {e}"


# ==================== è§„åˆ™ 14: ä¿„è¯­å½¢å®¹è¯ç±»å‹æ£€æµ‹ ====================
def russian_adjective_type_count(content_list, adj_type_str, expected_count_str, debug=False):
    """
    æ£€æŸ¥ä¿„è¯­æ–‡æœ¬ä¸­æŒ‡å®šç±»å‹å½¢å®¹è¯çš„æ•°é‡ï¼ˆä¸¥æ ¼åŒºåˆ†å‰¯è¯å’ŒçŸ­å°¾å½¢å®¹è¯ï¼‰
    
    Args:
        content_list: æ–‡æœ¬å†…å®¹åˆ—è¡¨
        adj_type_str: å½¢å®¹è¯ç±»å‹ï¼ˆshort/long/ĞºÑ€Ğ°Ñ‚ĞºĞ¸Ğµ Ğ¿Ñ€Ğ¸Ğ»Ğ°Ğ³Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ/Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ»Ğ°Ğ³Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµï¼‰
        expected_count_str: æœŸæœ›æ•°é‡
        debug: æ˜¯å¦è¾“å‡ºè°ƒè¯•ä¿¡æ¯
    
    Returns:
        tuple: (1/0, è¯´æ˜ä¿¡æ¯)
    """
    log = create_logger(debug)
    
    morph, available = LibraryManager.get_morph()
    
    if not available:
        log("[ERROR] Pymorphy2 åº“æœªèƒ½æ­£ç¡®åŠ è½½")
        return 0, "âŒ Pymorphy2 åº“æœªèƒ½æ­£ç¡®åŠ è½½ï¼Œè¯·æ£€æŸ¥å®‰è£…"
    else:
        log("[INFO] Pymorphy2 åº“åŠ è½½æˆåŠŸ")
    
    if not content_list or content_list == "INVALID":
        return 0, "âŒ è¾“å…¥å†…å®¹ä¸ºç©º"

    adj_type_mapping = {
        'short': {'tag': 'ADJS', 'name': 'çŸ­å°¾å½¢å®¹è¯'},
        'long': {'tag': 'ADJF', 'name': 'å®Œå…¨å½¢å®¹è¯'},
        'ĞºÑ€Ğ°Ñ‚ĞºĞ¸Ğµ Ğ¿Ñ€Ğ¸Ğ»Ğ°Ğ³Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ': {'tag': 'ADJS', 'name': 'çŸ­å°¾å½¢å®¹è¯'},
        'Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ»Ğ°Ğ³Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ': {'tag': 'ADJF', 'name': 'å®Œå…¨å½¢å®¹è¯'}
    }
    
    target_info = adj_type_mapping.get(str(adj_type_str).strip().lower())
    if not target_info:
        return 0, f"âŒ ç±»å‹å‚æ•°æ— æ•ˆ: '{adj_type_str}'"
    
    try:
        expected_count = int(expected_count_str)
    except (ValueError, TypeError):
        return 0, f"âŒ æ•°é‡å‚æ•°æ— æ•ˆ: '{expected_count_str}'"

    found_adjectives = []
    filtered_words = []
    
    if not isinstance(content_list, list):
        content_list = [str(content_list)]

    # æ‰©å……ä»£è¯é»‘åå•
    PRONOUN_BLACKLIST = {
        'Ğ²ÑĞµ', 'Ğ²ÑÑ‘', 'Ğ²ĞµÑÑŒ', 'Ğ²ÑÑ', 'Ğ²ÑĞµĞ³Ğ¾', 'Ğ²ÑĞµÑ…', 'Ğ²ÑĞµĞ¼', 'Ğ²ÑĞµĞ¼Ğ¸', 'Ğ²ÑĞµĞ¼Ñƒ', 'Ğ²ÑĞµĞ¹', 'Ğ²ÑĞµÑ',
        'ÑÑ‚Ğ¾Ñ‚', 'ÑÑ‚Ğ°', 'ÑÑ‚Ğ¾', 'ÑÑ‚Ğ¸', 'ÑÑ‚Ğ¾Ğ³Ğ¾', 'ÑÑ‚Ğ¾Ğ¹', 'ÑÑ‚Ğ¸Ñ…', 'ÑÑ‚Ğ¾Ğ¼Ñƒ', 'ÑÑ‚Ğ¸Ğ¼', 'ÑÑ‚Ğ¸Ğ¼Ğ¸', 'ÑÑ‚Ğ¾Ñ',
        'Ñ‚Ğ¾Ñ‚', 'Ñ‚Ğ°', 'Ñ‚Ğ¾', 'Ñ‚Ğµ', 'Ñ‚Ğ¾Ğ³Ğ¾', 'Ñ‚Ğ¾Ğ¹', 'Ñ‚ĞµÑ…', 'Ñ‚Ğ¾Ğ¼Ñƒ', 'Ñ‚ĞµĞ¼', 'Ñ‚ĞµĞ¼Ğ¸', 'Ñ‚Ğ¾Ñ',
        'Ñ‚Ğ°ĞºĞ¾Ğ¹', 'Ñ‚Ğ°ĞºĞ°Ñ', 'Ñ‚Ğ°ĞºĞ¾Ğµ', 'Ñ‚Ğ°ĞºĞ¸Ğµ', 'Ñ‚Ğ°ĞºĞ¾Ğ³Ğ¾', 'Ñ‚Ğ°ĞºĞ¸Ñ…', 'Ñ‚Ğ°ĞºĞ¾Ğ¼Ñƒ', 'Ñ‚Ğ°ĞºĞ¸Ğ¼', 'Ñ‚Ğ°ĞºĞ¸Ğ¼Ğ¸', 'Ñ‚Ğ°ĞºĞ¾Ñ',
        'ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹', 'ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ', 'ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğµ', 'ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ', 'ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğ³Ğ¾', 'ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğ¹', 'ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ñ…',
        'ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğ¼Ñƒ', 'ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¼', 'ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¼Ğ¸', 'ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ñ',
        'ĞºĞ°Ğ¶Ğ´Ñ‹Ğ¹', 'ĞºĞ°Ğ¶Ğ´Ğ°Ñ', 'ĞºĞ°Ğ¶Ğ´Ğ¾Ğµ', 'ĞºĞ°Ğ¶Ğ´Ñ‹Ğµ', 'ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾', 'ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¹', 'ĞºĞ°Ğ¶Ğ´Ñ‹Ñ…',
        'ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¼Ñƒ', 'ĞºĞ°Ğ¶Ğ´Ñ‹Ğ¼', 'ĞºĞ°Ğ¶Ğ´Ñ‹Ğ¼Ğ¸', 'ĞºĞ°Ğ¶Ğ´Ğ¾Ñ',
        'Ğ»ÑĞ±Ğ¾Ğ¹', 'Ğ»ÑĞ±Ğ°Ñ', 'Ğ»ÑĞ±Ğ¾Ğµ', 'Ğ»ÑĞ±Ñ‹Ğµ', 'Ğ»ÑĞ±Ğ¾Ğ³Ğ¾', 'Ğ»ÑĞ±Ñ‹Ñ…', 'Ğ»ÑĞ±Ğ¾Ğ¹',
        'Ğ»ÑĞ±Ğ¾Ğ¼Ñƒ', 'Ğ»ÑĞ±Ñ‹Ğ¼', 'Ğ»ÑĞ±Ñ‹Ğ¼Ğ¸', 'Ğ»ÑĞ±Ğ¾Ñ',
        'Ğ´Ñ€ÑƒĞ³Ğ¾Ğ¹', 'Ğ´Ñ€ÑƒĞ³Ğ°Ñ', 'Ğ´Ñ€ÑƒĞ³Ğ¾Ğµ', 'Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ', 'Ğ´Ñ€ÑƒĞ³Ğ¾Ğ³Ğ¾', 'Ğ´Ñ€ÑƒĞ³Ğ¸Ñ…', 'Ğ´Ñ€ÑƒĞ³Ğ¾Ğ¹',
        'Ğ´Ñ€ÑƒĞ³Ğ¾Ğ¼Ñƒ', 'Ğ´Ñ€ÑƒĞ³Ğ¸Ğ¼', 'Ğ´Ñ€ÑƒĞ³Ğ¸Ğ¼Ğ¸', 'Ğ´Ñ€ÑƒĞ³Ğ¾Ñ',
        'ÑĞ°Ğ¼', 'ÑĞ°Ğ¼Ğ°', 'ÑĞ°Ğ¼Ğ¾', 'ÑĞ°Ğ¼Ğ¸', 'ÑĞ°Ğ¼Ğ¾Ğ³Ğ¾', 'ÑĞ°Ğ¼Ğ¾Ğ¹', 'ÑĞ°Ğ¼Ğ¸Ñ…',
        'ÑĞ°Ğ¼Ğ¾Ğ¼Ñƒ', 'ÑĞ°Ğ¼Ğ¸Ğ¼', 'ÑĞ°Ğ¼Ğ¸Ğ¼Ğ¸', 'ÑĞ°Ğ¼Ğ¾Ñ',
        'Ğ¼Ğ¾Ğ¹', 'Ğ¼Ğ¾Ñ', 'Ğ¼Ğ¾Ñ‘', 'Ğ¼Ğ¾Ğ¸', 'Ğ¼Ğ¾ĞµĞ³Ğ¾', 'Ğ¼Ğ¾ĞµĞ¹', 'Ğ¼Ğ¾Ğ¸Ñ…', 'Ğ¼Ğ¾ĞµĞ¼Ñƒ', 'Ğ¼Ğ¾Ğ¸Ğ¼', 'Ğ¼Ğ¾Ğ¸Ğ¼Ğ¸', 'Ğ¼Ğ¾ĞµÑ',
        'Ñ‚Ğ²Ğ¾Ğ¹', 'Ñ‚Ğ²Ğ¾Ñ', 'Ñ‚Ğ²Ğ¾Ñ‘', 'Ñ‚Ğ²Ğ¾Ğ¸', 'Ñ‚Ğ²Ğ¾ĞµĞ³Ğ¾', 'Ñ‚Ğ²Ğ¾ĞµĞ¹', 'Ñ‚Ğ²Ğ¾Ğ¸Ñ…', 'Ñ‚Ğ²Ğ¾ĞµĞ¼Ñƒ', 'Ñ‚Ğ²Ğ¾Ğ¸Ğ¼', 'Ñ‚Ğ²Ğ¾Ğ¸Ğ¼Ğ¸', 'Ñ‚Ğ²Ğ¾ĞµÑ',
        'ĞµĞ³Ğ¾', 'ĞµÑ‘', 'Ğ¸Ñ…',
        'Ğ½Ğ°Ñˆ', 'Ğ½Ğ°ÑˆĞ°', 'Ğ½Ğ°ÑˆĞµ', 'Ğ½Ğ°ÑˆĞ¸', 'Ğ½Ğ°ÑˆĞµĞ³Ğ¾', 'Ğ½Ğ°ÑˆĞµĞ¹', 'Ğ½Ğ°ÑˆĞ¸Ñ…', 'Ğ½Ğ°ÑˆĞµĞ¼Ñƒ', 'Ğ½Ğ°ÑˆĞ¸Ğ¼', 'Ğ½Ğ°ÑˆĞ¸Ğ¼Ğ¸', 'Ğ½Ğ°ÑˆĞµÑ',
        'Ğ²Ğ°Ñˆ', 'Ğ²Ğ°ÑˆĞ°', 'Ğ²Ğ°ÑˆĞµ', 'Ğ²Ğ°ÑˆĞ¸', 'Ğ²Ğ°ÑˆĞµĞ³Ğ¾', 'Ğ²Ğ°ÑˆĞµĞ¹', 'Ğ²Ğ°ÑˆĞ¸Ñ…', 'Ğ²Ğ°ÑˆĞµĞ¼Ñƒ', 'Ğ²Ğ°ÑˆĞ¸Ğ¼', 'Ğ²Ğ°ÑˆĞ¸Ğ¼Ğ¸', 'Ğ²Ğ°ÑˆĞµÑ',
        'ÑĞ²Ğ¾Ğ¹', 'ÑĞ²Ğ¾Ñ', 'ÑĞ²Ğ¾Ñ‘', 'ÑĞ²Ğ¾Ğ¸', 'ÑĞ²Ğ¾ĞµĞ³Ğ¾', 'ÑĞ²Ğ¾ĞµĞ¹', 'ÑĞ²Ğ¾Ğ¸Ñ…', 'ÑĞ²Ğ¾ĞµĞ¼Ñƒ', 'ÑĞ²Ğ¾Ğ¸Ğ¼', 'ÑĞ²Ğ¾Ğ¸Ğ¼Ğ¸', 'ÑĞ²Ğ¾ĞµÑ',
        'ĞºĞ°ĞºĞ¾Ğ¹', 'ĞºĞ°ĞºĞ°Ñ', 'ĞºĞ°ĞºĞ¾Ğµ', 'ĞºĞ°ĞºĞ¸Ğµ', 'ĞºĞ°ĞºĞ¾Ğ³Ğ¾', 'ĞºĞ°ĞºĞ¸Ñ…', 'ĞºĞ°ĞºĞ¾Ğ¼Ñƒ', 'ĞºĞ°ĞºĞ¸Ğ¼', 'ĞºĞ°ĞºĞ¸Ğ¼Ğ¸', 'ĞºĞ°ĞºĞ¾Ñ',
        'Ñ‡ĞµĞ¹', 'Ñ‡ÑŒÑ', 'Ñ‡ÑŒÑ‘', 'Ñ‡ÑŒĞ¸', 'Ñ‡ÑŒĞµĞ³Ğ¾', 'Ñ‡ÑŒĞµĞ¹', 'Ñ‡ÑŒĞ¸Ñ…', 'Ñ‡ÑŒĞµĞ¼Ñƒ', 'Ñ‡ÑŒĞ¸Ğ¼', 'Ñ‡ÑŒĞ¸Ğ¼Ğ¸', 'Ñ‡ÑŒĞµÑ',
        'Ğ½ĞµĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹', 'Ğ½ĞµĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ', 'Ğ½ĞµĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğµ', 'Ğ½ĞµĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ',
        'Ğ½ĞµĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğ³Ğ¾', 'Ğ½ĞµĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ñ…', 'Ğ½ĞµĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğ¼Ñƒ', 'Ğ½ĞµĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¼', 'Ğ½ĞµĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¼Ğ¸',
        'Ğ½Ğ¸ĞºĞ°ĞºĞ¾Ğ¹', 'Ğ½Ğ¸ĞºĞ°ĞºĞ°Ñ', 'Ğ½Ğ¸ĞºĞ°ĞºĞ¾Ğµ', 'Ğ½Ğ¸ĞºĞ°ĞºĞ¸Ğµ',
        'Ğ½Ğ¸ĞºĞ°ĞºĞ¾Ğ³Ğ¾', 'Ğ½Ğ¸ĞºĞ°ĞºĞ¸Ñ…', 'Ğ½Ğ¸ĞºĞ°ĞºĞ¾Ğ¼Ñƒ', 'Ğ½Ğ¸ĞºĞ°ĞºĞ¸Ğ¼', 'Ğ½Ğ¸ĞºĞ°ĞºĞ¸Ğ¼Ğ¸',
    }
    
    PURE_ADVERB_BLACKLIST = {
        'Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾', 'Ñ‡Ğ°ÑÑ‚Ğ¾', 'Ñ€ĞµĞ´ĞºĞ¾', 'Ğ´Ğ¾Ğ»Ğ³Ğ¾', 'ÑĞºĞ¾Ñ€Ğ¾', 'Ğ½ĞµĞ´Ğ°Ğ²Ğ½Ğ¾', 'Ğ´Ğ°Ğ²Ğ½Ğ¾', 
        'Ğ¼ĞµĞ´Ğ»ĞµĞ½Ğ½Ğ¾', 'Ğ¼Ğ½Ğ¾Ğ³Ğ¾', 'Ğ¼Ğ°Ğ»Ğ¾', 'Ğ½ĞµĞ¼Ğ½Ğ¾Ğ³Ğ¾',
        'Ğ¾Ñ‡ĞµĞ½ÑŒ', 'ÑĞ»Ğ¸ÑˆĞºĞ¾Ğ¼', 'Ğ´Ğ¾Ğ²Ğ¾Ğ»ÑŒĞ½Ğ¾', 'ÑĞ¾Ğ²ÑĞµĞ¼', 'Ğ²Ğ¿Ğ¾Ğ»Ğ½Ğµ', 'Ğ¿Ğ¾Ñ‡Ñ‚Ğ¸',
        'Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾', 'Ğ»Ğ¸ÑˆÑŒ', 'Ğ´Ğ°Ğ¶Ğµ', 'ÑƒĞ¶Ğµ', 'ĞµÑ‰Ñ‘', 'Ğ²ÑĞµĞ³Ğ´Ğ°', 'Ğ½Ğ¸ĞºĞ¾Ğ³Ğ´Ğ°',
        'Ğ¸Ğ½Ğ¾Ğ³Ğ´Ğ°', 'Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ğ¾', 'Ğ¾Ğ±ÑĞ·Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾', 'ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾', 'ÑĞ»ÑƒÑ‡Ğ°Ğ¹Ğ½Ğ¾',
        'Ğ²Ğ¼ĞµÑÑ‚Ğµ', 'Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ğ¾', 'Ğ²Ğ´Ñ€ÑƒĞ³', 'ÑÑ€Ğ°Ğ·Ñƒ', 'Ğ¿Ğ¾Ñ‚Ğ¾Ğ¼', 'ÑĞµĞ¹Ñ‡Ğ°Ñ',
        'ÑĞ½Ğ¾Ğ²Ğ°', 'Ğ¾Ğ¿ÑÑ‚ÑŒ', 'Ğ²ĞµĞ·Ğ´Ğµ', 'Ğ²ÑÑĞ´Ñƒ', 'Ğ½Ğ¸Ğ³Ğ´Ğµ', 'Ğ½Ğ¸ĞºÑƒĞ´Ğ°',
        'Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾', 'Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾', 'ĞºĞ¾Ğ½ĞµÑ‡Ğ½Ğ¾', 'Ğ±ĞµĞ·ÑƒÑĞ»Ğ¾Ğ²Ğ½Ğ¾',
    }
    
    PURE_PREDICATIVE_BLACKLIST = {
        'Ğ¶Ğ°Ğ»ÑŒ', 'Ğ¶Ğ°Ğ»ĞºĞ¾', 'Ğ¿Ğ¾Ñ€Ğ°', 'Ğ»ĞµĞ½ÑŒ', 'Ğ½ĞµĞ´Ğ¾ÑÑƒĞ³', 'Ğ¾Ñ…Ğ¾Ñ‚Ğ°', 'Ğ½ĞµĞ¾Ñ…Ğ¾Ñ‚Ğ°',
        'Ğ³Ñ€ĞµÑ…', 'ÑÑ‚Ñ‹Ğ´Ğ½Ğ¾', 'ÑĞ¾Ğ²ĞµÑÑ‚Ğ½Ğ¾', 'Ğ½ĞµĞ²Ğ¼Ğ¾Ğ³Ğ¾Ñ‚Ñƒ', 'Ğ½ĞµĞ²Ñ‚ĞµÑ€Ğ¿Ñ‘Ğ¶', 'Ğ½ĞµĞ²Ñ‚ĞµÑ€Ğ¿ĞµĞ¶',
        'Ğ²Ğ¸Ğ´Ğ½Ğ¾', 'ÑĞ»Ñ‹ÑˆĞ½Ğ¾',
    }
    
    MODAL_WORDS_BLACKLIST = {
        'Ğ¼Ğ¾Ğ¶Ğ½Ğ¾', 'Ğ½ĞµĞ»ÑŒĞ·Ñ', 'Ğ½Ğ°Ğ´Ğ¾', 'Ğ½ÑƒĞ¶Ğ½Ğ¾', 'Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾', 'Ğ´Ğ¾Ğ»Ğ¶Ğ½Ğ¾', 'ÑĞ»ĞµĞ´ÑƒĞµÑ‚',
    }
    
    CONTEXT_DEPENDENT_WORDS = {
        'Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾', 'Ğ¿Ğ»Ğ¾Ñ…Ğ¾', 'Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¾', 'Ğ»ĞµĞ³ĞºĞ¾', 'Ğ¸Ğ½Ñ‚ĞµÑ€ĞµÑĞ½Ğ¾', 'ÑĞºÑƒÑ‡Ğ½Ğ¾',
        'Ğ¿Ñ€Ğ¸ÑÑ‚Ğ½Ğ¾', 'Ğ½ĞµĞ¿Ñ€Ğ¸ÑÑ‚Ğ½Ğ¾', 'ÑƒĞ´Ğ¾Ğ±Ğ½Ğ¾', 'Ğ½ĞµÑƒĞ´Ğ¾Ğ±Ğ½Ğ¾', 'Ğ²Ğ°Ğ¶Ğ½Ğ¾',
        'Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾', 'Ğ½ĞµĞ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾', 'Ğ¿Ğ¾Ğ½ÑÑ‚Ğ½Ğ¾', 'Ğ½ĞµĞ¿Ğ¾Ğ½ÑÑ‚Ğ½Ğ¾', 'ÑÑĞ½Ğ¾', 'Ñ‚ĞµĞ¼Ğ½Ğ¾', 'ÑĞ²ĞµÑ‚Ğ»Ğ¾',
        'Ñ…Ğ¾Ğ»Ğ¾Ğ´Ğ½Ğ¾', 'Ğ¶Ğ°Ñ€ĞºĞ¾', 'Ñ‚ĞµĞ¿Ğ»Ğ¾', 'Ğ¿Ñ€Ğ¾Ñ…Ğ»Ğ°Ğ´Ğ½Ğ¾', 'Ğ´ÑƒÑˆĞ½Ğ¾', 'ÑĞ²ĞµĞ¶Ğ¾',
        'Ñ‚Ğ¸Ñ…Ğ¾', 'Ğ³Ñ€Ğ¾Ğ¼ĞºĞ¾', 'ÑˆÑƒĞ¼Ğ½Ğ¾', 'ÑĞ¿Ğ¾ĞºĞ¾Ğ¹Ğ½Ğ¾', 'Ğ²ĞµÑĞµĞ»Ğ¾', 'Ğ³Ñ€ÑƒÑÑ‚Ğ½Ğ¾',
        'ÑÑ‚Ñ€Ğ°ÑˆĞ½Ğ¾', 'Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾', 'Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾', 'Ğ¿Ğ¾Ğ»ĞµĞ·Ğ½Ğ¾', 'Ğ²Ñ€ĞµĞ´Ğ½Ğ¾',
        'Ğ·Ğ°Ğ¼ĞµÑ‡Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾', 'Ğ¿Ñ€ĞµĞºÑ€Ğ°ÑĞ½Ğ¾', 'Ñ‡ÑƒĞ´ĞµÑĞ½Ğ¾', 'ÑƒĞ¶Ğ°ÑĞ½Ğ¾', 'ÑÑ‚Ñ€Ğ°Ğ½Ğ½Ğ¾',
        'ÑÑ€ĞºĞ¾', 'Ñ‚ÑƒÑĞºĞ»Ğ¾', 'Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾', 'Ğ¶Ğ¸Ğ²Ğ¾Ğ¿Ğ¸ÑĞ½Ğ¾', 'ĞºÑ€Ğ°ÑĞ¸Ğ²Ğ¾',
        'Ğ½ĞµĞ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾', 'ÑƒĞ´Ğ¸Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾', 'Ğ¿Ğ¾Ñ€Ğ°Ğ·Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾',
    }
    
    NOUN_LEMMA_BLACKLIST = {
        'Ğ²ÑÑ‘', 'Ğ¿Ñ€Ğ¾ÑˆĞ»Ğ¾Ğµ', 'Ğ±ÑƒĞ´ÑƒÑ‰ĞµĞµ', 'Ğ½Ğ°ÑÑ‚Ğ¾ÑÑ‰ĞµĞµ', 'Ğ³Ğ»Ğ°Ğ²Ğ½Ğ¾Ğµ', 'Ğ²Ğ°Ğ¶Ğ½Ğ¾Ğµ',
        'Ğ½Ğ¾Ğ²Ğ¾Ğµ', 'ÑÑ‚Ğ°Ñ€Ğ¾Ğµ', 'Ñ…Ğ¾Ñ€Ğ¾ÑˆĞµĞµ', 'Ğ¿Ğ»Ğ¾Ñ…Ğ¾Ğµ', 'Ğ»ÑƒÑ‡ÑˆĞµĞµ', 'Ñ…ÑƒĞ´ÑˆĞµĞµ',
        'Ğ²ĞµÑ‰ÑŒ', 'Ğ´ĞµĞ»Ğ¾', 'Ğ¼ĞµÑÑ‚Ğ¾', 'Ğ²Ñ€ĞµĞ¼Ñ', 'ÑĞ»Ğ¾Ğ²Ğ¾', 'Ğ»Ğ¸Ñ†Ğ¾'
    }
    
    NOUN_FORM_BLACKLIST = {
        'Ğ²ĞµÑ‰Ğ¸', 'Ğ´ĞµĞ»Ğ°', 'Ğ¼ĞµÑÑ‚Ğ°', 'Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ°', 'ÑĞ»Ğ¾Ğ²Ğ°', 'Ğ»Ğ¸Ñ†Ğ°',
        'Ğ³Ğ»Ğ°Ğ·Ğ°', 'Ñ€ÑƒĞºĞ¸', 'Ğ½Ğ¾Ğ³Ğ¸', 'Ğ³Ğ¾Ğ»Ğ¾Ğ²Ñ‹', 'ÑĞµÑ€Ğ´Ñ†Ğ°', 'Ğ´ÑƒÑˆĞ¸'
    }
    
    processed_positions = set()

    for seg_idx, text_segment in enumerate(content_list):
        text = str(text_segment)
        if not text.strip():
            continue
        
        log(f"[DEBUG] å¤„ç†æ®µè½ {seg_idx + 1}")
        
        for match in re.finditer(r'\b[Ğ°-ÑÑ‘Ğ-Ğ¯Ğ]+\b', text):
            word = match.group(0)
            start_pos = match.start()
            word_lower = word.lower()
            
            # ğŸ”¥ ç‰¹åˆ«æ ‡è®°æ–°çš„
            is_novye = (word_lower == 'Ğ½Ğ¾Ğ²Ñ‹Ğµ')
            if is_novye:
                log(f"[DEBUG] âš ï¸âš ï¸âš ï¸ å¼€å§‹å¤„ç†'Ğ½Ğ¾Ğ²Ñ‹Ğµ' at pos {start_pos}")
            
            position_key = (seg_idx, start_pos, word_lower)
            if position_key in processed_positions:
                if is_novye:
                    log(f"[DEBUG] âŒ 'Ğ½Ğ¾Ğ²Ñ‹Ğµ'å› é‡å¤è¢«è·³è¿‡")
                continue
            processed_positions.add(position_key)
            
            if word_lower in PRONOUN_BLACKLIST:
                if is_novye:
                    log(f"[DEBUG] âŒ 'Ğ½Ğ¾Ğ²Ñ‹Ğµ'åœ¨ä»£è¯é»‘åå•ä¸­")
                filtered_words.append(f"{word}(ä»£è¯é»‘åå•)")
                continue
            
            if target_info['tag'] == 'ADJS' and word_lower in PURE_PREDICATIVE_BLACKLIST:
                if is_novye:
                    log(f"[DEBUG] âŒ 'Ğ½Ğ¾Ğ²Ñ‹Ğµ'åœ¨çŠ¶æ€è¯é»‘åå•ä¸­")
                filtered_words.append(f"{word}(çº¯çŠ¶æ€è¯)")
                continue
            
            if target_info['tag'] == 'ADJS' and word_lower in MODAL_WORDS_BLACKLIST:
                if is_novye:
                    log(f"[DEBUG] âŒ 'Ğ½Ğ¾Ğ²Ñ‹Ğµ'åœ¨æƒ…æ€è¯é»‘åå•ä¸­")
                filtered_words.append(f"{word}(æƒ…æ€è¯)")
                continue
            
            if word_lower in NOUN_FORM_BLACKLIST:
                if is_novye:
                    log(f"[DEBUG] âŒ 'Ğ½Ğ¾Ğ²Ñ‹Ğµ'åœ¨åè¯è¯å½¢é»‘åå•ä¸­")
                filtered_words.append(f"{word}(çº¯åè¯è¯å½¢)")
                continue
            
            if target_info['tag'] == 'ADJS' and word_lower in PURE_ADVERB_BLACKLIST:
                if is_novye:
                    log(f"[DEBUG] âŒ 'Ğ½Ğ¾Ğ²Ñ‹Ğµ'åœ¨çº¯å‰¯è¯é»‘åå•ä¸­")
                filtered_words.append(f"{word}(çº¯å‰¯è¯)")
                continue
            
            try:
                parses = morph.parse(word)
                if not parses:
                    if is_novye:
                        log(f"[DEBUG] âŒ 'Ğ½Ğ¾Ğ²Ñ‹Ğµ'æ— æ³•è§£æ")
                    continue
                
                if is_novye:
                    log(f"[DEBUG] 'Ğ½Ğ¾Ğ²Ñ‹Ğµ'çš„è§£æç»“æœ:")
                    for p in parses:
                        log(f"[DEBUG]   POS={p.tag.POS}, lemma={p.normal_form}")
                
                best_parse = None
                has_noun = False
                noun_parse = None
                has_adj = False
                adj_parse = None
                has_adverb = False
                has_adjs = False
                adjs_parse = None
                has_pred = False
                has_pronoun = False
                
                all_noun = all(p.tag.POS == 'NOUN' for p in parses)
                
                if all_noun:
                    if is_novye:
                        log(f"[DEBUG] âŒ 'Ğ½Ğ¾Ğ²Ñ‹Ğµ'æ‰€æœ‰è§£æéƒ½æ˜¯åè¯")
                    filtered_words.append(f"{word}(çº¯åè¯)")
                    continue
                
                for parse in parses:
                    pos = parse.tag.POS
                    lemma = parse.normal_form
                    
                    log(f"[DEBUG] è¯ '{word}' çš„è§£æ: POS={pos}, lemma={lemma}")
                    
                    if pos == 'NPRO':
                        has_pronoun = True
                    if pos == 'PRED':
                        has_pred = True
                    if pos == 'ADVB':
                        has_adverb = True
                    if pos == 'ADJS':
                        has_adjs = True
                        adjs_parse = parse
                    if pos == 'NOUN':
                        has_noun = True
                        noun_parse = parse
                    
                    if lemma in PRONOUN_BLACKLIST:
                        has_pronoun = True
                    
                    if target_info['tag'] == 'ADJF':
                        if pos in ['ADJF', 'PRTF']:
                            has_adj = True
                            adj_parse = parse
                            if not best_parse:
                                best_parse = parse
                    elif target_info['tag'] == 'ADJS':
                        if pos == 'ADJS':
                            has_adj = True
                            adj_parse = parse
                            if not best_parse:
                                best_parse = parse
                
                if is_novye:
                    log(f"[DEBUG] 'Ğ½Ğ¾Ğ²Ñ‹Ğµ'çš„æ ‡è®°: has_adj={has_adj}, has_noun={has_noun}, has_pronoun={has_pronoun}")
                
                if has_pronoun:
                    if is_novye:
                        log(f"[DEBUG] âŒ 'Ğ½Ğ¾Ğ²Ñ‹Ğµ'è¢«åˆ¤å®šä¸ºä»£è¯")
                    filtered_words.append(f"{word}(ä»£è¯-è¯æ€§åˆ†æ)")
                    continue
                
                # ğŸ”¥ ä¿®å¤ï¼šåªæœ‰åœ¨æ²¡æœ‰å½¢å®¹è¯è§£ææ—¶æ‰æ’é™¤åè¯é»‘åå•
                if has_noun and noun_parse and noun_parse.normal_form in NOUN_LEMMA_BLACKLIST:
                    if not has_adj:  # ğŸ”¥ å…³é”®ä¿®å¤
                        if is_novye:
                            log(f"[DEBUG] âŒ 'Ğ½Ğ¾Ğ²Ñ‹Ğµ'åœ¨åè¯é»‘åå•ä¸”æ— å½¢å®¹è¯è§£æ")
                        filtered_words.append(f"{word}(åè¯é»‘åå•-{noun_parse.normal_form})")
                        continue
                    else:
                        if is_novye:
                            log(f"[DEBUG] âœ“ 'Ğ½Ğ¾Ğ²Ñ‹Ğµ'è™½åœ¨åè¯é»‘åå•ä½†æœ‰å½¢å®¹è¯è§£æï¼Œä¿ç•™")
                
                if has_noun and not has_adj and not has_adjs:
                    if is_novye:
                        log(f"[DEBUG] âŒ 'Ğ½Ğ¾Ğ²Ñ‹Ğµ'åªæœ‰åè¯è§£æ")
                    filtered_words.append(f"{word}(åªæœ‰åè¯è§£æ)")
                    continue
                
                if has_noun and noun_parse and noun_parse.normal_form in ['Ğ¿Ñ€Ğ¾ÑˆĞ»Ğ¾Ğµ', 'Ğ±ÑƒĞ´ÑƒÑ‰ĞµĞµ', 'Ğ½Ğ°ÑÑ‚Ğ¾ÑÑ‰ĞµĞµ']:
                    words_before = re.findall(r'\b[Ğ°-ÑÑ‘]+\b', text[:start_pos].lower())
                    prepositions = ['Ğ¸Ğ·', 'Ğ²', 'Ğº', 'Ğ¾Ñ‚', 'Ğ´Ğ»Ñ', 'Ğ¿Ñ€Ğ¾', 'Ğ¾', 'Ğ¾Ğ±', 'Ğ½Ğ°', 'Ğ¿Ñ€Ğ¸', 'Ğ¿Ğ¾']
                    if words_before and words_before[-1] in prepositions:
                        filtered_words.append(f"{word}(ä»‹è¯+åè¯)")
                        continue
                    if has_adj:
                        best_parse = adj_parse
                
                if best_parse is None:
                    best_parse = parses[0]
                
                main_pos = best_parse.tag.POS
                main_lemma = best_parse.normal_form
                
                if is_novye:
                    log(f"[DEBUG] 'Ğ½Ğ¾Ğ²Ñ‹Ğµ' best_parse: POS={main_pos}, lemma={main_lemma}")
                
                if main_pos == 'NPRO':
                    if is_novye:
                        log(f"[DEBUG] âŒ 'Ğ½Ğ¾Ğ²Ñ‹Ğµ'çš„main_posæ˜¯NPRO")
                    filtered_words.append(f"{word}(ä»£è¯-{main_lemma})")
                    continue
                
                # ğŸ”¥ å®Œå…¨å½¢å®¹è¯æ£€æµ‹
                if target_info['tag'] == 'ADJF':
                    if main_pos == 'PRTF':
                        if is_novye:
                            log(f"[DEBUG] âœ“ 'Ğ½Ğ¾Ğ²Ñ‹Ğµ'æ˜¯å½¢åŠ¨è¯")
                        found_adjectives.append(word)
                        continue
                    
                    # ğŸ”¥ å…³é”®ä¿®å¤ï¼šç›´æ¥æ¥å—ADJF
                    if main_pos == 'ADJF':
                        if is_novye:
                            log(f"[DEBUG] âœ“âœ“âœ“ 'Ğ½Ğ¾Ğ²Ñ‹Ğµ'è¢«æ¥å—ä¸ºå®Œå…¨å½¢å®¹è¯")
                        found_adjectives.append(word)
                        continue
                    
                    # å¦‚æœä¸æ˜¯ADJFæˆ–PRTF
                    if is_novye:
                        log(f"[DEBUG] âŒ 'Ğ½Ğ¾Ğ²Ñ‹Ğµ'è¯æ€§ä¸åŒ¹é…: {main_pos}")
                    continue
                
                # çŸ­å°¾å½¢å®¹è¯æ£€æµ‹é€»è¾‘
                if target_info['tag'] == 'ADJS':
                    log(f"[DEBUG] æ£€æµ‹çŸ­å°¾å½¢å®¹è¯: {word}, ADJS={has_adjs}, ADVB={has_adverb}, PRED={has_pred}")
                    
                    if has_pred and not has_adjs:
                        filtered_words.append(f"{word}(PRED-çŠ¶æ€è¯)")
                        continue
                    
                    if has_noun and not has_adjs and not has_adverb:
                        filtered_words.append(f"{word}(åè¯-æ— ADJS)")
                        continue
                    
                    if has_adjs and has_adverb:
                        if _adj_is_modifying_adjective(text, start_pos, morph, debug):
                            filtered_words.append(f"{word}(å‰¯è¯-ä¿®é¥°å½¢å®¹è¯)")
                            continue
                        
                        if _adj_check_action_verb_before(text[:start_pos], morph, debug):
                            filtered_words.append(f"{word}(å‰¯è¯-ä¿®é¥°åŠ¨ä½œåŠ¨è¯)")
                            continue
                        
                        if _adj_check_descriptive_verb_before(text[:start_pos], morph, debug):
                            found_adjectives.append(word)
                            continue
                        
                        if _adj_check_copula_verb_before(text[:start_pos], debug):
                            found_adjectives.append(word)
                            continue
                        
                        found_adjectives.append(word)
                        continue
                    
                    elif has_adverb and not has_adjs and word_lower in CONTEXT_DEPENDENT_WORDS:
                        if _adj_is_modifying_adjective(text, start_pos, morph, debug):
                            filtered_words.append(f"{word}(å‰¯è¯-ä¿®é¥°å½¢å®¹è¯)")
                            continue
                        
                        if _adj_check_copula_verb_before(text[:start_pos], debug):
                            found_adjectives.append(word)
                            continue
                        
                        if _adj_check_action_verb_before(text[:start_pos], morph, debug):
                            filtered_words.append(f"{word}(å‰¯è¯-ä¿®é¥°åŠ¨ä½œåŠ¨è¯)")
                            continue
                        
                        found_adjectives.append(word)
                        continue
                    
                    elif has_adverb and not has_adjs:
                        filtered_words.append(f"{word}(çº¯å‰¯è¯)")
                        continue
                    
                    elif has_adjs:
                        found_adjectives.append(word)
                        continue
                    
                    continue

            except Exception as e:
                log(f"[DEBUG] è§£æé”™è¯¯ '{word}': {e}")
                if debug:
                    import traceback
                    traceback.print_exc()
                continue

    actual_count = len(found_adjectives)
    adj_name = target_info['name']
    
    if found_adjectives:
        found_lines = []
        for i in range(0, len(found_adjectives), 10):
            found_lines.append(", ".join(found_adjectives[i:i+10]))
        found_str = "\n       ".join(found_lines)
    else:
        found_str = "æ— "
    
    if actual_count == expected_count:
        status = "âœ…"
        message = f"{adj_name}æ•°é‡æ­£ç¡®"
    else:
        status = "âŒ"
        message = f"{adj_name}æ•°é‡ä¸ç¬¦"

    explanation = (
        f"{status} {message}\n"
        f"   æœŸæœ›: {expected_count}ä¸ª\n"
        f"   å®é™…: {actual_count}ä¸ª\n"
        f"   æ‰¾åˆ°: {found_str}"
    )
    
    if debug and filtered_words:
        log(f"\n[DEBUG] è¢«è¿‡æ»¤çš„è¯ï¼ˆå‰30ä¸ªï¼‰:")
        for i, word in enumerate(filtered_words[:30], 1):
            log(f"  {i}. {word}")
        if len(filtered_words) > 30:
            log(f"[DEBUG] ... å…±è¿‡æ»¤ {len(filtered_words)} ä¸ªè¯")
    
    return 1 if actual_count == expected_count else 0, explanation


def _adj_is_modifying_adjective(text, word_start_pos, morph, debug=False):
    """æ£€æŸ¥è¯åæ˜¯å¦ç´§è·Ÿå½¢å®¹è¯ï¼ˆå‰¯è¯ä¿®é¥°å½¢å®¹è¯çš„ç»“æ„ï¼‰"""
    log = create_logger(debug)
    
    text_after = text[word_start_pos:]
    current_match = re.match(r'\b[Ğ°-ÑÑ‘Ğ-Ğ¯Ğ]+\b', text_after)
    if not current_match:
        return False
    
    current_word = current_match.group(0).lower()
    text_after_current = text_after[current_match.end():]
    next_match = re.search(r'\b([Ğ°-ÑÑ‘Ğ-Ğ¯Ğ]+)\b', text_after_current)
    
    if not next_match:
        return False
    
    next_word = next_match.group(1).lower()
    
    try:
        current_parses = morph.parse(current_word)
        current_has_adjf = any(p.tag.POS == 'ADJF' for p in current_parses)
        current_has_advb = any(p.tag.POS == 'ADVB' for p in current_parses)
        
        next_parses = morph.parse(next_word)
        next_has_adjf = any(p.tag.POS == 'ADJF' for p in next_parses)
        
        log(f"[DEBUG] æ£€æŸ¥ç»“æ„: {current_word}(ADJF={current_has_adjf},ADVB={current_has_advb}) -> {next_word}(ADJF={next_has_adjf})")
        
        if current_has_adjf and next_has_adjf:
            text_after_next = text_after_current[next_match.end():]
            third_match = re.search(r'\b([Ğ°-ÑÑ‘Ğ-Ğ¯Ğ]+)\b', text_after_next)
            
            if third_match:
                third_word = third_match.group(1).lower()
                third_parses = morph.parse(third_word)
                third_has_noun = any(p.tag.POS == 'NOUN' for p in third_parses)
                
                if third_has_noun:
                    log(f"[DEBUG] âœ“ å¹¶åˆ—å½¢å®¹è¯: {current_word} {next_word} {third_word}")
                    return False
            
            if current_has_advb:
                log(f"[DEBUG] å½“å‰è¯æœ‰ADVBï¼Œåˆ¤å®šä¸ºå‰¯è¯")
                return True
            else:
                log(f"[DEBUG] å½“å‰è¯æ— ADVBï¼Œåˆ¤å®šä¸ºå¹¶åˆ—å½¢å®¹è¯")
                return False
        
        elif current_has_advb and not current_has_adjf and next_has_adjf:
            log(f"[DEBUG] å‰¯è¯ä¿®é¥°å½¢å®¹è¯")
            return True
        
        return False
        
    except Exception as e:
        log(f"[DEBUG] æ£€æŸ¥ä¿®é¥°å…³ç³»å‡ºé”™: {e}")
        return False


def _adj_check_descriptive_verb_before(text_before, morph, debug=False):
    """æ£€æŸ¥è¯å‰æ˜¯å¦æœ‰æè¿°æ€§åŠ¨è¯"""
    log = create_logger(debug)
    words_before = re.findall(r'\b[Ğ°-ÑÑ‘]+\b', text_before.lower())
    if not words_before:
        return False
    
    descriptive_verbs = {
        'Ğ²Ñ‹Ğ³Ğ»ÑĞ´ĞµÑ‚ÑŒ', 'Ğ²Ñ‹Ğ³Ğ»ÑĞ´Ğ¸Ñ‚', 'Ğ²Ñ‹Ğ³Ğ»ÑĞ´ĞµĞ»', 'Ğ²Ñ‹Ğ³Ğ»ÑĞ´ĞµĞ»Ğ°', 'Ğ²Ñ‹Ğ³Ğ»ÑĞ´ĞµĞ»Ğ¾', 'Ğ²Ñ‹Ğ³Ğ»ÑĞ´ĞµĞ»Ğ¸',
        'ĞºĞ°Ğ·Ğ°Ñ‚ÑŒÑÑ', 'ĞºĞ°Ğ¶ĞµÑ‚ÑÑ', 'ĞºĞ°Ğ·Ğ°Ğ»ÑÑ', 'ĞºĞ°Ğ·Ğ°Ğ»Ğ°ÑÑŒ', 'ĞºĞ°Ğ·Ğ°Ğ»Ğ¾ÑÑŒ', 'ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ÑÑŒ',
        'Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ÑŒÑÑ', 'Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»ÑÑ', 'Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ°ÑÑŒ', 'Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾ÑÑŒ', 'Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ÑÑŒ',
        'Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ÑŒÑÑ', 'Ğ¾ĞºĞ°Ğ·Ğ°Ğ»ÑÑ', 'Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ°ÑÑŒ', 'Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾ÑÑŒ', 'Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ÑÑŒ',
        'ÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ¸Ñ‚ÑŒÑÑ', 'ÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ¸Ñ‚ÑÑ', 'ÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ¸Ğ»ÑÑ', 'ÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ¸Ğ»Ğ°ÑÑŒ', 'ÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ¸Ğ»Ğ¾ÑÑŒ',
        'ÑÑ‡Ğ¸Ñ‚Ğ°Ñ‚ÑŒÑÑ', 'ÑÑ‡Ğ¸Ñ‚Ğ°ĞµÑ‚ÑÑ', 'ÑÑ‡Ğ¸Ñ‚Ğ°Ğ»ÑÑ', 'ÑÑ‡Ğ¸Ñ‚Ğ°Ğ»Ğ°ÑÑŒ', 'ÑÑ‡Ğ¸Ñ‚Ğ°Ğ»Ğ¾ÑÑŒ',
    }
    
    for i in range(min(3, len(words_before))):
        word = words_before[-(i+1)]
        if word in descriptive_verbs:
            return True
        
        try:
            parses = morph.parse(word)
            for parse in parses:
                if parse.normal_form in ['Ğ²Ñ‹Ğ³Ğ»ÑĞ´ĞµÑ‚ÑŒ', 'ĞºĞ°Ğ·Ğ°Ñ‚ÑŒÑÑ', 'Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ÑŒÑÑ', 'Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ÑŒÑÑ']:
                    return True
        except:
            pass
    return False


def _adj_check_copula_verb_before(text_before, debug=False):
    """æ£€æŸ¥è¯å‰æ˜¯å¦æœ‰ç³»åŠ¨è¯"""
    log = create_logger(debug)
    words_before = re.findall(r'\b[Ğ°-ÑÑ‘]+\b', text_before.lower())
    if not words_before:
        return False
    
    copula_verbs = {
        'Ğ±Ñ‹Ğ»', 'Ğ±Ñ‹Ğ»Ğ°', 'Ğ±Ñ‹Ğ»Ğ¾', 'Ğ±Ñ‹Ğ»Ğ¸', 'ĞµÑÑ‚ÑŒ', 'ÑÑƒÑ‚ÑŒ',
        'Ğ±ÑƒĞ´ĞµÑ‚', 'Ğ±ÑƒĞ´ÑƒÑ‚', 'Ğ±ÑƒĞ´ĞµĞ¼', 'Ğ±ÑƒĞ´ĞµÑˆÑŒ', 'Ğ±ÑƒĞ´ĞµÑ‚Ğµ',
        'Ğ±Ñ‹Ñ‚ÑŒ', 'ÑÑ‚Ğ°Ğ»', 'ÑÑ‚Ğ°Ğ»Ğ°', 'ÑÑ‚Ğ°Ğ»Ğ¾', 'ÑÑ‚Ğ°Ğ»Ğ¸',
        'ÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ¸Ñ‚ÑŒÑÑ', 'ĞºĞ°Ğ·Ğ°Ñ‚ÑŒÑÑ', 'Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ÑŒÑÑ'
    }
    
    for i in range(min(5, len(words_before))):
        if words_before[-(i+1)] in copula_verbs:
            return True
    return False


def _adj_check_action_verb_before(text_before, morph, debug=False):
    """æ£€æŸ¥è¯å‰æ˜¯å¦æœ‰åŠ¨ä½œåŠ¨è¯"""
    log = create_logger(debug)
    words_before = re.findall(r'\b[Ğ°-ÑÑ‘]+\b', text_before.lower())
    if not words_before:
        return False
    
    copula_verbs = {'Ğ±Ñ‹Ğ»', 'Ğ±Ñ‹Ğ»Ğ°', 'Ğ±Ñ‹Ğ»Ğ¾', 'Ğ±Ñ‹Ğ»Ğ¸', 'ĞµÑÑ‚ÑŒ', 'ÑÑƒÑ‚ÑŒ',
                    'Ğ±ÑƒĞ´ĞµÑ‚', 'Ğ±ÑƒĞ´ÑƒÑ‚', 'Ğ±ÑƒĞ´ĞµĞ¼', 'Ğ±ÑƒĞ´ĞµÑˆÑŒ', 'Ğ±ÑƒĞ´ĞµÑ‚Ğµ',
                    'Ğ±Ñ‹Ñ‚ÑŒ', 'ÑÑ‚Ğ°Ğ»', 'ÑÑ‚Ğ°Ğ»Ğ°', 'ÑÑ‚Ğ°Ğ»Ğ¾', 'ÑÑ‚Ğ°Ğ»Ğ¸'}
    
    descriptive_verbs = {'Ğ²Ñ‹Ğ³Ğ»ÑĞ´ĞµÑ‚ÑŒ', 'ĞºĞ°Ğ·Ğ°Ñ‚ÑŒÑÑ', 'Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ÑŒÑÑ', 'Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ÑŒÑÑ', 'ÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ¸Ñ‚ÑŒÑÑ', 'ÑÑ‡Ğ¸Ñ‚Ğ°Ñ‚ÑŒÑÑ'}
    
    for i in range(min(3, len(words_before))):
        word = words_before[-(i+1)]
        if word in copula_verbs:
            continue
        
        try:
            parses = morph.parse(word)
            for parse in parses:
                if parse.tag.POS == 'VERB' and parse.normal_form not in descriptive_verbs:
                    return True
        except:
            pass
    return False




# ==================== è§„åˆ™ 15: æ®µè½æ•°é‡æ£€æµ‹ ====================
def check_paragraph_count(content_list, expected_count, debug=False):
    """
    æ£€æŸ¥æ®µè½æ•°é‡
    
    Args:
        content_list: æ–‡æœ¬å†…å®¹åˆ—è¡¨
        expected_count: æœŸæœ›çš„æ®µè½æ•°é‡
        debug: æ˜¯å¦è¾“å‡ºè°ƒè¯•ä¿¡æ¯
    
    Returns:
        tuple: (1/0, è¯´æ˜ä¿¡æ¯)
    """
    log = create_logger(debug)
    
    if not content_list or content_list == "INVALID":
        return 0, "âŒ è¾“å…¥å†…å®¹ä¸ºç©º"
    
    try:
        expected_count = int(expected_count)
    except (ValueError, TypeError):
        return 0, f"âŒ æœŸæœ›æ•°é‡ '{expected_count}' ä¸æ˜¯æœ‰æ•ˆçš„æ•´æ•°"
    
    try:
        if isinstance(content_list, list):
            actual_count = len([item for item in content_list if item and str(item).strip()])
        else:
            text = str(content_list)
            paragraphs = [p.strip() for p in text.split('\n\n') if p.strip()]
            if len(paragraphs) <= 1:
                paragraphs = [p.strip() for p in text.split('\n') if p.strip()]
            actual_count = len(paragraphs)
        
        log(f"[DEBUG] æ®µè½æ•°é‡: {actual_count}")
        
        if actual_count == expected_count:
            return 1, f"âœ… æ®µè½æ•°é‡æ­£ç¡®ï¼š{actual_count}ä¸ª"
        else:
            return 0, f"âŒ æ®µè½æ•°é‡ä¸ç¬¦ï¼šæœŸæœ›{expected_count}ä¸ªï¼Œå®é™…{actual_count}ä¸ª"
    
    except Exception as e:
        log(f"[ERROR] å¼‚å¸¸: {e}")
        if debug:
            import traceback
            traceback.print_exc()
        return 0, f"âŒ å‡½æ•°æ‰§è¡Œå¼‚å¸¸: {e}"


# ==================== è§„åˆ™ 16: æ–‡æœ¬æ€»é•¿åº¦æ£€æµ‹ ====================
def russian_total_length(content_list, min_length, max_length, debug=False):
    """
    æ£€æŸ¥ä¿„è¯­æ–‡æœ¬æ€»è¯æ•°
    
    Args:
        content_list: æ–‡æœ¬å†…å®¹åˆ—è¡¨
        min_length: æœ€å°è¯æ•°
        max_length: æœ€å¤§è¯æ•°
        debug: æ˜¯å¦è¾“å‡ºè°ƒè¯•ä¿¡æ¯
    
    Returns:
        tuple: (1/0, è¯´æ˜ä¿¡æ¯)
    """
    log = create_logger(debug)
    
    if not content_list or content_list == "INVALID":
        return 0, "âŒ è¾“å…¥å†…å®¹ä¸ºç©º"
    
    try:
        min_length = int(min_length)
        max_length = int(max_length)
    except (ValueError, TypeError):
        return 0, f"âŒ é•¿åº¦å‚æ•°å¿…é¡»æ˜¯æ•´æ•°"
    
    try:
        if isinstance(content_list, list):
            text = ' '.join(map(str, content_list))
        else:
            text = str(content_list)
        
        words = re.findall(r'\b[Ğ°-ÑÑ‘Ğ-Ğ¯Ğ]+\b', text)
        actual_count = len(words)
        
        log(f"[DEBUG] è¯æ•°: {actual_count}")
        
        if min_length <= actual_count <= max_length:
            if max_length >= 9999:
                return 1, f"âœ… è¯æ•°ç¬¦åˆè¦æ±‚ï¼š{actual_count}è¯ï¼ˆâ‰¥{min_length}è¯ï¼‰"
            else:
                return 1, f"âœ… è¯æ•°ç¬¦åˆè¦æ±‚ï¼š{actual_count}è¯"
        else:
            if max_length >= 9999:
                return 0, f"âŒ è¯æ•°ä¸ç¬¦ï¼š{actual_count}è¯ï¼Œè¦æ±‚â‰¥{min_length}è¯"
            else:
                return 0, f"âŒ è¯æ•°ä¸ç¬¦ï¼š{actual_count}è¯ï¼Œè¦æ±‚{min_length}-{max_length}è¯"
    
    except Exception as e:
        log(f"[ERROR] å¼‚å¸¸: {e}")
        if debug:
            import traceback
            traceback.print_exc()
        return 0, f"âŒ å‡½æ•°æ‰§è¡Œå¼‚å¸¸: {e}"
# ==================== è§„åˆ™ 17:ä¿„è‹±å•è¯æ¯”ä¾‹  ====================
def russian_english_ratio(content_list, ratio_a, ratio_b, debug=False):
    """æ£€æŸ¥ä¿„è‹±æ¯”ä¾‹"""
    log = create_logger(debug)
    
    if not content_list or content_list == "INVALID":
        return 0, "âŒ è¾“å…¥å†…å®¹ä¸ºç©º"
    
    try:
        ratio_a = float(ratio_a)
        ratio_b = float(ratio_b)
    except (ValueError, TypeError):
        return 0, f"âŒ æ¯”ä¾‹å‚æ•°å¿…é¡»æ˜¯æ•°å­—"
    
    if ratio_a <= 0 or ratio_b <= 0:
        return 0, f"âŒ æ¯”ä¾‹å‚æ•°å¿…é¡»å¤§äº0"
    
    try:
        if isinstance(content_list, list):
            text = ' '.join(map(str, content_list))
        else:
            text = str(content_list)
        
        # æ¸…ç†æ–‡æœ¬ï¼šç§»é™¤å¤šä½™ç©ºç™½
        text = re.sub(r'\s+', ' ', text).strip()
        
        # æ”¹è¿›ï¼šä½¿ç”¨æ›´ç²¾ç¡®çš„åˆ†è¯æ–¹å¼
        # 1. å…ˆå°†æ ‡ç‚¹ç¬¦å·æ›¿æ¢ä¸ºç©ºæ ¼ï¼ˆä¿ç•™è¿å­—ç¬¦ï¼‰
        text_cleaned = re.sub(r'[^\w\s-]', ' ', text)
        # 2. åˆ†è¯
        words = text_cleaned.split()
        
        # è¿‡æ»¤å‡ºä¿„è¯­è¯ï¼ˆçº¯ä¿„è¯­å­—ç¬¦ï¼Œå¯å«è¿å­—ç¬¦ï¼‰
        russian_words = []
        for w in words:
            w = w.strip('-')  # å»é™¤é¦–å°¾è¿å­—ç¬¦
            if w and re.fullmatch(r'[Ğ°-ÑÑ‘Ğ-Ğ¯Ğ]+(?:-[Ğ°-ÑÑ‘Ğ-Ğ¯Ğ]+)*', w):
                russian_words.append(w)
        russian_count = len(russian_words)
        
        # æ”¹è¿›ï¼šè¿‡æ»¤å‡ºè‹±è¯­è¯ï¼ˆçº¯è‹±è¯­å­—ç¬¦ï¼Œæ›´ä¸¥æ ¼ï¼‰
        english_words = []
        for w in words:
            w = w.strip('-')  # å»é™¤é¦–å°¾è¿å­—ç¬¦
            # åªæ¥å—çº¯è‹±è¯­å­—æ¯ï¼Œä¸”é•¿åº¦>=2ï¼ˆé¿å…å•å­—æ¯å¹²æ‰°ï¼‰
            if w and len(w) >= 2 and re.fullmatch(r'[a-zA-Z]+', w):
                english_words.append(w)
            # ç‰¹æ®Šå¤„ç†ï¼šå•å­—æ¯ä½†æ˜¯å¸¸è§è¯ï¼ˆI, aï¼‰
            elif w and len(w) == 1 and w.lower() in ['i', 'a']:
                english_words.append(w)
        english_count = len(english_words)
        
        log(f"[DEBUG] ä¿„è¯­è¯æ•°: {russian_count}")
        log(f"[DEBUG] è‹±è¯­è¯æ•°: {english_count}")
        log(f"[DEBUG] ä¿„è¯­è¯ç¤ºä¾‹: {russian_words[:10]}")
        log(f"[DEBUG] è‹±è¯­è¯ç¤ºä¾‹: {english_words[:10]}")
        log(f"[DEBUG] è¦æ±‚æ¯”ä¾‹: {ratio_a}:{ratio_b}")
        
        if russian_count == 0 and english_count == 0:
            return 0, f"âŒ æœªæ£€æµ‹åˆ°ä¿„è¯­å’Œè‹±è¯­å•è¯"
        
        if english_count == 0 and ratio_b > 0:
            return 0, f"âŒ æœªæ£€æµ‹åˆ°è‹±è¯­å•è¯ï¼Œæ— æ³•æ»¡è¶³ {ratio_a}:{ratio_b} æ¯”ä¾‹"
        
        if russian_count == 0 and ratio_a > 0:
            return 0, f"âŒ æœªæ£€æµ‹åˆ°ä¿„è¯­å•è¯ï¼Œæ— æ³•æ»¡è¶³ {ratio_a}:{ratio_b} æ¯”ä¾‹"
        
        expected_ratio = ratio_a / ratio_b
        actual_ratio = russian_count / english_count if english_count > 0 else float('inf')
        
        log(f"[DEBUG] æœŸæœ›æ¯”ä¾‹å€¼: {expected_ratio:.2f}")
        log(f"[DEBUG] å®é™…æ¯”ä¾‹å€¼: {actual_ratio:.2f}")
        
        # å¢å¤§å®¹å·®åˆ° Â±40%ï¼Œé€‚åº”æ··åˆè¯­è¨€çš„ä¸ç¡®å®šæ€§
        tolerance = 0.4
        lower_bound = expected_ratio * (1 - tolerance)
        upper_bound = expected_ratio * (1 + tolerance)
        
        log(f"[DEBUG] å…è®¸èŒƒå›´: {lower_bound:.2f} - {upper_bound:.2f}")
        
        # æ ¼å¼åŒ–æ¯”ä¾‹æ˜¾ç¤ºï¼ˆä¿ç•™1ä½å°æ•°ï¼‰
        actual_ratio_str = f"{actual_ratio:.1f}:1"
        expected_ratio_str = f"{expected_ratio:.1f}:1"
        
        if lower_bound <= actual_ratio <= upper_bound:
            return 1, f"âœ… ä¿„è‹±æ¯”ä¾‹ç¬¦åˆè¦æ±‚ï¼šå½“å‰æ¯”ä¾‹ {actual_ratio_str}ï¼ŒæœŸæœ›æ¯”ä¾‹ {expected_ratio_str}"
        else:
            return 0, f"âŒ ä¿„è‹±æ¯”ä¾‹ä¸ç¬¦ï¼šå½“å‰æ¯”ä¾‹ {actual_ratio_str}ï¼ŒæœŸæœ›æ¯”ä¾‹ {expected_ratio_str}"
    
    except Exception as e:
        log(f"[ERROR] å¼‚å¸¸: {e}")
        if debug:
            import traceback
            traceback.print_exc()
        return 0, f"âŒ å‡½æ•°æ‰§è¡Œå¼‚å¸¸: {e}"


if __name__ == "__main__":
    word = "Ğ¾Ñ€Ğ³Ğ°Ğ½"
    content_list = [
        "12 Ğ¸ÑĞ½Ñ\n\nĞ¡ĞµĞ³Ğ¾Ğ´Ğ½Ñ Ñ Ñ€ĞµÑˆĞ¸Ğ» Ğ·Ğ°Ğ¿Ğ¸ÑĞ°Ñ‚ÑŒ ÑĞ²Ğ¾Ğ¸ Ğ²Ğ¿ĞµÑ‡Ğ°Ñ‚Ğ»ĞµĞ½Ğ¸Ñ Ğ¾Ñ‚ Ğ¾Ñ‚Ğ¿ÑƒÑĞºĞ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾ĞºĞ°Ğ·Ğ°Ğ»ÑÑ ÑƒĞ´Ğ¸Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ½Ğ°ÑÑ‹Ñ‰ĞµĞ½Ğ½Ñ‹Ğ¼. Ğ’ ÑÑ‚Ğ¾Ğ¼ Ğ³Ğ¾Ğ´Ñƒ Ñ Ğ¾Ñ‚Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑÑ Ğ² Ğ¿ÑƒÑ‚ĞµÑˆĞµÑÑ‚Ğ²Ğ¸Ğµ Ğ¿Ğ¾ Ğ•Ğ²Ñ€Ğ¾Ğ¿Ğµ, Ğ¸ ĞºĞ°Ğ¶Ğ´Ñ‹Ğ¹ Ğ´ĞµĞ½ÑŒ Ğ¿Ñ€Ğ¸Ğ½Ğ¾ÑĞ¸Ğ» Ñ‡Ñ‚Ğ¾-Ñ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ¾Ğµ. ĞÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ·Ğ°Ğ¿Ğ¾Ğ¼Ğ½Ğ¸Ğ»Ğ°ÑÑŒ ÑĞºÑĞºÑƒÑ€ÑĞ¸Ñ Ğ² Ğ’ĞµĞ½Ñƒ, Ğ³Ğ´Ğµ Ñ Ğ¿Ğ¾ÑĞµÑ‚Ğ¸Ğ» Ğ·Ğ½Ğ°Ğ¼ĞµĞ½Ğ¸Ñ‚ÑƒÑ Ñ„Ğ¸Ğ»Ğ°Ñ€Ğ¼Ğ¾Ğ½Ğ¸Ñ. ĞÑ€Ğ³Ğ°Ğ½, ÑƒÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ² Ğ³Ğ»Ğ°Ğ²Ğ½Ğ¾Ğ¼ Ğ·Ğ°Ğ»Ğµ, Ğ¿Ğ¾Ñ€Ğ°Ğ·Ğ¸Ğ» Ğ¼ĞµĞ½Ñ ÑĞ²Ğ¾Ğ¸Ğ¼ Ğ²ĞµĞ»Ğ¸Ñ‡Ğ¸ĞµĞ¼ Ğ¸ Ñ‡Ğ¸ÑÑ‚Ğ¾Ñ‚Ğ¾Ğ¹ Ğ·Ğ²ÑƒÑ‡Ğ°Ğ½Ğ¸Ñ. Ğ¯ Ğ´Ğ°Ğ¶Ğµ Ğ·Ğ°Ğ´ÑƒĞ¼Ğ°Ğ»ÑÑ, ĞºĞ°Ğº Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ñ‚Ñ€ÑƒĞ´Ğ° Ğ²Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¾ Ğ² ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ°ĞºĞ¾Ğ³Ğ¾ Ğ¼ÑƒĞ·Ñ‹ĞºĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°, Ğ²ĞµĞ´ÑŒ Ğ¾Ñ€Ğ³Ğ°Ğ½ â€” ÑÑ‚Ğ¾ Ğ½Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾ Ñ‡Ğ°ÑÑ‚ÑŒ Ğ¾Ñ€ĞºĞµÑÑ‚Ñ€Ğ°, Ğ° ĞµĞ³Ğ¾ Ğ´ÑƒÑˆĞ°.\n\nĞŸĞ¾ÑĞ»Ğµ Ğ’ĞµĞ½Ñ‹ Ñ Ğ¿Ğ¾ĞµÑ…Ğ°Ğ» Ğ² ĞŸÑ€Ğ°Ğ³Ñƒ, Ğ³Ğ´Ğµ Ğ¿Ğ¾Ğ·Ğ½Ğ°ĞºĞ¾Ğ¼Ğ¸Ğ»ÑÑ Ñ Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğ¹ ĞºÑƒÑ…Ğ½ĞµĞ¹ Ğ¸ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ¾Ğ¹. Ğ’ Ğ¾Ğ´Ğ½Ğ¾Ğ¼ Ğ¸Ğ· Ğ¼ÑƒĞ·ĞµĞµĞ² Ğ¼Ğ½Ğµ Ñ€Ğ°ÑÑĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ¾ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ…Ñ€Ğ°Ğ½Ñ‹ ĞºÑƒĞ»ÑŒÑ‚ÑƒÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ½Ğ°ÑĞ»ĞµĞ´Ğ¸Ñ, Ğ¸ Ñ Ğ·Ğ°Ğ´ÑƒĞ¼Ğ°Ğ»ÑÑ Ğ¾ Ñ‚Ğ¾Ğ¼, ĞºĞ°Ğº ĞºĞ°Ğ¶Ğ´Ñ‹Ğ¹ Ğ¾Ñ€Ğ³Ğ°Ğ½ Ğ²Ğ»Ğ°ÑÑ‚Ğ¸ Ğ² ÑÑ‚Ğ¸Ñ… ÑÑ‚Ñ€Ğ°Ğ½Ğ°Ñ… Ğ·Ğ°Ğ±Ğ¾Ñ‚Ğ¸Ñ‚ÑÑ Ğ¾ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ğ¸. Ğ­Ñ‚Ğ¾ ÑĞ»Ğ¾Ğ²Ğ¾ Ğ·Ğ´ĞµÑÑŒ Ğ¿Ñ€Ğ¸Ğ¾Ğ±Ñ€ĞµĞ»Ğ¾ ÑĞ¾Ğ²ÑĞµĞ¼ Ğ´Ñ€ÑƒĞ³Ğ¾Ğµ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğµ: Ğ¾Ñ€Ğ³Ğ°Ğ½ ĞºĞ°Ğº ÑƒÑ‡Ñ€ĞµĞ¶Ğ´ĞµĞ½Ğ¸Ğµ, Ğ¾Ñ‚Ğ²ĞµÑ‡Ğ°ÑÑ‰ĞµĞµ Ğ·Ğ° Ğ¿Ğ¾Ñ€ÑĞ´Ğ¾Ğº Ğ¸ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ğµ Ğ¾Ğ±Ñ‰ĞµÑÑ‚Ğ²Ğ°. ĞŸÑƒÑ‚ĞµÑˆĞµÑÑ‚Ğ²Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ğ°Ñ€Ğ¸Ğ»Ğ¾ Ğ¼Ğ½Ğµ Ğ½Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ ÑÑ€ĞºĞ¸Ğµ ÑĞ¼Ğ¾Ñ†Ğ¸Ğ¸, Ğ½Ğ¾ Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ¾ Ğ¼Ğ¸Ñ€Ğµ, ĞµĞ³Ğ¾ ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğµ Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğ¹ Ğ¿Ñ€Ğ¸Ğ²Ñ‹Ñ‡Ğ½Ñ‹Ñ… ÑĞ»Ğ¾Ğ²."
    ]
    rus_stress_homonym_usage(content_list, word, 2)