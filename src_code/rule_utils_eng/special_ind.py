"""
å°å°¼è¯­ç‰¹æ®Šè§„åˆ™æ£€æµ‹æ¨¡å—
æ¯ä¸ªå‡½æ•°åŒ…å«ç‹¬ç«‹çš„è¯åº“å’Œè¾…åŠ©å‡½æ•°ï¼Œæ–¹ä¾¿å•ç‹¬ä¿®æ”¹
"""
try:
    from spellchecker import SpellChecker
    spellchecker_AVAILABLE = True
except ImportError:
    spellchecker_AVAILABLE = False
    print("spellcheckeråº“æœªå®‰è£…ï¼Œæ­£åœ¨è‡ªåŠ¨å®‰è£…...")
    try:
        import subprocess
        import sys
        subprocess.check_call([sys.executable, "-m", "pip", "install", "pyspellchecker", "-i", "https://pypi.tuna.tsinghua.edu.cn/simple"])
        print("spellcheckeråº“å®‰è£…æˆåŠŸï¼Œæ­£åœ¨å¯¼å…¥...")
        from spellchecker import SpellChecker
        spellchecker_AVAILABLE = True
        print("âœ… spellcheckeråº“å·²æˆåŠŸå¯¼å…¥")
    except Exception as e:
        print(f"âŒ è‡ªåŠ¨å®‰è£…å¤±è´¥: {e}")
        print("è¯·æ‰‹åŠ¨è¿è¡Œ: pip install pyspellchecker")
        spellchecker_AVAILABLE = False

        
import re


def create_logger(debug):
    """åˆ›å»ºæ—¥å¿—å‡½æ•°"""
    def log(msg):
        if debug:
            print(msg)
    return log

import re
from spellchecker import SpellChecker

# ==================== å°å°¼è¯­ä¸“é¡¹è§„åˆ™é›†åˆ ====================
# ==================== é«˜å‡†ç¡®ç‡å¤–æ¥è¯æ£€æµ‹ï¼ˆè‹±è¯­+æ³•è¯­+è·å…°è¯­ï¼‰- æ”¯æŒé¦–å­—æ¯ç­›é€‰ ====================

import re
from typing import Tuple, Dict, List, Set, Optional
from collections import Counter

def check_indonesian_loanwords(content, *args, debug: bool = False, **kwargs) -> Tuple[int, str]:
    """
    é«˜å‡†ç¡®ç‡å¤–æ¥è¯å€Ÿè¯æ£€æµ‹ï¼ˆä»…è‹±è¯­ã€æ³•è¯­ã€è·å…°è¯­æ¥æºï¼‰
    
    â­ æ”¯æŒä¸‰ç§è°ƒç”¨æ–¹å¼ï¼š
    1. check_indonesian_loanwords(content, required_count)
    2. check_indonesian_loanwords(content, required_count, start_letter)
    3. check_indonesian_loanwords(content, "required_count,start_letter")  # å­—ç¬¦ä¸²æ ¼å¼
    
    Args:
        content: æ–‡ç« å†…å®¹
        *args: ä½ç½®å‚æ•°
            - args[0]: required_count (int) æˆ– "count,letter" (str)
            - args[1]: start_letter (str, optional) - å½“args[0]æ˜¯æ•´æ•°æ—¶ä½¿ç”¨
        debug: è°ƒè¯•æ¨¡å¼
    
    Returns:
        tuple: (1/0, è¯´æ˜ä¿¡æ¯)
    """
    
    # ============ è§£æå‚æ•°ï¼ˆå…¼å®¹å­—ç¬¦ä¸²æ ¼å¼ï¼‰============
    if len(args) == 0:
        return 0, "âŒ é”™è¯¯ï¼šç¼ºå°‘å¿…éœ€å‚æ•° required_count"
    
    first_arg = str(args[0])
    
    # â­ æƒ…å†µ1ï¼šå­—ç¬¦ä¸²æ ¼å¼ "3,t"
    if ',' in first_arg:
        parts = [p.strip() for p in first_arg.split(',')]
        try:
            required_count = int(parts[0])
            start_letter = parts[1].lower() if len(parts) > 1 and parts[1] else None
        except (ValueError, IndexError) as e:
            return 0, f"âŒ é”™è¯¯ï¼šå‚æ•°æ ¼å¼é”™è¯¯ '{first_arg}'ï¼Œåº”ä¸º 'æ•°å­—,å­—æ¯' æ ¼å¼"
    
    # â­ æƒ…å†µ2ï¼šåˆ†ç¦»å‚æ•° (3, 't')
    else:
        try:
            required_count = int(args[0])
        except (ValueError, TypeError):
            return 0, f"âŒ é”™è¯¯ï¼šå€Ÿè¯æ•°é‡å¿…é¡»æ˜¯æ•´æ•°ï¼Œå½“å‰å€¼: '{args[0]}'"
        
        # ç¬¬äºŒä¸ªå‚æ•°ï¼šé¦–å­—æ¯ï¼ˆå¯é€‰ï¼‰
        start_letter = None
        if len(args) > 1:
            start_letter = str(args[1]).lower().strip()
    
    # ============ éªŒè¯é¦–å­—æ¯ ============
    if start_letter and (len(start_letter) != 1 or not start_letter.isalpha()):
        return 0, f"âŒ é”™è¯¯ï¼šé¦–å­—æ¯å‚æ•°æ— æ•ˆï¼ˆ'{start_letter}'ï¼‰ï¼Œåº”ä¸ºå•ä¸ªå­—æ¯"
    
    # ============ å¤„ç†è¾“å…¥ ============
    if isinstance(content, list):
        content = ' '.join(str(item) for item in content)
    content = str(content)
    
    if not content.strip():
        return 0, "âŒ é”™è¯¯ï¼šè¾“å…¥æ–‡æœ¬ä¸ºç©º"
    
    # ============ æ ¸å¿ƒå€Ÿè¯åº“ï¼ˆä»…è‹±è¯­+æ³•è¯­+è·å…°è¯­æ¥æºï¼‰============
    
    CORE_LOANWORDS = {
        # === è‹±è¯­æ¥æº ===
        'komputer': 'computer',
        'teknologi': 'technology',
        'aplikasi': 'application',
        'sistem': 'system',
        'elektronik': 'electronic',
        'otomatis': 'automatic',
        'mekanis': 'mechanical',
        
        # å•†ä¸š/ç»æµ
        'bisnis': 'business',
        'strategi': 'strategy',
        'ekonomi': 'economy',
        'investasi': 'investment',
        'diskon': 'discount',
        'promo': 'promotion',
        'desain': 'design',
        'produk': 'product',
        'standar': 'standard',
        'garansi': 'guarantee',
        'servis': 'service',
        'kualitas': 'quality',
        'manajemen': 'management',
        'pemasaran': 'marketing',
        'industri': 'industry',
        'komersial': 'commercial',
        'finansial': 'financial',
        'kredit': 'credit',
        'saldo': 'balance',
        'asuransi': 'insurance',
        'klaim': 'claim',
        'transaksi': 'transaction',
        
        # é¤é¥®ï¼ˆè‹±è¯­ï¼‰
        'jus': 'juice',
        'kuliner': 'culinary',
        'resep': 'recipe',
        'ingredien': 'ingredient',
        'krim': 'cream',
        'cokelat': 'chocolate',
        
        # æ–‡åŒ–/å†å²
        'simbol': 'symbol',
        'situs': 'site',
        'monumen': 'monument',  # âœ… ä¿ç•™
        'artefak': 'artifact',
        'arsitektur': 'architecture',
        'kolonial': 'colonial',
        'galeri': 'gallery',
        'koleksi': 'collection',
        'seremoni': 'ceremony',
        'tradisi': 'tradition',
        
        # æ•™è‚²
        'universitas': 'university',
        'fakultas': 'faculty',
        'kampus': 'campus',
        'presentasi': 'presentation',
        'diskusi': 'discussion',
        'kelas': 'class',
        'literatur': 'literature',
        'akademis': 'academic',
        'teori': 'theory',
        'praktis': 'practical',
        
        # äº¤é€š
        'taksi': 'taxi',
        'tiket': 'ticket',
        'parkir': 'parking',
        'transportasi': 'transportation',
        'bis': 'bus',
        
        # ä½å®¿
        'apartemen': 'apartment',
        'vila': 'villa',
        
        # åŒ»ç–—
        'klinik': 'clinic',
        'apotik': 'pharmacy',
        'terapi': 'therapy',
        'kalori': 'calorie',
        'konsultasi': 'consultation',
        'diagnosa': 'diagnosis',
        'medis': 'medical',
        
        # ç§‘å­¦
        'spesies': 'species',
        'botani': 'botany',
        'biologi': 'biology',
        'kimia': 'chemistry',
        'fisika': 'physics',
        'geografi': 'geography',
        'astronomi': 'astronomy',
        'geologi': 'geology',
        'ekologi': 'ecology',
        
        # å½¢å®¹è¯
        'internasional': 'international',
        'nasional': 'national',
        'lokal': 'local',
        'sentral': 'central',
        'spesial': 'special',
        'aktif': 'active',
        'pasif': 'passive',
        'positif': 'positive',
        'negatif': 'negative',
        'privat': 'private',
        'publik': 'public',
        'profesional': 'professional',
        'efektif': 'effective',
        'efisien': 'efficient',
        'populer': 'popular',
        'favorit': 'favorite',
        'eksklusif': 'exclusive',
        'kompleks': 'complex',
        'fleksibel': 'flexible',
        'stabil': 'stable',
        'dinamis': 'dynamic',
        'unik': 'unique',
        'klasik': 'classic',
        'eksotis': 'exotic',
        'tradisional': 'traditional',
        'spektakuler': 'spectacular',
        'romantis': 'romantic',
        'elegan': 'elegant',
        'kontemporer': 'contemporary',
        'antik': 'antique',
        'historis': 'historic',
        'artifisial': 'artificial',
        'maksimal': 'maximal',
        'minimal': 'minimal',
        'optimal': 'optimal',
        'ilegal': 'illegal',
        
        # åª’ä½“/å¨±ä¹
        'foto': 'photo',
        'kamera': 'camera',
        'televisi': 'television',
        'musik': 'music',
        'konser': 'concert',
        'komedi': 'comedy',
        'hobi': 'hobby',
        'joging': 'jogging',
        'fantasi': 'fantasy',
        
        # æ”¿æ²»/æ”¿åºœ
        'politik': 'politics',
        'demokrasi': 'democracy',
        'republik': 'republic',
        'parlemen': 'parliament',
        'konstitusi': 'constitution',
        'revolusi': 'revolution',
        'presiden': 'president',  # â­ æ–°å¢
        
        # å…¶ä»–é«˜é¢‘
        'prioritas': 'priority',
        'metode': 'method',
        'teknik': 'technique',
        'proses': 'process',
        'prosedur': 'procedure',
        'dokumen': 'document',
        'formulir': 'form',
        'informasi': 'information',
        'komunikasi': 'communication',
        'organisasi': 'organization',
        'institusi': 'institution',
        'komunitas': 'community',
        'individu': 'individual',
        'grup': 'group',
        'kolaborasi': 'collaboration',
        'proyek': 'project',
        'kampanye': 'campaign',
        'dekorasi': 'decoration',
        'konsep': 'concept',
        'ide': 'idea',
        'motivasi': 'motivation',
        'emosi': 'emotion',
        'karakter': 'character',
        'personalitas': 'personality',
        'situasi': 'situation',
        'kondisi': 'condition',
        'posisi': 'position',
        'lokasi': 'location',
        'destinasi': 'destination',
        'rute': 'route',
        'akses': 'access',
        'fasilitas': 'facility',
        'utilitas': 'utility',
        'infrastruktur': 'infrastructure',
        'zona': 'zone',
        'sektor': 'sector',
        'kategori': 'category',
        'visi': 'vision',
        'misi': 'mission',
        'sosial': 'social',
        'notifikasi': 'notification',
        'struktur': 'structure',
        'konstruksi': 'construction',
        'renovasi': 'renovation',
        'inovasi': 'innovation',
        'inspirasi': 'inspiration',
        'kreativitas': 'creativity',
        'produktivitas': 'productivity',
        'kompetisi': 'competition',
        'petisi': 'petition',
        'aksesori': 'accessory',
        'dokumentasi': 'documentation',
        'publikasi': 'publication',
        
        # === æ³•è¯­æ¥æº ===
        'restoran': 'restaurant',
        'kafe': 'cafÃ©',
        'saus': 'sauce',
        'suvenir': 'souvenir',
        'garasi': 'garage',
        'butik': 'boutique',
        
        # === è·å…°è¯­æ¥æº ===
        'kantor': 'kantoor',
        'kopi': 'koffie',
        'handuk': 'handdoek',
        'koper': 'koffer',
        'kursi': 'stoel',
        'karcis': 'kaartje',
        'kereta': 'karretje',
        'polisi': 'politie',
        'pastor': 'pastoor',
        'tenis': 'tennis',
    }
    
    # ============ è¿‡æ»¤ï¼šæ’é™¤æ‹¼å†™ç›¸åŒçš„è¯ ============
    FILTERED_LOANWORDS = {}
    for word, origin in CORE_LOANWORDS.items():
        word_normalized = word.lower().replace('Ã©', 'e').replace('Ã¨', 'e').replace('Ãª', 'e')
        origin_normalized = origin.lower().replace('Ã©', 'e').replace('Ã¨', 'e').replace('Ãª', 'e')
        
        if word_normalized != origin_normalized:
            FILTERED_LOANWORDS[word] = origin
    
    # â­ æŒ‰é¦–å­—æ¯ç­›é€‰å€Ÿè¯åº“
    if start_letter:
        FILTERED_LOANWORDS = {
            word: origin 
            for word, origin in FILTERED_LOANWORDS.items() 
            if word.startswith(start_letter)
        }
        
        if debug:
            print(f"ğŸ”¤ é¦–å­—æ¯ç­›é€‰ï¼šä»…ä¿ç•™ä»¥ '{start_letter}' å¼€å¤´çš„å€Ÿè¯")
            print(f"   ç­›é€‰åè¯åº“æ•°é‡ï¼š{len(FILTERED_LOANWORDS)} ä¸ª")
    
    # ============ â­ æ‰©å±•çš„å°å°¼è¯­åŸç”Ÿè¯åº“ ============
    
    CORE_NATIVE_WORDS = {
        # è¯­æ³•è¯
        'yang', 'dengan', 'untuk', 'dari', 'pada', 'di', 'ke', 'oleh', 'karena',
        'ini', 'itu', 'ada', 'tidak', 'bukan', 'akan', 'sudah', 'telah',
        'dan', 'atau', 'tetapi', 'jika', 'kalau', 'bahwa', 'adalah', 'sebagai',
        'tentang', 'kepada', 'bagi', 'antara', 'hingga', 'sampai',
        'melalui', 'menurut', 'selama', 'sejak', 'sebelum', 'sesudah',
        
        # ä»£è¯
        'saya', 'anda', 'kamu', 'kami', 'kita', 'mereka', 'dia', 'ia', 'beliau',
        
        # â­ æ‰©å±•çš„é«˜é¢‘åŠ¨è¯è¯æ ¹
        'buat', 'beri', 'terima', 'ambil', 'lihat', 'dengar', 'bicara', 'kata',
        'tanya', 'jawab', 'kerja', 'ajar', 'main', 'tawar', 'saji', 'laku',
        'hadap', 'tunjuk', 'milik', 'guna', 'temu', 'dapat', 'jalan', 'lari',
        'diri', 'duduk', 'tidur', 'bangun', 'makan', 'minum', 'masak', 'datang',
        'pergi', 'pulang', 'kembali', 'tiba', 'kunjung', 'nikmat', 'rasa',
        'suka', 'cinta', 'benci', 'takut', 'harap', 'mimpi', 'usaha', 'latih',
        
        # â­ é‡ç‚¹ï¼šæ·»åŠ å¸¸è¢«è¯¯åˆ¤çš„è¯æ ¹
        'kenal', 'jual', 'belanja', 'bangga',
        'juang', 'sayang', 'ingat', 'lupa', 'pakai', 'simpan',
        'tulis', 'baca', 'hitung', 'ukur', 'timbang', 'timbul',
        'hidup', 'mati', 'lahir', 'tumbuh', 'kembang', 'ubah',
        
        # é«˜é¢‘åè¯
        'rumah', 'tempat', 'kota', 'desa', 'kampung', 'orang', 'manusia', 'anak',
        'bapak', 'ibu', 'keluarga', 'teman', 'waktu', 'hari', 'tahun', 'bulan',
        'minggu', 'jam', 'malam', 'pagi', 'siang', 'sore', 'jalan', 'pintu',
        'pasar', 'toko', 'warung', 'air', 'api', 'tanah', 'angin', 'udara',
        'bunga', 'pohon', 'daun', 'gunung', 'laut', 'pantai', 'sungai', 'hutan',
        'batu', 'pulau', 'matahari', 'bulan', 'bintang', 'hewan', 'kucing',
        'anjing', 'ayam', 'burung', 'ikan', 'gedung', 'pusat',
        
        # é«˜é¢‘å½¢å®¹è¯
        'baik', 'buruk', 'besar', 'kecil', 'tinggi', 'rendah', 'panjang', 'pendek',
        'lebar', 'sempit', 'berat', 'ringan', 'indah', 'cantik', 'bagus', 'jelek',
        'senang', 'sedih', 'marah', 'takut', 'malu', 'bangga', 'mudah', 'sulit',
        'cepat', 'lambat', 'panas', 'dingin', 'hangat', 'sejuk', 'mahal', 'murah',
        'baru', 'lama', 'tua', 'muda', 'ramai', 'sepi', 'tenang', 'lezat', 'enak',
        
        # å…¶ä»–é«˜é¢‘è¯
        'dapat', 'bisa', 'mampu', 'harus', 'boleh', 'ingin', 'mau', 'sangat',
        'sekali', 'juga', 'lagi', 'hanya', 'cuma', 'semua', 'setiap', 'banyak',
        'sedikit', 'beberapa', 'siapa', 'apa', 'mana', 'kapan', 'mengapa',
        'bagaimana', 'berapa', 'begitu', 'seperti', 'penuh', 'kaya', 'salah', 'satu',
    }
    
    # â­ å°å°¼è¯­å¸¸è§è¯æ ¹åº“
    INDONESIAN_ROOTS = {
        'kenal', 'jual', 'belanja', 'bangga', 'terkenal',
        'buat', 'beri', 'ambil', 'lihat', 'dengar', 'kata', 'tanya', 'jawab',
        'kerja', 'ajar', 'main', 'tawar', 'saji', 'laku', 'juang', 'sayang',
        'datang', 'pergi', 'pulang', 'kembali', 'tiba', 'kunjung',
        'suka', 'cinta', 'takut', 'harap', 'mimpi', 'usaha', 'ingat', 'lupa',
    }
    
    # â­ ä¸“æœ‰åè¯å’Œåœ°åï¼ˆä¿®è®¢ç‰ˆ - ç§»é™¤ monumenï¼‰
    PROPER_NOUNS = {
        # åœ°å
        'jakarta', 'bali', 'yogyakarta', 'surabaya', 'bandung', 'medan',
        'semarang', 'malang', 'solo', 'ubud', 'kuta', 'seminyak',
        'malioboro', 'borobudur', 'prambanan', 'parangtritis', 'tanah', 'lot',
        'nusa', 'dua', 'sanur', 'jimbaran', 'lombok', 'gili', 'trawangan',
        
        # âš ï¸ ç§»é™¤ 'monumen'ï¼ˆå®ƒæ˜¯å€Ÿè¯ï¼Œä¸æ˜¯ä¸“æœ‰åè¯ï¼‰
        # ä¿ç•™åœ°ç†åè¯
        'candi', 'pantai', 'gunung', 'pulau', 'danau', 'taman',
    }
    
    # ============ â­ è¯ç¼€æ£€æµ‹å‡½æ•° ============
    
    def is_affixed_native_word(word: str) -> Tuple[bool, str]:
        if word.startswith('per') and word.endswith('an') and len(word) > 7:
            root = word[3:-2]
            if root in INDONESIAN_ROOTS or root in CORE_NATIVE_WORDS:
                return True, f'per-{root}-an (å°å°¼è¯­æ„è¯)'
        
        if word.startswith('ke') and word.endswith('an') and len(word) > 6:
            root = word[2:-2]
            if root in INDONESIAN_ROOTS or root in CORE_NATIVE_WORDS:
                return True, f'ke-{root}-an (å°å°¼è¯­æ„è¯)'
        
        if word.startswith('ter') and len(word) > 5:
            root = word[3:]
            if root in INDONESIAN_ROOTS or root in CORE_NATIVE_WORDS:
                return True, f'ter-{root} (å°å°¼è¯­è¢«åŠ¨/çŠ¶æ€)'
        
        for prefix in ['meny', 'meng', 'men', 'mem']:
            if word.startswith(prefix) and len(word) > len(prefix) + 2:
                root = word[len(prefix):]
                if root in INDONESIAN_ROOTS or root in CORE_NATIVE_WORDS:
                    return True, f'{prefix}-{root} (å°å°¼è¯­ä¸»åŠ¨è¯­æ€)'
        
        if word.startswith('ber') and len(word) > 5:
            root = word[3:]
            if root in INDONESIAN_ROOTS or root in CORE_NATIVE_WORDS:
                return True, f'ber-{root} (å°å°¼è¯­åŠ¨è¯)'
        
        return False, ''
    
    def is_proper_noun_context(word: str, word_index: int, words_list: List[str]) -> bool:
        if word in PROPER_NOUNS:
            return True
        
        if word_index > 0:
            prev_word = words_list[word_index - 1]
            if prev_word in PROPER_NOUNS:
                return True
        
        if word_index < len(words_list) - 1:
            next_word = words_list[word_index + 1]
            if next_word in PROPER_NOUNS:
                return True
        
        return False
    
    # ============ ä¸»æ£€æµ‹æµç¨‹ ============
    
    text_lower = content.lower()
    text_cleaned = re.sub(r'[^a-zÃ Ã¡Ã¢Ã£Ã¨Ã©ÃªÃ«Ã¬Ã­Ã®Ã¯Ã²Ã³Ã´ÃµÃ¹ÃºÃ»Ã¼\s]', ' ', text_lower)
    words = text_cleaned.split()
    
    unique_words = []
    word_positions = {}
    for i, word in enumerate(words):
        if len(word) >= 3 and word not in word_positions:
            unique_words.append(word)
            word_positions[word] = i
    
    found_loanwords = []
    excluded_words = []
    
    for word in unique_words:
        word_index = word_positions[word]
        
        # â­ é¦–å­—æ¯ç­›é€‰
        if start_letter and not word.startswith(start_letter):
            continue
        
        if word in CORE_NATIVE_WORDS:
            if debug:
                excluded_words.append((word, 'æ ¸å¿ƒåŸç”Ÿè¯'))
            continue
        
        is_affixed, affix_reason = is_affixed_native_word(word)
        if is_affixed:
            if debug:
                excluded_words.append((word, affix_reason))
            continue
        
        if is_proper_noun_context(word, word_index, words):
            if debug:
                excluded_words.append((word, 'ä¸“æœ‰åè¯/åœ°å'))
            continue
        
        if word in FILTERED_LOANWORDS:
            found_loanwords.append({
                'word': word,
                'origin': FILTERED_LOANWORDS[word],
                'method': 'core_dict',
                'confidence': 100
            })
            continue
    
    # ============ ç»Ÿè®¡ç»“æœ ============
    
    actual_count = len(found_loanwords)
    
    # ============ ç”Ÿæˆè¾“å‡º ============
    
    letter_info = f" (ä»¥å­—æ¯ '{start_letter}' å¼€å¤´)" if start_letter else ""
    
    if actual_count == required_count:
        loanword_list = [f"{item['word']} â† {item['origin']}" for item in found_loanwords]
        loanwords_str = ', '.join(loanword_list)
        
        return 1, (
            f"âœ… å¤–æ¥è¯å€Ÿè¯æ•°é‡ç¬¦åˆè¦æ±‚ï¼ˆ{actual_count} ä¸ª{letter_info}ï¼‰\n\n"
            f"æ‰¾åˆ°çš„å€Ÿè¯ï¼š{loanwords_str}"
        )
    
    elif actual_count < required_count:
        shortage = required_count - actual_count
        loanword_list = [f"{item['word']} â† {item['origin']}" for item in found_loanwords]
        loanwords_str = ', '.join(loanword_list) if loanword_list else 'æ— '
        
        return 0, (
            f"âŒ é”™è¯¯ï¼šåªæ‰¾åˆ° {actual_count} ä¸ªå¤–æ¥è¯å€Ÿè¯{letter_info}ï¼ˆè‹±è¯­/æ³•è¯­/è·å…°è¯­ï¼‰ï¼Œ"
            f"å°‘äºè¦æ±‚çš„ {required_count} ä¸ªï¼ˆè¿˜å·® {shortage} ä¸ªï¼‰\n\n"
            f"å·²æ‰¾åˆ°çš„å€Ÿè¯ï¼š{loanwords_str}"
        )
    
    else:
        excess = actual_count - required_count
        loanword_list = [f"{item['word']} â† {item['origin']}" for item in found_loanwords]
        loanwords_str = ', '.join(loanword_list)
        
        return 0, (
            f"âŒ é”™è¯¯ï¼šæ‰¾åˆ° {actual_count} ä¸ªå¤–æ¥è¯å€Ÿè¯{letter_info}ï¼ˆè‹±è¯­/æ³•è¯­/è·å…°è¯­ï¼‰ï¼Œ"
            f"è¶…è¿‡è¦æ±‚çš„ {required_count} ä¸ªï¼ˆå¤šäº† {excess} ä¸ªï¼‰\n\n"
            f"æ‰¾åˆ°çš„å€Ÿè¯ï¼š{loanwords_str}"
        )


def _levenshtein_distance(s1: str, s2: str) -> int:
    """è®¡ç®—ç¼–è¾‘è·ç¦»"""
    if len(s1) < len(s2):
        return _levenshtein_distance(s2, s1)
    if len(s2) == 0:
        return len(s1)
    
    previous_row = range(len(s2) + 1)
    for i, c1 in enumerate(s1):
        current_row = [i + 1]
        for j, c2 in enumerate(s2):
            insertions = previous_row[j + 1] + 1
            deletions = current_row[j] + 1
            substitutions = previous_row[j] + (c1 != c2)
            current_row.append(min(insertions, deletions, substitutions))
        previous_row = current_row
    
    return previous_row[-1]



# ==================== 2. ç¼©å†™è¯æ£€æµ‹ï¼ˆä¸¥æ ¼ä¹¦é¢ç¼©å†™ç‰ˆ + æ‹¬å·æ³¨é‡Šç¼©å†™ï¼‰====================

def check_indonesian_abbreviations(content, required_count, count_mode='total', debug=False, **kwargs):
    """
    æ£€æµ‹å°å°¼è¯­æ–‡æœ¬ä¸­çš„ç¼©å†™è¯æ•°é‡æ˜¯å¦è¾¾æ ‡
    
    è¯†åˆ«ä¸¤ç±»ç¼©å†™ï¼š
    1. è¯åº“ä¸­çš„æ ‡å‡†ç¼©å†™ï¼ˆKPK, TNI, PR, yg, dgn, sdhç­‰ï¼‰
    2. å¸¦æ‹¬å·æ³¨é‡Šçš„è‡ªå®šä¹‰ç¼©å†™ï¼ˆå¦‚ï¼šLMS (Learning Management System)ï¼‰
    
    ä¸¥æ ¼å®šä¹‰ï¼šä»…è¯†åˆ«çœŸæ­£çš„ä¹¦é¢ç¼©å†™å½¢å¼
    - âœ… åŒ…æ‹¬ï¼šé¦–å­—æ¯ç¼©å†™ã€ä¹¦é¢å­—æ¯ç¼©å†™ã€æ ‡å‡†ç¼©å†™ã€æ‹¬å·æ³¨é‡Šç¼©å†™
    - âŒ ä¸åŒ…æ‹¬ï¼šå£è¯­åŒ–è¡¨è¾¾ï¼ˆudah, nggak, tuh, gitu, aja, sihç­‰ï¼‰
    - âŒ ä¸åŒ…æ‹¬ï¼šæ ‡å‡†å®Œæ•´è¯æ±‡ï¼ˆyang, dengan, sudah, tapi, diç­‰ï¼‰
    - âŒ ä¸åŒ…æ‹¬ï¼šå¸¦ç‚¹å·çš„æ•¬è¯­ç¼©å†™ï¼ˆYth., Bpk., Ibu.ç­‰ï¼‰
    
    Args:
        content: æ–‡ç« å†…å®¹ï¼ˆå­—ç¬¦ä¸²æˆ–åˆ—è¡¨ï¼‰
        required_count: è¦æ±‚çš„ç¼©å†™è¯æ•°é‡ï¼ˆå¿…é¡»æ­£å¥½ç­‰äºï¼‰
        count_mode: è®¡æ•°æ¨¡å¼ ('total' æˆ– 'unique')
        debug: æ˜¯å¦è¾“å‡ºè°ƒè¯•ä¿¡æ¯ï¼ˆé»˜è®¤ Falseï¼‰
        **kwargs: å…¶ä»–å‚æ•°ï¼ˆå…¼å®¹æ€§ï¼‰
    
    Returns:
        tuple: (1/0, è¯´æ˜ä¿¡æ¯)
    """
    import re
    from collections import Counter
    
    # å¤„ç†contentä¸ºåˆ—è¡¨çš„æƒ…å†µ
    if isinstance(content, list):
        content = ' '.join(str(item) for item in content)
    content = str(content)
    
    # ==================== ä»…ä¹¦é¢ç¼©å†™è¯åº“ ====================
    
    ABBREVIATIONS = {
        # === é¦–å­—æ¯ç¼©å†™ï¼ˆä¸“æœ‰åè¯/æœºæ„ï¼‰===
        'kpk',          # Komisi Pemberantasan Korupsi
        'tni',          # Tentara Nasional Indonesia
        'polri',        # Kepolisian Republik Indonesia
        'ri',           # Republik Indonesia
        'dpr',          # Dewan Perwakilan Rakyat
        'pbb',          # Perserikatan Bangsa-Bangsa
        'asean',        # Association of Southeast Asian Nations
        'pmk',          # Peraturan Menteri Keuangan
        'uu',           # Undang-Undang
        'sop',          # Standard Operating Procedure
        'npwp',         # Nomor Pokok Wajib Pajak
        'ktp',          # Kartu Tanda Penduduk
        'sim',          # Surat Izin Mengemudi
        'pln',          # Perusahaan Listrik Negara
        'bpjs',         # Badan Penyelenggara Jaminan Sosial
        'pr',           # Pekerjaan Rumah
        'pt',           # Perseroan Terbatas
        'cv',           # Curriculum Vitae / Commanditaire Vennootschap
        'mpr',          # Majelis Permusyawaratan Rakyat
        'bumn',         # Badan Usaha Milik Negara
        'bumd',         # Badan Usaha Milik Daerah
        'pkk',          # Pemberdayaan Kesejahteraan Keluarga
        'apa',          # American Psychological Association
        'lms',          # Learning Management System
        'nim',          # Nomor Induk Mahasiswa
        'wa',           # WhatsApp
        'hrd',          # Human Resources Development
        
        # === ä¹¦é¢å­—æ¯ç¼©å†™ï¼ˆè¿è¯/ä»‹è¯ï¼‰===
        'yg',           # yang â†’ yg
        'dgn', 'dg',    # dengan â†’ dgn/dg
        'utk',          # untuk â†’ utk
        'krn',          # karena â†’ krn
        'spy',          # supaya â†’ spy
        'tp',           # tetapi â†’ tpï¼ˆä¹¦é¢ç¼©å†™ï¼‰
        'jd',           # jadi â†’ jd
        'jg',           # juga â†’ jg
        'sm',           # sama â†’ sm
        'pd',           # pada â†’ pd
        'dr',           # dari â†’ dr
        'sbg',          # sebagai â†’ sbg
        'thd',          # terhadap â†’ thd
        'tsb',          # tersebut â†’ tsb
        
        # === ä¹¦é¢éŸ³èŠ‚ç¼©å†™ï¼ˆåŠ¨è¯/å½¢å®¹è¯ï¼‰===
        'sdh',          # sudah â†’ sdhï¼ˆä¹¦é¢ï¼‰
        'blm',          # belum â†’ blm
        'hrs',          # harus â†’ hrs
        'msh',          # masih â†’ msh
        'bs',           # bisa â†’ bs
        'tdk',          # tidak â†’ tdkï¼ˆä¹¦é¢ï¼‰
        'blh',          # boleh â†’ blh
        'prlu',         # perlu â†’ prlu
        'tlh',          # telah â†’ tlh
        'bkn',          # bukan â†’ bkn
        'dpt',          # dapat â†’ dpt
        'akn',          # akan â†’ akn
        
        # === æ—¶é—´è¯ä¹¦é¢ç¼©å†™ ===
        'skrg',         # sekarang â†’ skrg
        'kmrn',         # kemarin â†’ kmrn
        'bsk',          # besok â†’ bsk
        'hr',           # hari â†’ hr
        'thn',          # tahun â†’ thn
        'bln',          # bulan â†’ bln
        'mgg',          # minggu â†’ mgg
        'tgl',          # tanggal â†’ tgl
        'td',           # tadi â†’ td
        
        # === ç–‘é—®è¯ä¹¦é¢ç¼©å†™ ===
        'gmn',          # bagaimana â†’ gmnï¼ˆä¹¦é¢ï¼‰
        'knp',          # kenapa â†’ knp
        'kpn',          # kapan â†’ kpn
        'dmn',          # dimana â†’ dmn
        'kmn',          # kemana â†’ kmn
        'brp',          # berapa â†’ brp
        'bgmn',         # bagaimana â†’ bgmn
        
        # === ç¨‹åº¦å‰¯è¯ä¹¦é¢ç¼©å†™ ===
        'bgt',          # banget â†’ bgtï¼ˆä¹¦é¢ï¼‰
        'bnr',          # benar â†’ bnr
        'bnyk',         # banyak â†’ bnyk
        'sgt',          # sangat â†’ sgt
        'trs',          # terus â†’ trs
        
        # === åè¯ç¼©å†™ ===
        'org',          # orang â†’ org
        'tmp',          # tempat â†’ tmp
        'no',           # nomor â†’ no
        'tlp', 'telp',  # telepon â†’ tlp
        'hp',           # handphone â†’ hp
        'info',         # informasi â†’ info
        'foto',         # fotografi â†’ foto
        'dok',          # dokter â†’ dok
        
        # === ä»‹è¯ç¼©å†™ ===
        'sblm',         # sebelum â†’ sblm
        'stlh',         # setelah â†’ stlh
        'slm',          # selama â†’ slm
        'krg',          # kurang â†’ krg
        'lbh',          # lebih â†’ lbh
        
        # === æ ‡å‡†ç¼©å†™ ===
        'dll',          # dan lain-lain
        'dsb',          # dan sebagainya
        'dst',          # dan seterusnya
        'dkk',          # dan kawan-kawan
        'aj',           # saja â†’ ajï¼ˆä¹¦é¢ç¼©å†™ï¼‰
        'etc',          # et cetera
        
        # === è´§å¸å’Œå•ä½ ===
        'rp',           # Rupiah
        'km',           # kilometer
        'kg',           # kilogram
        'gr',           # gram
        'lt',           # liter
        'cm',           # centimeter
        'mm',           # millimeter
        'wib',          # Waktu Indonesia Barat
        'wita',         # Waktu Indonesia Tengah
        'wit',          # Waktu Indonesia Timur
        
        # === å…¶ä»–ä¸“ä¸šç¼©å†™ ===
        'rt', 'rw',     # Rukun Tetangga/Warga
        'atm',          # ATM
        'nib',          # Nomor Induk Berusaha
        'nik',          # Nomor Induk Kependudukan
        'skck',         # Surat Keterangan Catatan Kepolisian
        'stnk',         # Surat Tanda Nomor Kendaraan
        
        # === æ•™è‚²ç›¸å…³ ===
        'sd',           # Sekolah Dasar
        'smp',          # Sekolah Menengah Pertama
        'sma',          # Sekolah Menengah Atas
        'smk',          # Sekolah Menengah Kejuruan
        's1', 's2', 's3',  # Strata 1/2/3 (å­¦ä½)
        
        # === ç½‘ç»œæ ‡å‡†ç¼©å†™ ===
        'btw', 'fyi', 'asap', 'thx', 'pls', 'msg',
        'omg', 'lol', 'brb', 'ttyl', 'imho',
    }
    
    # === å®Œå…¨æ’é™¤åˆ—è¡¨ï¼ˆå£è¯­+æ ‡å‡†è¯+æ•¬è¯­ç¼©å†™ï¼‰===
    EXCLUDED_WORDS = {
        # === å¸¦ç‚¹å·çš„æ•¬è¯­ç¼©å†™ï¼ˆä¸è®¡å…¥ç¼©å†™è¯ï¼‰===
        'yth',          # Yang Terhormat (Yth.)
        'bpk',          # Bapak (Bpk.)
        'ibu',          # Ibu (Ibu.)
        'sdr',          # Saudara (Sdr.)
        'sdri',         # Saudari (Sdri.)
        'tn',           # Tuan (Tn.)
        'ny',           # Nyonya (Ny.)
        'nn',           # Nona (Nn.)
        
        # === å£è¯­éŸ³èŠ‚ç®€åŒ–ï¼ˆä¸æ˜¯ä¹¦é¢ç¼©å†™ï¼‰===
        'udah', 'dah',      # sudahçš„å£è¯­
        'gak', 'ga',        # tidakçš„å£è¯­
        'nggak', 'ngga',    # tidakçš„å£è¯­
        'gpp',              # gak apa-apa
        'tuh', 'nih',       # itu/iniçš„å£è¯­
        'gitu', 'gini',     # begitu/beginiçš„å£è¯­
        'ntar', 'nti',      # nantiçš„å£è¯­
        'tau',              # tahuçš„å£è¯­
        'emg',              # emangçš„ç¼©å†™ï¼ˆä½†emangæœ¬èº«æ˜¯å£è¯­ï¼‰
        
        # === å£è¯­ä»£è¯ ===
        'gue', 'gua', 'gw', 'ane',
        'lu', 'elu', 'lo', 'loe',
        
        # === è¯­æ°”è¯ï¼ˆå®Œæ•´è¯ï¼Œä¸æ˜¯ç¼©å†™ï¼‰===
        'sih', 'aja', 'deh', 'dong', 'kok', 'dunk',
        'ya', 'yah', 'nih', 'lah', 'kah',
        
        # === å£è¯­å®Œæ•´å½¢å¼ ===
        'banget',           # å®Œæ•´è¯
        'emang',            # å®Œæ•´è¯
        'kalo', 'kalau',    # å®Œæ•´è¯
        'gimana',           # å®Œæ•´è¯
        'kenapa',           # å®Œæ•´è¯
        'bener', 'benar',   # å®Œæ•´è¯
        'banyak',           # å®Œæ•´è¯
        'sekarang',         # å®Œæ•´è¯
        'tapi', 'tetapi',   # å®Œæ•´è¯
        'kayak', 'kaya',    # å®Œæ•´è¯
        'buat',             # å®Œæ•´è¯
        'pake', 'pakai',    # å®Œæ•´è¯
        'bikin',            # å®Œæ•´è¯
        'dapet', 'dapat',   # å®Œæ•´è¯
        'kasih', 'ngasih',  # å®Œæ•´è¯
        'cuma', 'cuman',    # å®Œæ•´è¯
        'abis', 'habis',    # å®Œæ•´è¯
        'soal', 'soalnya',  # å®Œæ•´è¯
        'nyampe', 'sampai', # å®Œæ•´è¯
        'mesti',            # å®Œæ•´è¯
        'terus',            # å®Œæ•´è¯
        'ngerti',           # å®Œæ•´è¯ï¼ˆå£è¯­ mengertiï¼‰
        'ngurangin',        # å®Œæ•´è¯ï¼ˆå£è¯­ mengurangiï¼‰
        
        # === æ ‡å‡†ä¹¦é¢è¯­ï¼ˆå®Œæ•´è¯ï¼‰===
        'yang', 'dengan', 'untuk', 'karena', 'jadi', 'juga',
        'bisa', 'sudah', 'belum', 'lagi', 'masih', 'harus', 'akan',
        'sama', 'dari', 'pada', 'oleh', 'atau', 'bila', 'jika',
        'maka', 'lalu', 'kemudian', 'seperti', 'tahu', 'orang',
        'sebelum', 'setelah', 'saja', 'ini', 'itu', 'ada', 'tidak',
        'bukan', 'mana', 'siapa', 'kapan', 'dimana', 'bagaimana',
        'apakah', 'bahwa', 'agar', 'supaya', 'tetapi',
        
        # === å®Œæ•´å½¢å®¹è¯/å‰¯è¯ï¼ˆä¸æ˜¯ç¼©å†™ï¼‰===
        'singkat', 'panjang', 'pendek', 'maksimal', 'minimal',
        'cukup', 'kurang', 'lebih',
        
        # === æ ‡å‡†ä»‹è¯/è¿è¯ï¼ˆå®Œæ•´è¯ï¼Œä¸æ˜¯ç¼©å†™ï¼‰===
        'di', 'ke', 'dan', 'dalam', 'oleh', 'pada',
        
        # === å…¶ä»–å®Œæ•´è¯ ===
        'ya', 'tidak', 'bukan', 'jangan', 'belum', 'sudah',
        'bisa', 'boleh', 'harus', 'perlu', 'mau', 'ingin',
    }
    
    # ==================== æ¸…ç†æ–‡æœ¬å¹¶æå–å•è¯ ====================
    text_lower = content.lower()
    
    # å…ˆç§»é™¤æ‰€æœ‰å¸¦ç‚¹å·çš„æ•¬è¯­ç¼©å†™ï¼ˆå¦‚ Yth., Bpk., Ibu.ï¼‰
    # è¿™æ ·å®ƒä»¬ä¸ä¼šè¢«è®¡å…¥ç¼©å†™è¯
    text_lower = re.sub(r'\b(yth|bpk|ibu|sdr|sdri|tn|ny|nn)\.', '', text_lower)
    
    # ==================== 1. æ”¹è¿›çš„æ‹¬å·æ³¨é‡Šç¼©å†™è¯†åˆ« ====================
    # åªåŒ¹é…çœŸæ­£çš„ç¼©å†™å½¢å¼ï¼ˆ2-6ä¸ªå­—æ¯ï¼‰+ è¾ƒé•¿çš„æ‹¬å·æ³¨é‡Šï¼ˆ20+å­—ç¬¦ï¼‰
    # é¿å…è¯¯åŒ¹é…å¦‚ "singkat (maksimal 1000 kata)"
    bracket_pattern = r'\b([a-z]{2,6})\s*\(([^)]{20,})\)'
    bracket_matches = re.finditer(bracket_pattern, text_lower)
    
    bracket_abbreviations = []
    for match in bracket_matches:
        abbr = match.group(1)
        explanation = match.group(2)
        
        # é¢å¤–éªŒè¯ï¼šæ‹¬å·å†…å®¹åº”è¯¥æ˜¯å®Œæ•´è§£é‡Šï¼ˆåŒ…å«ç©ºæ ¼å’Œå¤šä¸ªå•è¯ï¼‰
        if ' ' in explanation and len(explanation.split()) >= 2:
            # æ’é™¤æ™®é€šå½¢å®¹è¯
            if abbr not in EXCLUDED_WORDS:
                bracket_abbreviations.append(abbr)
    
    # ==================== 2. æå–è¯åº“ä¸­çš„ç¼©å†™ ====================
    words = re.findall(r'\b[a-z]+\b', text_lower)
    
    # ==================== æŸ¥æ‰¾ç¼©å†™è¯ ====================
    found_abbreviations = []
    found_abbreviations_dict = {}  # ç”¨äºè®°å½•æ¯ä¸ªç¼©å†™çš„æ¥æº
    
    # æ·»åŠ æ‹¬å·æ³¨é‡Šç¼©å†™
    for abbr in bracket_abbreviations:
        found_abbreviations.append(abbr)
        if abbr not in found_abbreviations_dict:
            found_abbreviations_dict[abbr] = 'æ‹¬å·æ³¨é‡Š'
    
    # æ·»åŠ è¯åº“ä¸­çš„ç¼©å†™
    for word in words:
        if len(word) < 2:
            continue
        if word in EXCLUDED_WORDS:
            continue
        if word in ABBREVIATIONS:
            found_abbreviations.append(word)
            if word not in found_abbreviations_dict:
                found_abbreviations_dict[word] = 'è¯åº“'
    
    # ==================== ç»Ÿè®¡ç»“æœ ====================
    if count_mode == 'unique':
        # å»é‡ï¼šç›¸åŒçš„ç¼©å†™åªç®—ä¸€æ¬¡
        unique_abbrs = list(set(found_abbreviations))
        actual_count = len(unique_abbrs)
        count_description = "ä¸åŒçš„ç¼©å†™è¯"
        display_list = sorted(unique_abbrs)
    else:
        # æ€»æ•°ï¼šåŒ…å«é‡å¤
        actual_count = len(found_abbreviations)
        count_description = "ç¼©å†™è¯ï¼ˆå«é‡å¤ï¼‰"
        display_list = sorted(list(set(found_abbreviations)))
    
    # ==================== åˆ¤æ–­æ˜¯å¦è¾¾æ ‡ï¼ˆæ­£å¥½ç­‰äºï¼‰====================
    if actual_count == required_count:
        abbreviation_list = ", ".join(display_list[:30])
        if len(display_list) > 30:
            abbreviation_list += f" ... (è¿˜æœ‰ {len(display_list) - 30} ä¸ª)"
        
        msg = f"âœ… æ­£ç¡®ï¼šæ–‡ç« åŒ…å«æ­£å¥½ {actual_count} ä¸ª{count_description}ï¼Œç¬¦åˆè¦æ±‚çš„ {required_count} ä¸ª\n\n"
        msg += f"æ‰¾åˆ°çš„ç¼©å†™è¯ï¼š{abbreviation_list}"
        
        if count_mode == 'total':
            unique_count = len(set(found_abbreviations))
            if unique_count != actual_count:
                msg += f"\n\nï¼ˆæ³¨ï¼šå»é‡åæœ‰ {unique_count} ä¸ªä¸åŒçš„ç¼©å†™è¯ï¼‰"
        
        if debug:
            word_counts = Counter(found_abbreviations)
            top_words = word_counts.most_common(10)
            msg += f"\n\nã€è°ƒè¯•ä¿¡æ¯ã€‘ä½¿ç”¨æœ€å¤šçš„10ä¸ªç¼©å†™è¯ï¼š"
            for word, count in top_words:
                source = found_abbreviations_dict.get(word, 'æœªçŸ¥')
                msg += f"\n  - {word}: {count}æ¬¡ (æ¥æº: {source})"
        
        return 1, msg
    else:
        if actual_count > required_count:
            difference = actual_count - required_count
            diff_msg = f"å¤šäº† {difference} ä¸ª"
        else:
            difference = required_count - actual_count
            diff_msg = f"å°‘äº† {difference} ä¸ª"
        
        msg = f"âŒ é”™è¯¯ï¼šæ–‡ç« åŒ…å« {actual_count} ä¸ª{count_description}ï¼Œä¸ç¬¦åˆè¦æ±‚çš„æ­£å¥½ {required_count} ä¸ªï¼ˆ{diff_msg}ï¼‰"
        
        if display_list:
            abbreviation_list = ", ".join(display_list)
            msg += f"\n\nå·²æ‰¾åˆ°çš„ç¼©å†™è¯ï¼š{abbreviation_list}"
            
            if count_mode == 'total' and len(found_abbreviations) > 0:
                word_counts = Counter(found_abbreviations)
                freq_info = [f"{word}({count}æ¬¡)" for word, count in word_counts.most_common()]
                msg += f"\nè¯¦ç»†ç»Ÿè®¡ï¼š{', '.join(freq_info)}"
        else:
            msg += "\n\næç¤ºï¼šæœªæ‰¾åˆ°ä»»ä½•ç¬¦åˆè¦æ±‚çš„ç¼©å†™è¯"
        
        return 0, msg

# ==================== 3. å¤æ•°å½¢å¼æ£€æµ‹ ====================

def check_indonesian_plurals(content, expected_count, debug=False, **kwargs):
    """
    æ£€æµ‹å°å°¼è¯­æ–‡ç« ä¸­æ ¼å¼æ­£ç¡®çš„å¤æ•°å½¢å¼æ•°é‡
    
    è®¡æ•°è§„åˆ™ï¼šé‡å¤å‡ºç°çš„å¤æ•°å½¢å¼ä¼šè¢«å¤šæ¬¡è®¡æ•°ï¼ˆtotalæ¨¡å¼ï¼‰
    
    Args:
        content: å°å°¼è¯­æ–‡ç« å†…å®¹ï¼ˆå­—ç¬¦ä¸²æˆ–åˆ—è¡¨ï¼‰
        expected_count: æœŸæœ›çš„å¤æ•°å½¢å¼æ•°é‡
        debug: æ˜¯å¦è¾“å‡ºè°ƒè¯•ä¿¡æ¯
        **kwargs: å…¶ä»–å‚æ•°ï¼ˆå…¼å®¹æ€§ï¼‰
    
    Returns:
        tuple: (1/0, è¯´æ˜ä¿¡æ¯)
    """
    import re
    from collections import Counter
    
    # ==================== æœ¬å‡½æ•°ä¸“ç”¨è¯åº“ ====================
    VALID_NOUNS = {
        'anak', 'orang', 'teman', 'keluarga', 'wisatawan', 'turis', 'pengunjung',
        'penduduk', 'warga', 'pemuda', 'kakek', 'nenek', 'pria', 'wanita',
        'guru', 'dokter', 'perawat', 'pelayan', 'sopir', 'pemandu',
        'tempat', 'kota', 'desa', 'negara', 'pulau', 'pantai', 'gunung',
        'sungai', 'danau', 'laut', 'taman', 'kebun', 'hutan', 'jalan',
        'gedung', 'rumah', 'toko', 'pasar', 'restoran', 'hotel', 'penginapan',
        'museum', 'galeri', 'sekolah', 'universitas', 'kantor', 'pabrik',
        'bandara', 'pelabuhan', 'terminal', 'stasiun', 'halte',
        'buku', 'koran', 'majalah', 'surat', 'dokumen', 'foto', 'gambar',
        'barang', 'produk', 'makanan', 'minuman', 'buah', 'sayur',
        'pakaian', 'sepatu', 'tas', 'dompet', 'kunci', 'handphone',
        'mobil', 'motor', 'sepeda', 'bus', 'kereta', 'pesawat', 'kapal',
        'kursi', 'meja', 'lemari', 'ranjang', 'pintu', 'jendela',
        'hari', 'minggu', 'bulan', 'tahun', 'jam', 'menit', 'detik', 'waktu',
        'pagi', 'siang', 'sore', 'malam', 'musim',
        'kegiatan', 'aktivitas', 'acara', 'festival', 'pertunjukan', 'konser',
        'masalah', 'solusi', 'pilihan', 'cara', 'metode', 'sistem',
        'koleksi', 'wahana', 'bangunan', 'objek', 'spot', 'lokasi',
        'pemandangan', 'panorama', 'atraksi', 'wisata',
        'pohon', 'bunga', 'rumput', 'daun', 'batu', 'pasir',
        'hewan', 'burung', 'ikan', 'kucing', 'anjing'
    }

    VALID_ADJECTIVES = {
        'besar', 'kecil', 'tinggi', 'rendah', 'panjang', 'pendek',
        'lebar', 'sempit', 'tebal', 'tipis', 'luas', 'dalam',
        'berat', 'ringan', 'kuat', 'lemah', 'keras', 'lembut',
        'indah', 'cantik', 'bagus', 'jelek', 'menarik', 'membosankan',
        'elok', 'menawan', 'rupawan', 'tampan', 'gagah',
        'bersih', 'kotor', 'rapi', 'berantakan', 'baru', 'lama', 'tua', 'muda',
        'segar', 'layu', 'hidup', 'mati', 'penuh', 'kosong',
        'cepat', 'lambat', 'mudah', 'sulit', 'gampang', 'susah',
        'mahal', 'murah', 'gratis', 'berharga', 'bernilai',
        'panas', 'dingin', 'hangat', 'sejuk', 'lembab', 'kering',
        'ramai', 'sepi', 'tenang', 'bising', 'terang', 'gelap', 'redup',
        'nyaring', 'pelan', 'keras', 'lembut',
        'baik', 'buruk', 'enak', 'lezat', 'pahit', 'manis', 'asin',
        'asam', 'gurih', 'pedas', 'hambar',
        'senang', 'sedih', 'gembira', 'marah', 'takut', 'berani',
        'ramah', 'kasar', 'sopan', 'rajin', 'malas'
    }
    
    # ==================== æœ¬å‡½æ•°ä¸“ç”¨è¾…åŠ©å‡½æ•° ====================
    
    def normalize_content(content):
        if isinstance(content, list):
            text = " ".join(str(item) for item in content)
        else:
            text = str(content)
        return text.strip()
    
    def parse_count(value):
        try:
            if isinstance(value, int):
                return value
            if isinstance(value, str):
                cleaned = value.strip().replace('###', '')
                return int(float(cleaned))
            return int(value)
        except:
            return None
    
    def find_reduplications(text):
        """æŸ¥æ‰¾æ‰€æœ‰é‡å è¯ï¼Œä¸å»é‡"""
        pattern = r'\b([a-zA-Z]+)-\1\b'
        matches = re.finditer(pattern, text.lower())
        results = []
        for match in matches:
            results.append({
                'full': match.group(0),
                'base': match.group(1),
                'position': match.start()
            })
        return results
    
    def validate_plurals(plural_matches):
        """éªŒè¯å¤æ•°å½¢å¼ï¼Œä¿ç•™æ‰€æœ‰é‡å¤"""
        valid = []
        
        for match in plural_matches:
            full_word = match['full']
            base_word = match['base']
            
            # éªŒè¯æ˜¯å¦ä¸ºæœ‰æ•ˆçš„å¤æ•°å½¢å¼
            if base_word in VALID_NOUNS:
                valid.append({
                    'word': full_word, 
                    'base': base_word, 
                    'type': 'åè¯',
                    'position': match['position']
                })
            elif base_word in VALID_ADJECTIVES:
                valid.append({
                    'word': full_word, 
                    'base': base_word, 
                    'type': 'å½¢å®¹è¯',
                    'position': match['position']
                })
            elif len(base_word) >= 3:
                # å¯¹äºä¸åœ¨è¯åº“ä¸­çš„è¯ï¼Œå¦‚æœé•¿åº¦>=3ï¼Œä¹Ÿè®¤ä¸ºæ˜¯æœ‰æ•ˆçš„
                valid.append({
                    'word': full_word, 
                    'base': base_word, 
                    'type': 'åè¯',
                    'position': match['position']
                })
        
        return valid
    
    # ==================== ä¸»é€»è¾‘ ====================
    
    debug = bool(debug) if debug is not None else False
    
    try:
        expected_count = parse_count(expected_count)
        if expected_count is None:
            return 0, "âŒ æœŸæœ›æ•°é‡æ ¼å¼é”™è¯¯"
        
        text = normalize_content(content)
        if not text:
            return 0, "âŒ æ–‡ç« å†…å®¹ä¸ºç©º"
        
        # æŸ¥æ‰¾æ‰€æœ‰é‡å è¯
        plural_matches = find_reduplications(text)
        
        # éªŒè¯æœ‰æ•ˆæ€§ï¼ˆä¸å»é‡ï¼‰
        valid_plurals = validate_plurals(plural_matches)
        
        # æ€»æ•°è®¡æ•°ï¼ˆåŒ…å«é‡å¤ï¼‰
        actual_count = len(valid_plurals)
        
        # ç»Ÿè®¡æ¯ä¸ªå¤æ•°å½¢å¼çš„å‡ºç°æ¬¡æ•°
        plural_counter = Counter([p['word'] for p in valid_plurals])
        unique_count = len(plural_counter)
        
        # ==================== ç”Ÿæˆè¯¦ç»†è¯´æ˜ ====================
        
        if valid_plurals:
            # æŒ‰å‡ºç°æ¬¡æ•°æ’åºæ˜¾ç¤º
            plural_items = []
            for word, count in plural_counter.most_common():
                base = next(p['base'] for p in valid_plurals if p['word'] == word)
                word_type = next(p['type'] for p in valid_plurals if p['word'] == word)
                if count > 1:
                    plural_items.append(f"{word}({base}çš„{word_type}å¤æ•°ï¼Œå‡ºç°{count}æ¬¡)")
                else:
                    plural_items.append(f"{word}({base}çš„{word_type}å¤æ•°)")
            plural_str = "ã€".join(plural_items)
        else:
            plural_str = "æœªæ‰¾åˆ°"
        
        # ==================== åˆ¤æ–­æ˜¯å¦è¾¾æ ‡ ====================
        
        if actual_count == expected_count:
            msg = f"âœ… æ­£ç¡®ï¼šæ–‡ç« ä¸­åŒ…å«æ­£å¥½ {actual_count} ä¸ªæ ¼å¼æ­£ç¡®çš„å¤æ•°å½¢å¼"
            if unique_count != actual_count:
                msg += f"ï¼ˆå»é‡åæœ‰ {unique_count} ä¸ªä¸åŒçš„å¤æ•°ï¼‰"
            msg += f"\n\næ‰¾åˆ°çš„å¤æ•°ï¼š{plural_str}"
            return 1, msg
        
        diff = actual_count - expected_count
        if diff > 0:
            msg = f"âŒ é”™è¯¯ï¼šæ–‡ç« ä¸­åŒ…å« {actual_count} ä¸ªå¤æ•°å½¢å¼ï¼Œè¶…è¿‡æœŸæœ›çš„ {expected_count} ä¸ªï¼ˆå¤šäº† {diff} ä¸ªï¼‰"
        else:
            msg = f"âŒ é”™è¯¯ï¼šæ–‡ç« ä¸­åŒ…å« {actual_count} ä¸ªå¤æ•°å½¢å¼ï¼Œå°‘äºæœŸæœ›çš„ {expected_count} ä¸ªï¼ˆå°‘äº† {-diff} ä¸ªï¼‰"
        
        if unique_count != actual_count:
            msg += f"\nï¼ˆæ³¨ï¼šå»é‡åæœ‰ {unique_count} ä¸ªä¸åŒçš„å¤æ•°ï¼‰"
        
        msg += f"\n\næ‰¾åˆ°çš„å¤æ•°ï¼š{plural_str}"
        
        if debug:
            msg += f"\n\nã€è°ƒè¯•ä¿¡æ¯ã€‘"
            msg += f"\n  - æ€»è®¡æ•°: {actual_count}"
            msg += f"\n  - ä¸åŒå¤æ•°: {unique_count}"
            msg += f"\n  - è¯¦ç»†åˆ—è¡¨:"
            for p in valid_plurals:
                msg += f"\n    Â· {p['word']} (ä½ç½®: {p['position']})"
        
        return 0, msg
        
    except Exception as e:
        import traceback
        return 0, f"âŒ å‡½æ•°æ‰§è¡Œå¼‚å¸¸: {str(e)}\n{traceback.format_exc()}"



# ==================== 4. å¦å®šè¯æ£€æµ‹ï¼ˆçµæ´»ç‰ˆ + è¯­å¢ƒåˆ†æï¼‰====================

def check_indonesian_negation_keyword(content, keyword, debug=False, **kwargs):
    """
    æ£€æµ‹ç‰¹å®šå¦å®šè¯çš„ä½¿ç”¨æ˜¯å¦æ­£ç¡®ï¼ˆçµæ´»ç‰ˆï¼‰
    
    æ ¸å¿ƒåŸåˆ™ï¼š
    - å¦‚æœæŸä¸ªå¦å®šç»“æ„ä¸¤ç§å¦å®šè¯éƒ½å¯ä»¥ï¼ˆè¯­ä¹‰ç•¥æœ‰ä¸åŒä½†éƒ½æ­£ç¡®ï¼‰ï¼Œä¸æŠ¥é”™
    - åªæ£€æµ‹æ˜ç¡®çš„ã€æ— äº‰è®®çš„è¯­æ³•é”™è¯¯
    - è€ƒè™‘è¯­å¢ƒå’Œå®Œæ•´çŸ­è¯­
    
    Args:
        content: æ–‡ç« å†…å®¹ï¼ˆå­—ç¬¦ä¸²æˆ–åˆ—è¡¨ï¼‰
        keyword: è¦æ£€æµ‹çš„å¦å®šè¯ (tidak/bukan/jangan)
        debug: æ˜¯å¦è¾“å‡ºè°ƒè¯•ä¿¡æ¯ï¼ˆé»˜è®¤ Falseï¼‰
        **kwargs: å¿½ç•¥å…¶ä»–å‚æ•°ï¼ˆå…¼å®¹æ€§ï¼‰
    
    Returns:
        tuple: (1/0, è¯´æ˜ä¿¡æ¯)
    """
    import re
    
    # ==================== æœ¬å‡½æ•°ä¸“ç”¨è¯åº“ ====================
    
    # å›ºå®šæ­é…ï¼ˆæ€»æ˜¯æ­£ç¡®ï¼Œä¸æ£€æŸ¥ï¼‰
    FIXED_EXPRESSIONS = {
        'bukan berarti', 'bukan berasal', 'bukan bermaksud', 'bukan berniat',
        'bukan berharap', 'bukan beranggapan', 'bukan berpikir', 'bukan berasumsi',
        'bukan bermain', 'bukan berbicara', 'bukan berlaku', 'bukan bekerja',
        'bukan bertujuan', 'bukan berfungsi', 'bukan main', 'bukan kepalang',
        'tidak lain', 'tidak beda', 'tidak ubahnya',
    }
    
    # çŸ­è¯­æ¨¡å¼ï¼šå¦‚æœåé¢è·Ÿè¿™äº›è¯ï¼Œè¯´æ˜æ˜¯åè¯æ€§çŸ­è¯­ï¼Œbukan/tidak éƒ½å¯ä»¥
    PHRASE_INDICATORS = {
        'tanpa', 'dengan', 'karena', 'untuk', 'dari', 'tentang', 'seperti',
        'hanya', 'saja', 'pun', 'lagi', 'juga',
    }
    
    # å¯¹æ¯”ç»“æ„æ ‡è®°è¯ï¼ˆå‡ºç°è¿™äº›è¯è¯´æ˜æ˜¯å¯¹æ¯”ï¼Œbukan/tidak éƒ½å¯ä»¥ï¼‰
    CONTRAST_MARKERS = {
        'tetapi', 'tapi', 'melainkan', 'namun', 'akan tetapi',
        'sebaliknya', 'justru', 'bahkan',
    }
    
    # åªèƒ½ç”¨ bukan çš„æ˜ç¡®æƒ…å†µï¼šçº¯åè¯ï¼ˆèº«ä»½/èŒä¸šï¼‰
    IDENTITY_NOUNS = {
        'guru', 'dokter', 'perawat', 'mahasiswa', 'siswa', 'murid',
        'pilot', 'polisi', 'tentara', 'petani', 'nelayan', 'sopir',
        'pengusaha', 'karyawan', 'pelayan', 'pemandu', 'wartawan',
        'artis', 'penyanyi', 'atlet', 'presiden', 'menteri',
    }
    
    # åªèƒ½ç”¨ tidak çš„æ˜ç¡®æƒ…å†µï¼šå•ä¸ªåŠ¨è¯è¯æ ¹ï¼ˆæ— å‰åç¼€ï¼Œæ— ä¿®é¥°è¯­ï¼‰
    SIMPLE_VERBS = {
        'pergi', 'datang', 'pulang', 'dateng', 'pergi',
        'makan', 'minum', 'tidur', 'bangun', 'jalan', 'lari',
        'beli', 'jual', 'bayar', 'kirim', 'terima', 'bawa',
        'buka', 'tutup', 'masuk', 'keluar', 'naik', 'turun',
        'suka', 'mau', 'ingin', 'bisa', 'boleh', 'dapat',
        'tahu', 'ingat', 'lupa', 'mengerti', 'paham',
    }
    
    # åªèƒ½ç”¨ tidak çš„æ˜ç¡®æƒ…å†µï¼šå•ä¸ªå½¢å®¹è¯ï¼ˆæ— ä¿®é¥°è¯­ï¼‰
    SIMPLE_ADJECTIVES = {
        'besar', 'kecil', 'tinggi', 'rendah', 'panjang', 'pendek',
        'baik', 'buruk', 'bagus', 'jelek', 'cantik', 'indah',
        'bersih', 'kotor', 'baru', 'lama', 'tua', 'muda',
        'panas', 'dingin', 'hangat', 'cepat', 'lambat', 'pelan',
        'mudah', 'sulit', 'gampang', 'susah', 'mahal', 'murah',
        'enak', 'lezat', 'pahit', 'manis', 'asin', 'asam', 'pedas',
        'senang', 'sedih', 'marah', 'takut', 'berani', 'ramah',
        'penting', 'perlu', 'cukup',
    }
    
    # ==================== è¾…åŠ©å‡½æ•° ====================
    
    def normalize_content(content):
        if isinstance(content, list):
            text = " ".join(str(item) for item in content)
        else:
            text = str(content)
        return text.strip()
    
    def is_phrase_context(text, match_start, match_end):
        """æ£€æŸ¥æ˜¯å¦åœ¨çŸ­è¯­è¯­å¢ƒä¸­ï¼ˆåé¢æœ‰ä¿®é¥°è¯­ï¼‰"""
        # è·å–å¦å®šè¯åçš„10ä¸ªè¯
        after_text = text[match_end:match_end+100].strip()
        
        # æ£€æŸ¥æ˜¯å¦æœ‰çŸ­è¯­æŒ‡ç¤ºè¯
        for indicator in PHRASE_INDICATORS:
            if after_text.startswith(indicator) or f' {indicator} ' in after_text[:50]:
                return True
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯è¾ƒé•¿çš„ä¿®é¥°ç»“æ„ï¼ˆ3ä¸ªè¯ä»¥ä¸Šï¼‰
        words_after = after_text.split()[:5]
        if len(words_after) >= 3:
            return True
        
        return False
    
    def is_contrast_context(text, match_start):
        """æ£€æŸ¥æ˜¯å¦åœ¨å¯¹æ¯”è¯­å¢ƒä¸­"""
        # å‘å‰çœ‹50ä¸ªå­—ç¬¦
        before_text = text[max(0, match_start-50):match_start]
        # å‘åçœ‹50ä¸ªå­—ç¬¦
        after_text = text[match_start:match_start+50]
        
        combined = before_text + after_text
        
        # æ£€æŸ¥å¯¹æ¯”æ ‡è®°
        for marker in CONTRAST_MARKERS:
            if marker in combined.lower():
                return True
        
        # æ£€æŸ¥æ˜¯å¦æœ‰é€—å·åˆ†éš”çš„å¯¹æ¯”ç»“æ„
        if ',' in before_text or ',' in after_text:
            return True
        
        return False
    
    def check_strict_errors_only(text, keyword_to_check):
        """åªæ£€æŸ¥æ— äº‰è®®çš„æ˜ç¡®é”™è¯¯"""
        errors = []
        text_lower = text.lower()
        
        if keyword_to_check == 'tidak':
            # æ˜ç¡®é”™è¯¯ï¼štidak + èº«ä»½åè¯ï¼ˆå•ä¸ªï¼Œæ— ä¿®é¥°ï¼‰
            for noun in IDENTITY_NOUNS:
                pattern = r'\btidak\s+' + re.escape(noun) + r'(?:\s|[.,;!?]|$)'
                for match in re.finditer(pattern, text_lower):
                    phrase = match.group(0).strip()
                    errors.append({
                        'phrase': phrase,
                        'reason': f'å¦å®šèº«ä»½åè¯"{noun}"å¿…é¡»ä½¿ç”¨"bukan"',
                        'suggestion': phrase.replace('tidak', 'bukan'),
                    })
        
        elif keyword_to_check == 'bukan':
            # æ˜ç¡®é”™è¯¯1ï¼šbukan + å•ä¸ªåŠ¨è¯ï¼ˆéå›ºå®šæ­é…ï¼Œéå¯¹æ¯”ï¼ŒéçŸ­è¯­ï¼‰
            for verb in SIMPLE_VERBS:
                pattern = r'\bbukan\s+' + re.escape(verb) + r'(?:\s|[.,;!?]|$)'
                for match in re.finditer(pattern, text_lower):
                    match_start = match.start()
                    match_end = match.end()
                    phrase = match.group(0).strip()
                    
                    # æ£€æŸ¥ä¾‹å¤–æƒ…å†µ
                    if phrase.lower() in FIXED_EXPRESSIONS:
                        continue
                    if is_phrase_context(text_lower, match_start, match_end):
                        continue  # çŸ­è¯­è¯­å¢ƒï¼Œä¸¤è€…éƒ½å¯ä»¥
                    if is_contrast_context(text_lower, match_start):
                        continue  # å¯¹æ¯”è¯­å¢ƒï¼Œä¸¤è€…éƒ½å¯ä»¥
                    
                    errors.append({
                        'phrase': phrase,
                        'reason': f'å¦å®šå•ä¸ªåŠ¨è¯"{verb}"ï¼ˆæ— ä¿®é¥°è¯­ï¼‰å¿…é¡»ä½¿ç”¨"tidak"',
                        'suggestion': phrase.replace('bukan', 'tidak'),
                    })
            
            # æ˜ç¡®é”™è¯¯2ï¼šbukan + å•ä¸ªå½¢å®¹è¯ï¼ˆæ— ä¿®é¥°è¯­ï¼‰
            for adj in SIMPLE_ADJECTIVES:
                pattern = r'\bbukan\s+' + re.escape(adj) + r'(?:\s|[.,;!?]|$)'
                for match in re.finditer(pattern, text_lower):
                    match_start = match.start()
                    match_end = match.end()
                    phrase = match.group(0).strip()
                    
                    # æ£€æŸ¥ä¾‹å¤–æƒ…å†µ
                    if is_phrase_context(text_lower, match_start, match_end):
                        continue  # çŸ­è¯­è¯­å¢ƒï¼Œä¸¤è€…éƒ½å¯ä»¥
                    if is_contrast_context(text_lower, match_start):
                        continue  # å¯¹æ¯”è¯­å¢ƒï¼Œä¸¤è€…éƒ½å¯ä»¥
                    
                    errors.append({
                        'phrase': phrase,
                        'reason': f'å¦å®šå•ä¸ªå½¢å®¹è¯"{adj}"ï¼ˆæ— ä¿®é¥°è¯­ï¼‰å¿…é¡»ä½¿ç”¨"tidak"',
                        'suggestion': phrase.replace('bukan', 'tidak'),
                    })
            
            # æ˜ç¡®é”™è¯¯3ï¼šbukan + me-/ber- åŠ¨è¯ï¼ˆéå›ºå®šæ­é…ï¼Œéå¯¹æ¯”ï¼‰
            active_verb_pattern = r'\bbukan\s+(me\w+|ber\w+)(?:\s|[.,;!?]|$)'
            for match in re.finditer(active_verb_pattern, text_lower):
                match_start = match.start()
                match_end = match.end()
                phrase = match.group(0).strip()
                phrase_clean = phrase.lower().strip('.,;!? ')
                
                # æ£€æŸ¥ä¾‹å¤–
                if phrase_clean in FIXED_EXPRESSIONS:
                    continue
                if is_contrast_context(text_lower, match_start):
                    continue  # å¯¹æ¯”è¯­å¢ƒï¼Œå…è®¸
                
                errors.append({
                    'phrase': phrase,
                    'reason': f'å¦å®šä¸»åŠ¨åŠ¨è¯ï¼ˆéå¯¹æ¯”è¯­å¢ƒï¼‰åº”ä½¿ç”¨"tidak"',
                    'suggestion': phrase.replace('bukan', 'tidak'),
                })
        
        elif keyword_to_check == 'jangan':
            # æ˜ç¡®é”™è¯¯ï¼šjangan + ä¸»è¯­ä»£è¯
            pronouns = ['saya', 'aku', 'kamu', 'dia', 'kami', 'kita', 'mereka']
            for pronoun in pronouns:
                pattern = r'\bjangan\s+' + re.escape(pronoun) + r'\s+\w+'
                for match in re.finditer(pattern, text_lower):
                    phrase = match.group(0).strip()
                    errors.append({
                        'phrase': phrase,
                        'reason': f'"jangan"æ˜¯å‘½ä»¤å¼ï¼Œä¸èƒ½æœ‰ä¸»è¯­"{pronoun}"',
                        'suggestion': phrase.replace('jangan', 'tidak'),
                    })
        
        return errors
    
    def create_logger(debug):
        def log(msg):
            if debug:
                print(f"[DEBUG] {msg}")
        return log
    
    # ==================== ä¸»é€»è¾‘ ====================
    
    debug = bool(debug) if debug is not None else False
    log = create_logger(debug)
    
    try:
        keyword = str(keyword).strip().lower()
        
        # æ”¯æŒå£è¯­å˜ä½“
        keyword_variants = {
            'tidak': ['tidak', 'gak', 'ga', 'nggak', 'ngga', 'tak'],
            'bukan': ['bukan', 'bkn'],
            'jangan': ['jangan', 'jgn']
        }
        
        if keyword not in ['tidak', 'bukan', 'jangan']:
            return 0, f"âŒ é”™è¯¯ï¼š'{keyword}' ä¸æ˜¯æœ‰æ•ˆçš„å°å°¼è¯­å¦å®šè¯ï¼ˆåº”è¯¥æ˜¯ tidak/bukan/janganï¼‰"
        
        text = normalize_content(content)
        if not text:
            return 0, "âŒ æ–‡ç« å†…å®¹ä¸ºç©º"
        
        text_lower = text.lower()
        
        # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨äº†è¦æ±‚çš„å¦å®šè¯
        variants = keyword_variants.get(keyword, [keyword])
        found_variant = None
        total_count = 0
        
        for variant in variants:
            count = len(re.findall(r'\b' + variant + r'\b', text_lower))
            if count > 0:
                found_variant = variant
                total_count += count
                log(f"æ‰¾åˆ° '{variant}': {count} æ¬¡")
        
        if not found_variant:
            return 0, f"âŒ é”™è¯¯ï¼šæ–‡ç« ä¸­æœªæ‰¾åˆ°å¦å®šè¯ '{keyword}' æˆ–å…¶å˜ä½“ {variants}"
        
        # åªæ£€æŸ¥æ˜ç¡®çš„ã€æ— äº‰è®®çš„é”™è¯¯
        strict_errors = check_strict_errors_only(text, keyword)
        
        if not strict_errors:
            return 1, f"âœ… æ­£ç¡®ï¼š'{found_variant}' ä½¿ç”¨æ­£ç¡®ï¼ˆå…±å‡ºç° {total_count} æ¬¡ï¼Œæ— æ˜ç¡®è¯­æ³•é”™è¯¯ï¼‰"
        else:
            # åªæŠ¥å‘Šé«˜ç½®ä¿¡åº¦çš„é”™è¯¯
            error_details = []
            for i, err in enumerate(strict_errors[:3], 1):  # æœ€å¤šæ˜¾ç¤º3ä¸ª
                detail = f"  {i}. é”™è¯¯çŸ­è¯­ï¼šã€Œ{err['phrase']}ã€\n"
                detail += f"     åŸå› ï¼š{err['reason']}\n"
                detail += f"     å»ºè®®ï¼š{err['suggestion']}"
                error_details.append(detail)
            
            error_summary = "\n".join(error_details)
            
            if len(strict_errors) > 3:
                error_summary += f"\n  ... è¿˜æœ‰ {len(strict_errors) - 3} ä¸ªç±»ä¼¼é”™è¯¯"
            
            return 0, f"âŒ é”™è¯¯ï¼š'{found_variant}' å­˜åœ¨ {len(strict_errors)} å¤„æ˜ç¡®çš„è¯­æ³•é”™è¯¯\n\n{error_summary}"
        
    except Exception as e:
        import traceback
        return 0, f"âŒ å‡½æ•°æ‰§è¡Œå¼‚å¸¸: {str(e)}\n{traceback.format_exc()}"


# ==================== æµ‹è¯•ä»£ç  ====================
if __name__ == "__main__":
    # æµ‹è¯•ç”¨ä¾‹
    test_cases = [
        # åº”è¯¥é€šè¿‡çš„æƒ…å†µ
        {
            'content': 'Aku bukan marah tanpa alasan.',
            'keyword': 'bukan',
            'expected': 1,
            'reason': 'çŸ­è¯­è¯­å¢ƒï¼Œbukan æ­£ç¡®'
        },
        {
            'content': 'Aku hanya membantu, bukan mengganggu.',
            'keyword': 'bukan',
            'expected': 1,
            'reason': 'å¯¹æ¯”è¯­å¢ƒï¼Œbukan æ­£ç¡®'
        },
        {
            'content': 'Aku tidak marah tanpa alasan.',
            'keyword': 'tidak',
            'expected': 1,
            'reason': 'tidak ä¹Ÿæ­£ç¡®'
        },
        
        # åº”è¯¥æŠ¥é”™çš„æƒ…å†µ
        {
            'content': 'Saya tidak guru.',
            'keyword': 'tidak',
            'expected': 0,
            'reason': 'èº«ä»½åè¯å¿…é¡»ç”¨ bukan'
        },
        {
            'content': 'Dia bukan pergi.',
            'keyword': 'bukan',
            'expected': 0,
            'reason': 'å•ä¸ªåŠ¨è¯å¿…é¡»ç”¨ tidak'
        },
        {
            'content': 'Ini bukan besar.',
            'keyword': 'bukan',
            'expected': 0,
            'reason': 'å•ä¸ªå½¢å®¹è¯å¿…é¡»ç”¨ tidak'
        },
    ]
    
    print("=" * 60)
    print("å¦å®šè¯æ£€æµ‹è§„åˆ™æµ‹è¯•")
    print("=" * 60)
    
    for i, test in enumerate(test_cases, 1):
        result, msg = check_indonesian_negation_keyword(
            test['content'], 
            test['keyword'], 
            debug=False
        )
        
        status = "âœ… é€šè¿‡" if result == test['expected'] else "âŒ å¤±è´¥"
        print(f"\næµ‹è¯• {i}: {status}")
        print(f"å†…å®¹: {test['content']}")
        print(f"å…³é”®è¯: {test['keyword']}")
        print(f"æœŸæœ›: {test['expected']}, å®é™…: {result}")
        print(f"åŸå› : {test['reason']}")
        print(f"ç»“æœ: {msg}")


# ==================== 5. satu ä½¿ç”¨è§„èŒƒæ£€æµ‹ï¼ˆå®Œæ•´ç‰ˆ - ä¿®å¤ paket ç¼ºå¤±ï¼‰====================

import re
from typing import Tuple, Optional, Dict
from collections import Counter

def check_se_usage(content, min_count: Optional[int] = None, debug: bool = False, **kwargs) -> Tuple[int, str, Dict]:
    """
    æ£€æµ‹å°å°¼è¯­ä¸­ se + kata pengukur + kata benda å½¢å¼çš„ satuï¼ˆä¸€ï¼‰çš„ä½¿ç”¨
    
    ä¸¤ç§æ¨¡å¼ï¼š
    1. å¦‚æœæŒ‡å®š min_countï¼šè¦æ±‚è‡³å°‘å‡ºç°æŒ‡å®šæ•°é‡
    2. å¦‚æœä¸æŒ‡å®š min_countï¼šåªè¦æœ‰ä½¿ç”¨ä¸”æ— é”™è¯¯å³å¯
    
    æ ¸å¿ƒè§„åˆ™ï¼š
    satuï¼ˆä¸€ï¼‰åœ¨é™¤äº†æ•°æ•°ç­‰å•ç‹¬ä½¿ç”¨ä»¥å¤–çš„æƒ…æ™¯ä¸­å¿…é¡»ç¼©å†™ä¸º se-
    å¦‚ï¼šseorangï¼ˆä¸€ä¸ªäººï¼‰ï¼Œsebuahï¼ˆä¸€ä¸ªç‰©å“ï¼‰ï¼Œsemingguï¼ˆä¸€å‘¨ï¼‰
    
    é”™è¯¯ç”¨æ³•ï¼š
    - âŒ satu orangï¼ˆåº”è¯¥æ˜¯ seorangï¼‰
    - âŒ satu buah rumahï¼ˆåº”è¯¥æ˜¯ sebuah rumahï¼‰
    - âŒ satu mingguï¼ˆåº”è¯¥æ˜¯ semingguï¼‰
    - âŒ satu paketï¼ˆåº”è¯¥æ˜¯ sepaketï¼‰
    
    Args:
        content: è¦æ£€æµ‹çš„æ–‡æœ¬ï¼ˆå¯ä»¥æ˜¯å­—ç¬¦ä¸²æˆ–åˆ—è¡¨ï¼‰
        min_count: æœ€å°‘éœ€è¦å‡ºç°çš„æ¬¡æ•°ï¼ˆNone=åªè¦æœ‰ä¸”æ— é”™è¯¯å³å¯ï¼‰
        debug: æ˜¯å¦è¾“å‡ºè°ƒè¯•ä¿¡æ¯
        **kwargs: å…¶ä»–å‚æ•°ï¼ˆå…¼å®¹æ€§ï¼‰
    
    Returns:
        (1/0, è¯¦ç»†è¯´æ˜, ç»Ÿè®¡æ•°æ®å­—å…¸)
    """
    
    # é»˜è®¤ç»Ÿè®¡æ•°æ®
    default_stats = {
        'total_se_words': 0,
        'correct_count': 0,
        'wrong_satu_usage_count': 0,
        'wrong_format_count': 0,
        'required_count': min_count,
        'passed': False,
        'correct_words': [],
        'wrong_satu_usages': [],
        'wrong_format_words': [],
        'all_se_words': [],
        'check_mode': 'flexible' if min_count is None else 'strict'
    }
    
    # ============ æœ¬åœ°ç±»å‹æ£€æŸ¥å’Œè½¬æ¢ ============
    if content is None:
        return 0, "âŒ é”™è¯¯ï¼šè¾“å…¥æ–‡æœ¬ä¸ºç©ºï¼ˆNoneï¼‰", default_stats
    
    # å¤„ç†åˆ—è¡¨ç±»å‹
    if isinstance(content, list):
        text_parts = []
        for item in content:
            if isinstance(item, str):
                text_parts.append(item)
            elif isinstance(item, dict):
                for key in ['text', 'content', 'message', 'response']:
                    if key in item and isinstance(item[key], str):
                        text_parts.append(item[key])
                        break
            else:
                text_parts.append(str(item))
        text = ' '.join(text_parts)
    else:
        try:
            text = str(content)
        except Exception:
            return 0, f"âŒ é”™è¯¯ï¼šæ— æ³•è½¬æ¢ä¸ºå­—ç¬¦ä¸²ï¼Œç±»å‹: {type(content)}", default_stats
    
    if not text or not text.strip():
        return 0, "âŒ é”™è¯¯ï¼šè¾“å…¥æ–‡æœ¬ä¸ºç©º", default_stats
    
    # ============ 1. é‡è¯æ¨¡å¼ï¼ˆkata pengukurï¼‰============
    CLASSIFIER_PATTERNS = [
        # äººçš„é‡è¯
        r'\bseorang\b',
        
        # ç‰©å“é‡è¯
        r'\bsebuah\b', r'\bsebiji\b', r'\bsebutir\b',
        
        # å½¢çŠ¶é‡è¯
        r'\bsebatang\b', r'\bselembar\b', r'\bsehelai\b', 
        r'\bsepotong\b', r'\bsekeping\b',
        
        # åŠ¨ç‰©é‡è¯
        r'\bseekor\b',
        
        # å®¹å™¨é‡è¯
        r'\bsepiring\b', r'\bsegelas\b', r'\bsekotak\b', 
        r'\bsekantong\b', r'\bsekarung\b', r'\bsebungkus\b', r'\bsebotol\b',
        
        # é›†åˆé‡è¯
        r'\bsepasang\b', r'\bsekelompok\b', r'\bserombongan\b', r'\bsekumpulan\b',
        
        # å•ä½é‡è¯
        r'\bseunit\b', r'\bsepaket\b', r'\bserangkaian\b',
        
        # æ—¶é—´é‡è¯
        r'\bsehari\b', r'\bseminggu\b', r'\bsebulan\b', r'\bsetahun\b',
        r'\bsejam\b', r'\bsemenit\b', r'\bsedetik\b',
        r'\bsesaat\b', r'\bsekejap\b',
    ]
    
    # ============ 2. é”™è¯¯æ¨¡å¼ï¼ˆsatu + é‡è¯ï¼‰- å¿…é¡»ä¸ä¸Šé¢å®Œå…¨å¯¹åº” ============
    WRONG_SATU_PATTERNS = [
        # ç‰©å“å’Œäºº
        ('orang', 'seorang'),
        ('buah', 'sebuah'),
        ('biji', 'sebiji'),
        ('butir', 'sebutir'),
        
        # å½¢çŠ¶
        ('batang', 'sebatang'),
        ('lembar', 'selembar'),
        ('helai', 'sehelai'),
        ('potong', 'sepotong'),
        ('keping', 'sekeping'),
        
        # åŠ¨ç‰©
        ('ekor', 'seekor'),
        
        # å®¹å™¨
        ('piring', 'sepiring'),
        ('gelas', 'segelas'),
        ('kotak', 'sekotak'),
        ('kantong', 'sekantong'),
        ('karung', 'sekarung'),
        ('bungkus', 'sebungkus'),
        ('botol', 'sebotol'),
        
        # é›†åˆ
        ('pasang', 'sepasang'),
        ('kelompok', 'sekelompok'),
        ('rombongan', 'serombongan'),
        ('kumpulan', 'sekumpulan'),
        
        # â­ å•ä½é‡è¯ï¼ˆä¿®å¤ï¼šæ·»åŠ  paketï¼‰
        ('unit', 'seunit'),
        ('paket', 'sepaket'),          # âœ… æ·»åŠ è¿™ä¸ªï¼
        ('rangkaian', 'serangkaian'),
        
        # æ—¶é—´é‡è¯
        ('hari', 'sehari'),
        ('minggu', 'seminggu'),
        ('bulan', 'sebulan'),
        ('tahun', 'setahun'),
        ('jam', 'sejam'),
        ('menit', 'semenit'),
        ('detik', 'sedetik'),
        ('saat', 'sesaat'),
        ('kejap', 'sekejap'),
    ]
    
    # ============ 3. æ ¼å¼é”™è¯¯æ¨¡å¼ ============
    WRONG_FORMAT_PATTERNS = [
        (r'\bse\s+\w+', 'se å’Œåé¢çš„è¯ä¹‹é—´ä¸åº”æœ‰ç©ºæ ¼ï¼ˆåº”è¿å†™ï¼‰'),
        (r'\bse-\w+', 'se å’Œåé¢çš„è¯ä¹‹é—´ä¸åº”æœ‰è¿å­—ç¬¦'),
    ]
    
    # ============ 4. æŸ¥æ‰¾æ‰€æœ‰ç”¨æ³• ============
    found_correct = []
    found_wrong_satu = []
    found_wrong_format = []
    
    text_lower = text.lower()
    
    # 4.1 æ£€æŸ¥é”™è¯¯çš„ satu + é‡è¯
    for classifier, correct_form in WRONG_SATU_PATTERNS:
        pattern = r'\bsatu\s+' + re.escape(classifier) + r'\b'
        for match in re.finditer(pattern, text_lower):
            found_wrong_satu.append({
                'text': match.group(),
                'wrong_form': f'satu {classifier}',
                'correct_form': correct_form,
                'position': match.start(),
                'context': text[max(0, match.start()-25):min(len(text), match.end()+25)]
            })
    
    # 4.2 æ£€æŸ¥æ ¼å¼é”™è¯¯
    for pattern, error_msg in WRONG_FORMAT_PATTERNS:
        for match in re.finditer(pattern, text, re.IGNORECASE):
            # æ’é™¤å·²ç»åœ¨ wrong_satu ä¸­æ£€æµ‹åˆ°çš„
            if not any(ws['position'] == match.start() for ws in found_wrong_satu):
                found_wrong_format.append({
                    'text': match.group(),
                    'position': match.start(),
                    'context': text[max(0, match.start()-25):min(len(text), match.end()+25)],
                    'error': error_msg
                })
    
    # 4.3 æŸ¥æ‰¾æ­£ç¡®çš„ se- é‡è¯å½¢å¼
    for pattern in CLASSIFIER_PATTERNS:
        for match in re.finditer(pattern, text_lower):
            start_pos = match.start()
            end_pos = match.end()
            found_correct.append({
                'word': text[start_pos:end_pos],  # ä¿ç•™åŸå§‹å¤§å°å†™
                'word_lower': match.group(),
                'position': start_pos,
                'context': text[max(0, start_pos-20):min(len(text), end_pos+20)]
            })
    
    # ============ 5. åˆ¤æ–­ç»“æœ ============
    has_no_errors = (len(found_wrong_satu) == 0 and len(found_wrong_format) == 0)
    
    if min_count is None:
        # çµæ´»æ¨¡å¼ï¼šåªè¦æœ‰ä½¿ç”¨ä¸”æ— é”™è¯¯å³å¯
        passed = (len(found_correct) > 0 and has_no_errors)
        check_mode = 'flexible'
    else:
        # ä¸¥æ ¼æ¨¡å¼ï¼šå¿…é¡»è¾¾åˆ°æŒ‡å®šæ•°é‡ä¸”æ— é”™è¯¯
        passed = (len(found_correct) >= min_count and has_no_errors)
        check_mode = 'strict'
    
    # ============ 6. ç”Ÿæˆè¯¦ç»†è¯´æ˜ ============
    detail_parts = []
    
    if passed:
        # ========== æˆåŠŸæƒ…å†µ ==========
        if check_mode == 'flexible':
            detail_parts.append(f"âœ… æ­£ç¡®ï¼šæ‰¾åˆ° {len(found_correct)} ä¸ªæ­£ç¡®çš„ se- é‡è¯å½¢å¼ï¼Œä¸”æ— é”™è¯¯ç”¨æ³•")
        else:
            detail_parts.append(f"âœ… æ­£ç¡®ï¼šæ‰¾åˆ° {len(found_correct)} ä¸ªæ­£ç¡®çš„ se- é‡è¯å½¢å¼ï¼ˆè¦æ±‚è‡³å°‘ {min_count} ä¸ªï¼‰ï¼Œä¸”æ— é”™è¯¯ç”¨æ³•")
        
        if found_correct:
            detail_parts.append("\næ­£ç¡®ä½¿ç”¨çš„ se- é‡è¯å½¢å¼ï¼š")
            word_counter = Counter([item['word_lower'] for item in found_correct])
            
            for i, (word, count) in enumerate(word_counter.most_common(), 1):
                detail_parts.append(f"  {i}. {word} ï¼ˆå‡ºç° {count} æ¬¡ï¼‰")
            
            if debug:
                detail_parts.append("\nè¯¦ç»†ä½ç½®ï¼š")
                for item in found_correct:
                    detail_parts.append(f"  - {item['word']} (ä½ç½® {item['position']})")
    
    else:
        # ========== å¤±è´¥æƒ…å†µ ==========
        
        # ç»Ÿè®¡é—®é¢˜æ•°é‡
        total_issues = len(found_wrong_satu) + len(found_wrong_format)
        if check_mode == 'strict' and min_count is not None and len(found_correct) < min_count:
            total_issues += 1
        elif check_mode == 'flexible' and len(found_correct) == 0:
            total_issues += 1
        
        detail_parts.append(f"âŒ æœªè¾¾åˆ°è¦æ±‚ï¼Œå‘ç° {total_issues} ä¸ªé—®é¢˜ï¼š")
        
        problem_num = 1
        
        # é—®é¢˜1ï¼šé”™è¯¯ä½¿ç”¨ satu + é‡è¯
        if found_wrong_satu:
            detail_parts.append(f"\nã€é—®é¢˜{problem_num}ã€‘é”™è¯¯ç”¨æ³•ï¼šå‘ç° {len(found_wrong_satu)} å¤„é”™è¯¯åœ°ä½¿ç”¨äº† 'satu + é‡è¯'")
            detail_parts.append("æ ¸å¿ƒè§„åˆ™ï¼šsatuï¼ˆä¸€ï¼‰åœ¨ä¸é‡è¯æ­é…æ—¶å¿…é¡»ç¼©å†™ä¸º se-")
            for i, item in enumerate(found_wrong_satu, 1):
                detail_parts.append(f"\n  é”™è¯¯ {i}:")
                detail_parts.append(f"    âŒ é”™è¯¯å†™æ³•: '{item['text']}'")
                detail_parts.append(f"    âœ… æ­£ç¡®å†™æ³•: '{item['correct_form']}'")
                detail_parts.append(f"    ğŸ“ ä½ç½®: å­—ç¬¦ {item['position']}")
                if debug:
                    detail_parts.append(f"    ğŸ“ ä¸Šä¸‹æ–‡: ...{item['context']}...")
            problem_num += 1
        
        # é—®é¢˜2ï¼šæ ¼å¼é”™è¯¯
        if found_wrong_format:
            detail_parts.append(f"\nã€é—®é¢˜{problem_num}ã€‘æ ¼å¼é”™è¯¯ï¼šå‘ç° {len(found_wrong_format)} ä¸ªé”™è¯¯çš„ se æ ¼å¼")
            for i, item in enumerate(found_wrong_format, 1):
                detail_parts.append(f"\n  é”™è¯¯ {i}:")
                detail_parts.append(f"    âŒ é”™è¯¯: '{item['text']}'")
                detail_parts.append(f"    ğŸ“‹ é—®é¢˜: {item['error']}")
                detail_parts.append(f"    ğŸ“ ä½ç½®: å­—ç¬¦ {item['position']}")
                if debug:
                    detail_parts.append(f"    ğŸ“ ä¸Šä¸‹æ–‡: ...{item['context']}...")
            problem_num += 1
        
        # é—®é¢˜3ï¼šæ•°é‡ä¸è¶³
        if check_mode == 'strict' and min_count is not None and len(found_correct) < min_count:
            shortage = min_count - len(found_correct)
            detail_parts.append(f"\nã€é—®é¢˜{problem_num}ã€‘æ•°é‡ä¸è¶³")
            detail_parts.append(f"  è¦æ±‚: è‡³å°‘ {min_count} ä¸ª se- é‡è¯å½¢å¼")
            detail_parts.append(f"  å®é™…: åªæ‰¾åˆ° {len(found_correct)} ä¸ª")
            detail_parts.append(f"  å·®è·: è¿˜å·® {shortage} ä¸ª")
        elif check_mode == 'flexible' and len(found_correct) == 0:
            detail_parts.append(f"\nã€é—®é¢˜{problem_num}ã€‘æœªä½¿ç”¨")
            detail_parts.append(f"  æ–‡ä¸­æ²¡æœ‰ä½¿ç”¨ä»»ä½• se- é‡è¯å½¢å¼")
        
        # æ˜¾ç¤ºå·²æœ‰çš„æ­£ç¡®ç”¨æ³•
        if found_correct:
            detail_parts.append("\n" + "=" * 50)
            detail_parts.append("\nâœ… å·²æ­£ç¡®ä½¿ç”¨çš„ se- é‡è¯å½¢å¼ï¼š")
            word_counter = Counter([item['word_lower'] for item in found_correct])
            
            for i, (word, count) in enumerate(word_counter.most_common(), 1):
                detail_parts.append(f"  {i}. {word} ï¼ˆå‡ºç° {count} æ¬¡ï¼‰")
            
            if debug:
                detail_parts.append("\nè¯¦ç»†ä½ç½®ï¼š")
                for item in found_correct:
                    detail_parts.append(f"  - {item['word']} (ä½ç½® {item['position']})")
    
    detail = '\n'.join(detail_parts)
    
    # ============ 7. æ„å»ºç»Ÿè®¡æ•°æ® ============
    stats = {
        'total_se_words': len(found_correct),
        'correct_count': len(found_correct),
        'wrong_satu_usage_count': len(found_wrong_satu),
        'wrong_format_count': len(found_wrong_format),
        'required_count': min_count,
        'passed': passed,
        'correct_words': [item['word'] for item in found_correct],
        'wrong_satu_usages': [item['text'] for item in found_wrong_satu],
        'wrong_format_words': [item['text'] for item in found_wrong_format],
        'all_se_words': [item['word'] for item in found_correct],
        'check_mode': check_mode
    }
    
    return (1 if passed else 0), detail, stats



# ==================== 6. å°å°¼è¯­ä¸»åŠ¨è¯­æ€æ£€æµ‹ï¼ˆä»… me- å‰ç¼€ç‰ˆ - å·²ä¿®æ­£ï¼‰====================

import re
from typing import Tuple, Dict, List

def check_active_voice(text: str, exact_count: int = 5, debug: bool = False) -> Tuple[bool, str, Dict]:
    """
    æ£€æµ‹å°å°¼è¯­ä¸»åŠ¨è¯­æ€åŠ¨è¯ï¼ˆä»… me- å‰ç¼€åŠå…¶å˜ä½“ï¼‰
    
    â­ æ ¸å¿ƒè§„åˆ™ï¼ˆ2024ç‰ˆ - å·²ä¿®æ­£ï¼‰ï¼š
    1. åªè®¡ç®— me-/mem-/men-/meng-/meny-/menge-/memper- å¼€å¤´çš„è¯
    2. æ’é™¤æ˜ç¡®çš„éåŠ¨è¯ï¼ˆåè¯ã€ä»‹è¯ã€å‰¯è¯ç­‰ï¼‰
    3. â­ åŒ…æ‹¬ç³»åŠ¨è¯ï¼ˆmerupakan, menjadiï¼‰- å®ƒä»¬ä¹Ÿæ˜¯ä¸»åŠ¨å½¢å¼
    4. â­ ber- å‰ç¼€çš„è¯å®Œå…¨ä¸è®¡å…¥ï¼ˆæ ¹æ®é¢˜ç›®è¦æ±‚ï¼‰
    
    Args:
        text: è¦æ£€æµ‹çš„æ–‡æœ¬
        exact_count: è¦æ±‚çš„ç²¾ç¡®ä¸»åŠ¨è¯­æ€åŠ¨è¯æ•°é‡
        debug: æ˜¯å¦è¾“å‡ºè°ƒè¯•ä¿¡æ¯
    
    Returns:
        (æ˜¯å¦é€šè¿‡, è¯¦ç»†è¯´æ˜, ç»Ÿè®¡æ•°æ®)
    """
    
    # ============ ç±»å‹æ£€æŸ¥å’Œè½¬æ¢ ============
    if text is None:
        return False, "âŒ é”™è¯¯ï¼šè¾“å…¥æ–‡æœ¬ä¸ºç©ºï¼ˆNoneï¼‰", {
            'total_active': 0,
            'all_verbs': [],
            'required_count': exact_count,
            'passed': False
        }
    
    # å¤„ç†åˆ—è¡¨ç±»å‹
    if isinstance(text, list):
        text_parts = []
        for item in text:
            if isinstance(item, str):
                text_parts.append(item)
            elif isinstance(item, dict):
                for key in ['text', 'content', 'message', 'response']:
                    if key in item and isinstance(item[key], str):
                        text_parts.append(item[key])
                        break
            else:
                text_parts.append(str(item))
        text = ' '.join(text_parts)
    
    if not isinstance(text, str):
        try:
            text = str(text)
        except Exception as e:
            return False, f"âŒ é”™è¯¯ï¼šæ— æ³•è½¬æ¢ä¸ºå­—ç¬¦ä¸²ï¼Œç±»å‹: {type(text)}", {
                'total_active': 0,
                'all_verbs': [],
                'required_count': exact_count,
                'passed': False
            }
    
    if not text.strip():
        return False, "âŒ é”™è¯¯ï¼šè¾“å…¥æ–‡æœ¬ä¸ºç©º", {
            'total_active': 0,
            'all_verbs': [],
            'required_count': exact_count,
            'passed': False
        }
    
    text_lower = text.lower()
    
    # ============ â­ me- å¼€å¤´ä½†ä¸æ˜¯åŠ¨è¯çš„æ’é™¤è¯æ±‡ ============
    me_non_verbs = {
        # ä»£è¯
        'mereka',
        
        # åè¯
        'media', 'meja', 'merah', 'metal', 'meter',
        'memori', 'menu', 'menteri', 'mesin', 'merek',
        'metode', 'medan',
        
        # å‰¯è¯/è¿è¯
        'memang', 'melainkan',
        
        # ä»‹è¯
        'melalui', 'menuju', 'menurut', 'mengenai', 'menjelang',
        'melampaui',
        
        # å½¢å®¹è¯
        'medis', 'mekanik',
        
        # âš ï¸ æ³¨æ„ï¼šä¸åŒ…æ‹¬ç³»åŠ¨è¯ï¼
        # 'menjadi' å’Œ 'merupakan' è™½ç„¶æ˜¯ç³»åŠ¨è¯ï¼Œ
        # ä½†å®ƒä»¬ä»ç„¶æ˜¯ä¸»åŠ¨è¯­æ€å½¢å¼ï¼Œæ‰€ä»¥ä¸æ’é™¤
    }
    
    # ============ æŸ¥æ‰¾æ‰€æœ‰ me- ä¸»åŠ¨åŠ¨è¯ ============
    all_active_verbs = []
    found_positions = set()
    excluded_verbs = []
    ber_verbs_found = []  # ç”¨äºè°ƒè¯•ï¼šè®°å½•è¢«æ’é™¤çš„ ber- è¯
    
    # 1. memper- å‰ç¼€ï¼ˆä¼˜å…ˆçº§æœ€é«˜ï¼‰
    memper_pattern = r'\bmemper[a-z]{2,}\b'
    for match in re.finditer(memper_pattern, text_lower):
        verb_lower = match.group()
        start_pos = match.start()
        end_pos = match.end()
        
        if start_pos not in found_positions:
            all_active_verbs.append({
                'verb': text[start_pos:end_pos],
                'verb_lower': verb_lower,
                'position': start_pos,
                'context': text[max(0, start_pos-30):min(len(text), end_pos+30)],
                'type': 'memper-',
                'reason': 'ä½¿å½¹åŠ¨è¯å‰ç¼€'
            })
            found_positions.add(start_pos)
    
    # 2. me- ç³»åˆ—å‰ç¼€
    me_pattern = r'\bme(?:m|n|ng|ny|nge)?[a-z]{2,}\b'
    for match in re.finditer(me_pattern, text_lower):
        verb_lower = match.group()
        start_pos = match.start()
        end_pos = match.end()
        
        if start_pos in found_positions:
            continue
            
        # æ’é™¤æ˜ç¡®çš„éåŠ¨è¯
        if verb_lower in me_non_verbs:
            if debug:
                excluded_verbs.append({
                    'verb': text[start_pos:end_pos],
                    'reason': f'éåŠ¨è¯ - {get_word_type_me(verb_lower)}',
                    'position': start_pos
                })
            continue
            
        # æ’é™¤é‡å¤çš„ memper-
        if verb_lower.startswith('memper'):
            if debug:
                excluded_verbs.append({
                    'verb': text[start_pos:end_pos],
                    'reason': 'memper- å·²åœ¨å‰é¢åŒ¹é…',
                    'position': start_pos
                })
            continue
        
        # â­ ä¿ç•™æ‰€æœ‰å…¶ä»– me- è¯ï¼ˆåŒ…æ‹¬ç³»åŠ¨è¯ï¼‰
        verb_type = 'me-'
        reason = 'ä¸»åŠ¨æ€å‰ç¼€ (me-)'
        
        # ç‰¹æ®Šæ ‡æ³¨ç³»åŠ¨è¯
        if verb_lower in {'menjadi', 'merupakan'}:
            reason = 'ä¸»åŠ¨æ€å‰ç¼€ (me-) - ç³»åŠ¨è¯'
        
        all_active_verbs.append({
            'verb': text[start_pos:end_pos],
            'verb_lower': verb_lower,
            'position': start_pos,
            'context': text[max(0, start_pos-30):min(len(text), end_pos+30)],
            'type': verb_type,
            'reason': reason
        })
        found_positions.add(start_pos)
    
    # â­ 3. æ£€æµ‹ ber- è¯æ±‡ï¼ˆä»…ç”¨äºè°ƒè¯•ï¼Œä¸è®¡å…¥ç»“æœï¼‰
    if debug:
        ber_pattern = r'\b(ber|bel|be)[a-z]{2,}\b'
        for match in re.finditer(ber_pattern, text_lower):
            verb_lower = match.group()
            start_pos = match.start()
            end_pos = match.end()
            
            # æ’é™¤æ˜æ˜¾çš„éåŠ¨è¯
            ber_non_verbs = {
                'beras', 'berita', 'belakang', 'besok', 'berat', 'besar',
                'berani', 'benar', 'begitu', 'belum', 'berdasarkan'
            }
            
            if verb_lower not in ber_non_verbs:
                ber_verbs_found.append({
                    'verb': text[start_pos:end_pos],
                    'position': start_pos,
                    'reason': 'æ ¹æ®é¢˜ç›®è¦æ±‚ä¸è®¡å…¥ï¼ˆä»…è®¡ç®— me- å‰ç¼€ï¼‰'
                })
    
    all_active_verbs.sort(key=lambda x: x['position'])
    
    # ============ åˆ¤æ–­ç»“æœ ============
    total_count = len(all_active_verbs)
    passed = (total_count == exact_count)
    
    # ============ ç”Ÿæˆè¯¦ç»†è¯´æ˜ ============
    detail_parts = []
    
    detail_parts.append("â­ æ£€æµ‹èŒƒå›´ï¼šä»…è®¡ç®— me- å‰ç¼€åŠå…¶å˜ä½“ï¼ˆme-/mem-/men-/meng-/meny-/menge-/memper-ï¼‰")
    detail_parts.append("â­ åŒ…æ‹¬ï¼šç³»åŠ¨è¯ï¼ˆmerupakan, menjadiï¼‰ä¹Ÿç®—ä½œä¸»åŠ¨å½¢å¼")
    detail_parts.append("â­ ä¸åŒ…æ‹¬ï¼šber- å‰ç¼€çš„è¯ï¼ˆæ ¹æ®é¢˜ç›®è¦æ±‚ï¼‰\n")
    
    if passed:
        detail_parts.append(f"âœ… æ­£ç¡®ï¼šæ‰¾åˆ°æ­£å¥½ {total_count} ä¸ªä¸»åŠ¨è¯­æ€åŠ¨è¯ï¼ˆè¦æ±‚æ­£å¥½ {exact_count} ä¸ªï¼‰\n")
    else:
        if total_count < exact_count:
            shortage = exact_count - total_count
            detail_parts.append(f"âŒ é”™è¯¯ï¼šåªæ‰¾åˆ° {total_count} ä¸ªä¸»åŠ¨è¯­æ€åŠ¨è¯ï¼Œå°‘äºè¦æ±‚çš„ {exact_count} ä¸ªï¼ˆè¿˜å·® {shortage} ä¸ªï¼‰\n")
        else:
            excess = total_count - exact_count
            detail_parts.append(f"âŒ é”™è¯¯ï¼šæ‰¾åˆ° {total_count} ä¸ªä¸»åŠ¨è¯­æ€åŠ¨è¯ï¼Œè¶…è¿‡è¦æ±‚çš„ {exact_count} ä¸ªï¼ˆå¤šäº† {excess} ä¸ªï¼‰\n")
    
    detail_parts.append(f"ğŸ“Š æ£€æµ‹åˆ°çš„ä¸»åŠ¨è¯­æ€åŠ¨è¯ï¼ˆå…± {total_count} ä¸ªï¼‰ï¼š")
    for i, item in enumerate(all_active_verbs, 1):
        # æ ‡æ³¨ç³»åŠ¨è¯
        marker = " ğŸ”¹ç³»åŠ¨è¯" if item['verb_lower'] in {'menjadi', 'merupakan'} else ""
        detail_parts.append(f"  {i:2d}. {item['verb']:20s} [{item['type']:8s}] (ä½ç½® {item['position']:4d}){marker}")
        if debug:
            detail_parts.append(f"      ä¸Šä¸‹æ–‡: ...{item['context']}...")
    
    if debug and excluded_verbs:
        detail_parts.append(f"\nğŸ” è°ƒè¯•ä¿¡æ¯ - è¢«æ’é™¤çš„ me- è¯ï¼ˆå…± {len(excluded_verbs)} ä¸ªï¼‰ï¼š")
        for i, item in enumerate(excluded_verbs, 1):
            detail_parts.append(f"  {i:2d}. {item['verb']:20s} (ä½ç½® {item['position']:4d})")
            detail_parts.append(f"      åŸå› : {item['reason']}")
    
    if debug and ber_verbs_found:
        detail_parts.append(f"\nğŸ” è°ƒè¯•ä¿¡æ¯ - æ£€æµ‹åˆ°ä½†æœªè®¡å…¥çš„ ber- è¯ï¼ˆå…± {len(ber_verbs_found)} ä¸ªï¼‰ï¼š")
        for i, item in enumerate(ber_verbs_found, 1):
            detail_parts.append(f"  {i:2d}. {item['verb']:20s} (ä½ç½® {item['position']:4d})")
            detail_parts.append(f"      åŸå› : {item['reason']}")
    
    detail = '\n'.join(detail_parts)
    
    # ============ ç»Ÿè®¡æ•°æ® ============
    stats = {
        'total_active': total_count,
        'all_verbs': [v['verb'] for v in all_active_verbs],
        'all_verbs_with_type': [(v['verb'], v['type']) for v in all_active_verbs],
        'all_verbs_detailed': [
            {
                'verb': v['verb'],
                'type': v['type'],
                'position': v['position'],
                'reason': v['reason']
            } for v in all_active_verbs
        ],
        'required_count': exact_count,
        'difference': total_count - exact_count,
        'passed': passed,
        'excluded_count': len(excluded_verbs),
        'excluded_verbs': [v['verb'] for v in excluded_verbs] if debug else [],
        'ber_verbs_found': len(ber_verbs_found),
        'ber_verbs_list': [v['verb'] for v in ber_verbs_found] if debug else [],
        'check_mode': 'me_prefix_only_include_copula',
        'copula_verbs': [v['verb'] for v in all_active_verbs if v['verb_lower'] in {'menjadi', 'merupakan'}]
    }
    
    return passed, detail, stats


def get_word_type_me(word: str) -> str:
    """
    è¾…åŠ©å‡½æ•°ï¼šè¿”å› me- å¼€å¤´è¯çš„ç±»å‹
    
    Args:
        word: è¦æ£€æŸ¥çš„è¯
    
    Returns:
        è¯çš„ç±»å‹ï¼ˆä¸­æ–‡è¯´æ˜ï¼‰
    """
    if word in {'menjadi', 'merupakan'}:
        return 'ç³»åŠ¨è¯ï¼ˆä½†ä»ç®—ä¸»åŠ¨å½¢å¼ï¼‰'
    elif word in {'mereka'}:
        return 'ä»£è¯'
    elif word in {'media', 'meja', 'metal', 'meter', 'memori', 'menu', 'menteri', 'mesin', 'merek', 'metode', 'medan'}:
        return 'åè¯'
    elif word in {'memang', 'melainkan'}:
        return 'å‰¯è¯/è¿è¯'
    elif word in {'melalui', 'menuju', 'menurut', 'mengenai', 'menjelang', 'melampaui'}:
        return 'ä»‹è¯'
    elif word in {'medis', 'mekanik'}:
        return 'å½¢å®¹è¯'
    else:
        return 'éåŠ¨è¯'


# ==================== æµ‹è¯•ä»£ç  ====================
if __name__ == "__main__":
    # æµ‹è¯•æ–‡æœ¬ï¼ˆåŒ…å« menjadiï¼‰
    test_text = [
        "Nusantara Tech menggelar acara peluncuran ponsel lipat Garuda X di Jakarta Convention Center pada 15 Januari 2024. Acara ini menarik perhatian banyak penggemar teknologi dan media lokal. Garuda X merupakan ponsel lipat 5G pertama buatan Indonesia, hasil kerja sama teknologi dengan Samsung Korea. Ponsel ini menawarkan desain inovatif dan fitur canggih yang diharapkan dapat bersaing di pasar global. Nusantara Tech berkomitmen untuk mengembangkan produk berkualitas tinggi yang memenuhi kebutuhan konsumen modern. Peluncuran ini menandai langkah besar bagi industri teknologi Indonesia, yang semakin berani bersaing di kancah internasional.",
        "Pada acara tersebut, CEO Nusantara Tech, Budi Santoso, menyampaikan rasa bangganya atas pencapaian ini. \"Kami berusaha keras untuk menghadirkan produk yang dapat dibanggakan oleh masyarakat Indonesia,\" ujar Budi. Garuda X dilengkapi dengan layar AMOLED fleksibel yang dapat dilipat, prosesor terbaru, dan kamera berkualitas tinggi. Fitur-fitur ini dirancang untuk memberikan pengalaman pengguna yang optimal. Selain itu, ponsel ini mendukung jaringan 5G, memungkinkan pengguna menikmati kecepatan internet yang lebih tinggi. Nusantara Tech juga berencana untuk memperluas pasar ke negara-negara Asia Tenggara lainnya, dengan harapan dapat meningkatkan ekspor produk teknologi Indonesia.",
        "Para pengunjung acara peluncuran berkesempatan mencoba langsung Garuda X dan memberikan tanggapan positif. Banyak yang terkesan dengan desain elegan dan performa ponsel ini. Nusantara Tech berharap dapat meningkatkan penjualan melalui strategi pemasaran yang efektif. Produk ini akan tersedia di toko-toko resmi dan platform e-commerce mulai bulan depan. Dengan peluncuran Garuda X, Nusantara Tech menunjukkan bahwa Indonesia mampu berinovasi dan bersaing di industri teknologi global. Dukungan dari Samsung Korea juga memperkuat posisi Nusantara Tech sebagai pemain utama dalam pengembangan teknologi di Indonesia. Para analis memprediksi produk ini akan menjadi salah satu unggulan."
    ]
    
    print("=" * 100)
    print("å°å°¼è¯­ä¸»åŠ¨è¯­æ€æ£€æµ‹ - ä»… me- å‰ç¼€ç‰ˆæµ‹è¯•ï¼ˆå·²ä¿®æ­£ - åŒ…å«ç³»åŠ¨è¯ï¼‰")
    print("=" * 100)
    
    # æµ‹è¯•ï¼šç»Ÿè®¡æ–‡æœ¬ä¸­çš„ me- ä¸»åŠ¨åŠ¨è¯
    print("\nã€æµ‹è¯•ï¼šç»Ÿè®¡æ–‡æœ¬ä¸­çš„ me- ä¸»åŠ¨è¯­æ€åŠ¨è¯æ•°é‡ã€‘")
    passed, detail, stats = check_active_voice(test_text, exact_count=999, debug=True)
    print(detail)
    print(f"\nğŸ’¡ ç»“æœï¼šæ–‡æœ¬ä¸­å®é™…åŒ…å« {stats['total_active']} ä¸ª me- ä¸»åŠ¨è¯­æ€åŠ¨è¯")
    
    if stats.get('copula_verbs'):
        print(f"ğŸ’¡ å…¶ä¸­åŒ…å«ç³»åŠ¨è¯ï¼š{', '.join(stats['copula_verbs'])}")
    
    if stats['ber_verbs_found'] > 0:
        print(f"ğŸ’¡ è¡¥å……ï¼šæ£€æµ‹åˆ° {stats['ber_verbs_found']} ä¸ª ber- è¯æ±‡ï¼ˆæœªè®¡å…¥ç»“æœï¼‰")
    
    print("\n" + "=" * 100)
    print("ğŸ“Œ ç»“è®ºï¼š")
    print(f"   - ä»…è®¡ç®— me- å‰ç¼€ï¼ˆåŒ…æ‹¬ç³»åŠ¨è¯ï¼‰ï¼Œæ–‡æœ¬åŒ…å« {stats['total_active']} ä¸ªä¸»åŠ¨è¯­æ€åŠ¨è¯")
    print(f"   - rule åç§°ä¿æŒä¸º: check_active_voice:###æ•°é‡1###")
    print("=" * 100)


# ==================== 7. å°å°¼è¯­è¢«åŠ¨è¯­æ€æ£€æµ‹ï¼ˆç²¾ç¡®æ•°é‡ç‰ˆ - ä»…è¯†åˆ« di- è¯å¤´ - å·²ä¿®æ­£ï¼‰====================

import re
from typing import Tuple, Dict, List

def check_passive_voice(text: str, exact_count: int = 8, debug: bool = False) -> Tuple[bool, str, Dict]:
    """
    æ£€æµ‹å°å°¼è¯­è¢«åŠ¨è¯­æ€åŠ¨è¯ï¼ˆä»… di- å‰ç¼€è¿å†™è¯ï¼‰
    
    â­ æ ¸å¿ƒè§„åˆ™ï¼ˆ2024ç‰ˆ - å·²ä¿®æ­£ï¼‰ï¼š
    1. åªè®¡ç®— di- å¼€å¤´çš„è¿å†™è¯ï¼ˆå¦‚ dikembangkan, dilakukanï¼‰
    2. æ’é™¤æ˜ç¡®çš„éåŠ¨è¯ï¼ˆåè¯ã€å½¢å®¹è¯ã€å¤–æ¥è¯ï¼‰
    3. â­ ä¸è®¡ç®— "di + ç©ºæ ¼ + è¯" çš„ä»‹è¯ç»“æ„ï¼ˆå¦‚ "di Jakarta"ï¼‰
    4. â­ æ’é™¤ distribusi ç­‰å¤–æ¥åè¯
    
    Args:
        text: è¦æ£€æµ‹çš„æ–‡æœ¬
        exact_count: è¦æ±‚çš„ç²¾ç¡®è¢«åŠ¨è¯­æ€åŠ¨è¯æ•°é‡
        debug: æ˜¯å¦è¾“å‡ºè°ƒè¯•ä¿¡æ¯
    
    Returns:
        (æ˜¯å¦é€šè¿‡, è¯¦ç»†è¯´æ˜, ç»Ÿè®¡æ•°æ®)
    """
    
    # ============ ç±»å‹æ£€æŸ¥å’Œè½¬æ¢ ============
    if text is None:
        return False, "âŒ é”™è¯¯ï¼šè¾“å…¥æ–‡æœ¬ä¸ºç©ºï¼ˆNoneï¼‰", {
            'total_passive': 0,
            'passive_verbs': [],
            'required_count': exact_count,
            'passed': False
        }
    
    if isinstance(text, list):
        text_parts = []
        for item in text:
            if isinstance(item, str):
                text_parts.append(item)
            elif isinstance(item, dict):
                for key in ['text', 'content', 'message', 'response']:
                    if key in item and isinstance(item[key], str):
                        text_parts.append(item[key])
                        break
            else:
                text_parts.append(str(item))
        text = ' '.join(text_parts)
    
    if not isinstance(text, str):
        try:
            text = str(text)
        except Exception as e:
            return False, f"âŒ é”™è¯¯ï¼šæ— æ³•è½¬æ¢ä¸ºå­—ç¬¦ä¸²ï¼Œç±»å‹: {type(text)}", {
                'total_passive': 0,
                'passive_verbs': [],
                'required_count': exact_count,
                'passed': False
            }
    
    if not text.strip():
        return False, "âŒ é”™è¯¯ï¼šè¾“å…¥æ–‡æœ¬ä¸ºç©º", {
            'total_passive': 0,
            'passive_verbs': [],
            'required_count': exact_count,
            'passed': False
        }
    
    text_lower = text.lower()
    
    # ============ â­ æ’é™¤è¯æ±‡ï¼ˆä»…æ˜ç¡®çš„éåŠ¨è¯ï¼‰============
    
    non_passive_di_words = {
        # ========== å¤–æ¥è¯ï¼ˆA-Dï¼‰==========
        'dialog', 'diameter', 'diagnosa', 'diploma', 'dinosaurus',
        'direksi', 'direktur', 'dinas', 'dinasti', 'diskon',
        'diskusi', 'divisi', 'diet', 'diesel', 'dinamis',
        'digital', 'dinamika', 'diagram', 'diare',
        
        # â­ æ–°å¢ï¼šdistribution ç›¸å…³ï¼ˆå¸¸è§é”™è¯¯ï¼‰
        'distribusi',      # åˆ†é…/åˆ†å‘ï¼ˆåè¯ï¼‰
        'distributor',     # åˆ†é”€å•†ï¼ˆåè¯ï¼‰
        'distrik',         # åŒºåŸŸï¼ˆåè¯ï¼‰
        
        # ========== å…¶ä»–å¤–æ¥è¯ ==========
        'dimensi',         # dimension
        'diplomasi',       # diplomacy
        'direktif',        # directive
        'disiplin',        # discipline
        'diversitas',      # diversity
        'dilemma',         # dilemma
        'dikotomi',        # dichotomy
        'divestasi',       # divestment
        'diskriminasi',    # discrimination
        'dispensasi',      # dispensation
        
        # ========== å°å°¼è¯­åŸç”Ÿè¯ï¼ˆåè¯ã€å½¢å®¹è¯ï¼‰==========
        'diri',            # è‡ªå·±
        'dinding',         # å¢™
        'dingin',          # å†·
        'diam',            # å®‰é™
    }
    
    # ============ æŸ¥æ‰¾è¢«åŠ¨åŠ¨è¯ï¼ˆä»…è¿å†™è¯ï¼‰============
    
    passive_verbs_list = []
    excluded_words = []
    di_space_phrases = []  # ç”¨äºè°ƒè¯•ï¼šè®°å½• "di + ç©ºæ ¼" çš„ä»‹è¯çŸ­è¯­
    found_positions = set()
    
    # â­ åªåŒ¹é… di- è¿å†™è¯ï¼ˆä¸åŒ…æ‹¬ "di + ç©ºæ ¼"ï¼‰
    di_connected_pattern = r'\bdi([a-z]{2,})\b'
    
    for match in re.finditer(di_connected_pattern, text_lower):
        full_word_lower = match.group(0)  # å®Œæ•´çš„è¯ï¼ˆå¦‚ dikembangkanï¼‰
        root = match.group(1)              # è¯æ ¹ï¼ˆå¦‚ kembangkanï¼‰
        start_pos = match.start()
        end_pos = match.end()
        original_word = text[start_pos:end_pos]  # ä¿ç•™åŸå§‹å¤§å°å†™
        
        if start_pos in found_positions:
            continue
        
        # æ’é™¤æ˜ç¡®çš„éåŠ¨è¯
        if full_word_lower in non_passive_di_words:
            if debug:
                excluded_words.append({
                    'word': original_word,
                    'type': get_word_type_di(full_word_lower),
                    'position': start_pos
                })
            found_positions.add(start_pos)
            continue
        
        # â­ ä¿ç•™ä¸ºè¢«åŠ¨è¯­æ€åŠ¨è¯
        passive_verbs_list.append({
            'verb': original_word,
            'verb_lower': full_word_lower,
            'root': root,
            'position': start_pos,
            'context': text[max(0, start_pos-30):min(len(text), end_pos+30)]
        })
        found_positions.add(start_pos)
    
    # â­ æ£€æµ‹ "di + ç©ºæ ¼ + è¯" çš„ä»‹è¯çŸ­è¯­ï¼ˆä»…ç”¨äºè°ƒè¯•ï¼Œä¸è®¡å…¥ç»“æœï¼‰
    if debug:
        di_separated_pattern = r'\bdi\s+([a-z]+)\b'
        for match in re.finditer(di_separated_pattern, text_lower):
            location = match.group(1)
            start_pos = match.start()
            end_pos = match.end()
            original_phrase = text[start_pos:end_pos]
            
            di_space_phrases.append({
                'phrase': original_phrase,
                'word_after_di': location,
                'position': start_pos,
                'reason': 'ä»‹è¯çŸ­è¯­ï¼ˆä¸è®¡å…¥è¢«åŠ¨è¯­æ€ï¼‰'
            })
    
    passive_verbs_list.sort(key=lambda x: x['position'])
    
    # ============ åˆ¤æ–­ç»“æœ ============
    total_count = len(passive_verbs_list)
    passed = (total_count == exact_count)
    
    # ============ ç”Ÿæˆè¯¦ç»†è¯´æ˜ ============
    detail_parts = []
    
    detail_parts.append("â­ æ£€æµ‹èŒƒå›´ï¼šä»…è®¡ç®— di- å‰ç¼€çš„è¿å†™è¯ï¼ˆå¦‚ dikembangkan, dilakukanï¼‰")
    detail_parts.append("â­ ä¸åŒ…æ‹¬ï¼š'di + ç©ºæ ¼ + è¯' çš„ä»‹è¯çŸ­è¯­ï¼ˆå¦‚ 'di Jakarta'ï¼‰")
    detail_parts.append("â­ ä¸åŒ…æ‹¬ï¼šå¤–æ¥åè¯ï¼ˆå¦‚ distribusi, dialogï¼‰\n")
    
    if passed:
        detail_parts.append(f"âœ… æ­£ç¡®ï¼šæ‰¾åˆ°æ­£å¥½ {total_count} ä¸ªè¢«åŠ¨è¯­æ€åŠ¨è¯ï¼ˆè¦æ±‚æ­£å¥½ {exact_count} ä¸ªï¼‰\n")
    else:
        if total_count < exact_count:
            shortage = exact_count - total_count
            detail_parts.append(f"âŒ é”™è¯¯ï¼šåªæ‰¾åˆ° {total_count} ä¸ªè¢«åŠ¨è¯­æ€åŠ¨è¯ï¼Œå°‘äºè¦æ±‚çš„ {exact_count} ä¸ªï¼ˆè¿˜å·® {shortage} ä¸ªï¼‰\n")
        else:
            excess = total_count - exact_count
            detail_parts.append(f"âŒ é”™è¯¯ï¼šæ‰¾åˆ° {total_count} ä¸ªè¢«åŠ¨è¯­æ€åŠ¨è¯ï¼Œè¶…è¿‡è¦æ±‚çš„ {exact_count} ä¸ªï¼ˆå¤šäº† {excess} ä¸ªï¼‰\n")
    
    detail_parts.append(f"ğŸ“Š æ£€æµ‹åˆ°çš„è¢«åŠ¨è¯­æ€åŠ¨è¯ï¼ˆå…± {total_count} ä¸ªï¼‰ï¼š")
    for i, item in enumerate(passive_verbs_list, 1):
        detail_parts.append(f"  {i:2d}. {item['verb']:20s} (è¯æ ¹: {item['root']:15s}, ä½ç½® {item['position']:4d})")
        if debug:
            detail_parts.append(f"      ä¸Šä¸‹æ–‡: ...{item['context']}...")
    
    if debug and excluded_words:
        detail_parts.append(f"\nğŸ” è°ƒè¯•ä¿¡æ¯ - è¢«æ’é™¤çš„ di- è¯ï¼ˆå…± {len(excluded_words)} ä¸ªï¼‰ï¼š")
        for i, item in enumerate(excluded_words, 1):
            detail_parts.append(f"  {i:2d}. {item['word']:20s} (ä½ç½® {item['position']:4d})")
            detail_parts.append(f"      åŸå› : {item['type']}")
    
    if debug and di_space_phrases:
        detail_parts.append(f"\nğŸ” è°ƒè¯•ä¿¡æ¯ - æ£€æµ‹åˆ°ä½†æœªè®¡å…¥çš„ 'di + ç©ºæ ¼' çŸ­è¯­ï¼ˆå…± {len(di_space_phrases)} ä¸ªï¼‰ï¼š")
        for i, item in enumerate(di_space_phrases[:10], 1):  # æœ€å¤šæ˜¾ç¤º10ä¸ª
            detail_parts.append(f"  {i:2d}. '{item['phrase']:20s}' (ä½ç½® {item['position']:4d})")
            detail_parts.append(f"      åŸå› : {item['reason']}")
    
    detail = '\n'.join(detail_parts)
    
    # ============ ç»Ÿè®¡æ•°æ® ============
    stats = {
        'total_passive': total_count,
        'passive_verbs': [v['verb'] for v in passive_verbs_list],
        'passive_verbs_detailed': [
            {
                'verb': v['verb'],
                'root': v['root'],
                'position': v['position']
            } for v in passive_verbs_list
        ],
        'passive_roots': [v['root'] for v in passive_verbs_list],
        'excluded_count': len(excluded_words),
        'excluded_words': [e['word'] for e in excluded_words] if debug else [],
        'di_space_phrases_count': len(di_space_phrases),
        'di_space_phrases': [p['phrase'] for p in di_space_phrases] if debug else [],
        'required_count': exact_count,
        'difference': total_count - exact_count,
        'passed': passed,
        'check_mode': 'di_prefix_connected_only_fixed'
    }
    
    return passed, detail, stats


def get_word_type_di(word: str) -> str:
    """
    è¾…åŠ©å‡½æ•°ï¼šè¿”å› di- å¼€å¤´è¯çš„ç±»å‹
    
    Args:
        word: è¦æ£€æŸ¥çš„è¯
    
    Returns:
        è¯çš„ç±»å‹ï¼ˆä¸­æ–‡è¯´æ˜ï¼‰
    """
    # å¤–æ¥åè¯
    foreign_nouns = {
        'distribusi', 'distributor', 'distrik', 'dialog', 'diameter',
        'diagnosa', 'diploma', 'dinosaurus', 'direksi', 'direktur',
        'dinas', 'dinasti', 'diskon', 'diskusi', 'divisi',
        'diet', 'diesel', 'diagram', 'diare', 'dimensi',
        'diplomasi', 'direktif', 'disiplin', 'diversitas', 'dilemma',
        'dikotomi', 'divestasi', 'diskriminasi', 'dispensasi'
    }
    
    # å°å°¼è¯­åŸç”Ÿåè¯/å½¢å®¹è¯
    native_words = {
        'diri': 'åè¯ï¼ˆè‡ªå·±ï¼‰',
        'dinding': 'åè¯ï¼ˆå¢™ï¼‰',
        'dingin': 'å½¢å®¹è¯ï¼ˆå†·ï¼‰',
        'diam': 'å½¢å®¹è¯ï¼ˆå®‰é™ï¼‰'
    }
    
    if word in foreign_nouns:
        return f'å¤–æ¥åè¯ï¼ˆ{word}ï¼‰'
    elif word in native_words:
        return native_words[word]
    else:
        return 'éè¢«åŠ¨åŠ¨è¯'


# ==================== æµ‹è¯•ä»£ç  ====================
if __name__ == "__main__":
    # æµ‹è¯•æ–‡æœ¬
    test_text = [
        "Nusantara Tech menggelar acara peluncuran ponsel lipat Garuda X di Jakarta Convention Center pada 15 Januari 2024. Acara ini menarik perhatian banyak penggemar teknologi dan media lokal. Garuda X merupakan ponsel lipat 5G pertama buatan Indonesia, hasil kerja sama teknologi dengan Samsung Korea. Ponsel ini menawarkan desain inovatif dan fitur canggih yang diharapkan dapat bersaing di pasar global. Para pengunjung acara berkesempatan mencoba langsung ponsel tersebut dan mengagumi kualitas layar serta kecepatan koneksi internetnya.",
        "Dalam sambutannya, CEO Nusantara Tech, Budi Santoso, menyampaikan rasa bangga atas pencapaian ini. \"Kami berkomitmen mengembangkan teknologi yang dapat bersaing secara internasional,\" ujar Budi. Kerja sama dengan Samsung memungkinkan Nusantara Tech mengakses teknologi terkini dan meningkatkan kapabilitas produksi. Garuda X dilengkapi dengan prosesor terbaru dan kamera berkualitas tinggi, yang diharapkan dapat memenuhi kebutuhan pengguna yang semakin kompleks. Selain itu, ponsel ini dirancang dengan mempertimbangkan aspek keberlanjutan, menggunakan bahan ramah lingkungan.",
        "Peluncuran Garuda X menandai langkah besar bagi industri teknologi Indonesia. Produk ini diharapkan dapat meningkatkan daya saing Indonesia di pasar teknologi global. Nusantara Tech berencana memperluas distribusi ponsel ini ke berbagai negara, termasuk Asia Tenggara dan Eropa. Dengan harga yang kompetitif, Garuda X diharapkan menarik minat konsumen yang mencari ponsel berkualitas dengan fitur inovatif. Para analis industri memprediksi bahwa ponsel ini akan menjadi salah satu produk unggulan Nusantara Tech dan memperkuat posisi Indonesia sebagai pemain penting dalam industri teknologi dunia."
    ]
    
    print("=" * 100)
    print("å°å°¼è¯­è¢«åŠ¨è¯­æ€æ£€æµ‹ - ä»… di- è¯å¤´è¿å†™è¯ç‰ˆæµ‹è¯•ï¼ˆå·²ä¿®æ­£ï¼‰")
    print("=" * 100)
    
    # æµ‹è¯•ï¼šç»Ÿè®¡æ–‡æœ¬ä¸­çš„è¢«åŠ¨åŠ¨è¯
    print("\nã€æµ‹è¯•ï¼šç»Ÿè®¡æ–‡æœ¬ä¸­çš„ di- è¢«åŠ¨è¯­æ€åŠ¨è¯æ•°é‡ã€‘")
    passed, detail, stats = check_passive_voice(test_text, exact_count=999, debug=True)
    print(detail)
    print(f"\nğŸ’¡ ç»“æœï¼šæ–‡æœ¬ä¸­å®é™…åŒ…å« {stats['total_passive']} ä¸ª di- è¢«åŠ¨è¯­æ€åŠ¨è¯")
    
    if stats['excluded_count'] > 0:
        print(f"ğŸ’¡ è¡¥å……ï¼šæ’é™¤äº† {stats['excluded_count']} ä¸ªéåŠ¨è¯ï¼ˆå¤–æ¥è¯ç­‰ï¼‰")
    
    if stats['di_space_phrases_count'] > 0:
        print(f"ğŸ’¡ è¡¥å……ï¼šæ£€æµ‹åˆ° {stats['di_space_phrases_count']} ä¸ª 'di + ç©ºæ ¼' ä»‹è¯çŸ­è¯­ï¼ˆæœªè®¡å…¥ç»“æœï¼‰")
    
    print("\n" + "=" * 100)
    print("ğŸ“Œ ç»“è®ºï¼š")
    print(f"   - ä»…è®¡ç®— di- è¿å†™è¯ï¼Œæ–‡æœ¬åŒ…å« {stats['total_passive']} ä¸ªè¢«åŠ¨è¯­æ€åŠ¨è¯")
    print(f"   - 'distribusi' ç­‰å¤–æ¥åè¯å·²è¢«æ­£ç¡®æ’é™¤")
    print(f"   - rule åç§°ä¿æŒä¸º: check_passive_voice:###æ•°é‡2###")
    print("=" * 100)



# ==================== å°å°¼è¯­å£è¯­åŒ–è¡¨è¾¾æ£€æµ‹ï¼ˆç²¾ç¡®æ•°é‡ç‰ˆ - å®Œå…¨å¢å¼ºç‰ˆ v4ï¼‰ ====================

import re
from typing import Tuple, Dict
from collections import Counter

def check_exact_colloquial_count(text: str, exact_count: int, debug: bool = False) -> Tuple[bool, str, Dict]:
    """
    æ£€æµ‹å°å°¼è¯­å£è¯­åŒ–è¡¨è¾¾æ•°é‡ï¼ˆå®Œå…¨å¢å¼ºç‰ˆ v4ï¼‰
    
    ä¸¥æ ¼æ¨¡å¼ï¼šå¿…é¡»æ­£å¥½ç­‰äº exact_countï¼Œä¸èƒ½å¤šä¹Ÿä¸èƒ½å°‘
    
    Args:
        text: è¦æ£€æµ‹çš„æ–‡æœ¬
        exact_count: è¦æ±‚çš„ç²¾ç¡®å£è¯­åŒ–è¡¨è¾¾æ•°é‡
        debug: æ˜¯å¦è¾“å‡ºè°ƒè¯•ä¿¡æ¯
    
    Returns:
        (æ˜¯å¦é€šè¿‡, è¯¦ç»†è¯´æ˜, ç»Ÿè®¡æ•°æ®)
    """
    
    # ============ ç±»å‹æ£€æŸ¥å’Œè½¬æ¢ ============
    if text is None:
        return False, "âŒ é”™è¯¯ï¼šè¾“å…¥æ–‡æœ¬ä¸ºç©ºï¼ˆNoneï¼‰", {
            'total_count': 0,
            'required_count': exact_count,
            'passed': False,
            'colloquial_words': [],
            'word_counts': {}
        }
    
    if isinstance(text, list):
        text_parts = []
        for item in text:
            if isinstance(item, str):
                text_parts.append(item)
            elif isinstance(item, dict):
                for key in ['text', 'content', 'message', 'response']:
                    if key in item and isinstance(item[key], str):
                        text_parts.append(item[key])
                        break
            else:
                text_parts.append(str(item))
        text = ' '.join(text_parts)
    
    if not isinstance(text, str):
        try:
            text = str(text)
        except:
            return False, f"âŒ é”™è¯¯ï¼šæ— æ³•è½¬æ¢ä¸ºå­—ç¬¦ä¸²", {
                'total_count': 0,
                'required_count': exact_count,
                'passed': False,
                'colloquial_words': [],
                'word_counts': {}
            }
    
    if not text.strip():
        return False, "âŒ é”™è¯¯ï¼šè¾“å…¥æ–‡æœ¬ä¸ºç©º", {
            'total_count': 0,
            'required_count': exact_count,
            'passed': False,
            'colloquial_words': [],
            'word_counts': {}
        }
    
    # ============ å£è¯­åŒ–è¡¨è¾¾è¯åº“ ============
    
    COLLOQUIAL_EXPRESSIONS = {
        # å£è¯­äººç§°ä»£è¯
        'gue': 'æˆ‘ï¼ˆå£è¯­ï¼Œæ ‡å‡†è¯­: sayaï¼‰',
        'gua': 'æˆ‘ï¼ˆå£è¯­ï¼Œæ ‡å‡†è¯­: sayaï¼‰',
        'gw': 'æˆ‘ï¼ˆå£è¯­ï¼Œæ ‡å‡†è¯­: sayaï¼‰',
        'ane': 'æˆ‘ï¼ˆå£è¯­ï¼Œæ ‡å‡†è¯­: sayaï¼‰',
        'lo': 'ä½ ï¼ˆå£è¯­ï¼Œæ ‡å‡†è¯­: kamu/Andaï¼‰',
        'lu': 'ä½ ï¼ˆå£è¯­ï¼Œæ ‡å‡†è¯­: kamu/Andaï¼‰',
        'loe': 'ä½ ï¼ˆå£è¯­ï¼Œæ ‡å‡†è¯­: kamu/Andaï¼‰',
        'elu': 'ä½ ï¼ˆå£è¯­ï¼Œæ ‡å‡†è¯­: kamu/Andaï¼‰',
        
        # å£è¯­å¦å®šè¯
        'nggak': 'ä¸ï¼ˆå£è¯­ï¼Œæ ‡å‡†è¯­: tidakï¼‰',
        'gak': 'ä¸ï¼ˆå£è¯­ï¼Œæ ‡å‡†è¯­: tidakï¼‰',
        'engga': 'ä¸ï¼ˆå£è¯­ï¼Œæ ‡å‡†è¯­: tidakï¼‰',
        'ngga': 'ä¸ï¼ˆå£è¯­ï¼Œæ ‡å‡†è¯­: tidakï¼‰',
        'ga': 'ä¸ï¼ˆå£è¯­ï¼Œæ ‡å‡†è¯­: tidakï¼‰',
        'enggak': 'ä¸ï¼ˆå£è¯­ï¼Œæ ‡å‡†è¯­: tidakï¼‰',
        
        # å£è¯­æ—¶é—´/çŠ¶æ€å‰¯è¯
        'udah': 'å·²ç»ï¼ˆå£è¯­ï¼Œæ ‡å‡†è¯­: sudahï¼‰',
        'dah': 'å·²ç»ï¼ˆå£è¯­ï¼Œæ ‡å‡†è¯­: sudahï¼‰',
        'udh': 'å·²ç»ï¼ˆå£è¯­ï¼Œæ ‡å‡†è¯­: sudahï¼‰',
        'abis': 'ä¹‹åï¼ˆå£è¯­ï¼Œæ ‡å‡†è¯­: setelah/habisï¼‰',
        'ntar': 'ç­‰ä¼šï¼ˆå£è¯­ï¼Œæ ‡å‡†è¯­: nantiï¼‰',
        'nti': 'ç­‰ä¼šï¼ˆå£è¯­ï¼Œæ ‡å‡†è¯­: nantiï¼‰',
        'bakal': 'å°†è¦ï¼ˆå£è¯­ï¼Œæ ‡å‡†è¯­: akanï¼‰',
        
        # å£è¯­åŠ¨è¯ï¼ˆçœç•¥å‰ç¼€ï¼‰
        'tau': 'çŸ¥é“ï¼ˆå£è¯­ï¼Œæ ‡å‡†è¯­: tahuï¼‰',
        'ketemu': 'é‡è§ï¼ˆå£è¯­ï¼Œæ ‡å‡†è¯­: bertemuï¼‰',
        'kenal': 'è®¤è¯†ï¼ˆå£è¯­ï¼Œæ ‡å‡†è¯­: mengenalï¼‰',
        'kasih': 'ç»™ï¼ˆå£è¯­ï¼Œæ ‡å‡†è¯­: beri/berikanï¼‰',
        'buat': 'ä¸ºäº†/åšï¼ˆå£è¯­ï¼Œæ ‡å‡†è¯­: untuk/membuatï¼‰',
        'nyesel': 'åæ‚”ï¼ˆå£è¯­ï¼Œæ ‡å‡†è¯­: menyesalï¼‰',
        'pengen': 'æƒ³è¦ï¼ˆå£è¯­ï¼Œæ ‡å‡†è¯­: inginï¼‰',
        
        # å£è¯­åŒ–åŠ¨è¯ï¼ˆng- å‰ç¼€ï¼‰
        'ngomong': 'è¯´è¯ï¼ˆå£è¯­ï¼Œæ ‡å‡†è¯­: berbicara/mengatakanï¼‰',
        'ngomongin': 'è°ˆè®ºï¼ˆå£è¯­ï¼Œæ ‡å‡†è¯­: membicarakanï¼‰',
        'ngasih': 'ç»™ï¼ˆå£è¯­ï¼Œæ ‡å‡†è¯­: memberiï¼‰',
        'ngasihin': 'ç»™äºˆï¼ˆå£è¯­ï¼Œæ ‡å‡†è¯­: memberikanï¼‰',
        'ngeliat': 'çœ‹ï¼ˆå£è¯­ï¼Œæ ‡å‡†è¯­: melihatï¼‰',
        'ngelakuin': 'åšï¼ˆå£è¯­ï¼Œæ ‡å‡†è¯­: melakukanï¼‰',
        'ngelupain': 'å¿˜è®°ï¼ˆå£è¯­ï¼Œæ ‡å‡†è¯­: melupakanï¼‰',
        'ngerasa': 'æ„Ÿè§‰ï¼ˆå£è¯­ï¼Œæ ‡å‡†è¯­: merasaï¼‰',
        'ngerti': 'æ‡‚ï¼ˆå£è¯­ï¼Œæ ‡å‡†è¯­: mengertiï¼‰',
        'ngulangin': 'é‡å¤ï¼ˆå£è¯­ï¼Œæ ‡å‡†è¯­: mengulangiï¼‰',
        'ngobrol': 'èŠå¤©ï¼ˆå£è¯­ï¼Œæ ‡å‡†è¯­: berbicara/mengobrolï¼‰',
        'ngobrolin': 'èŠå…³äºï¼ˆå£è¯­ï¼Œæ ‡å‡†è¯­: membicarakanï¼‰',
        
        # å£è¯­åŒ–åŠ¨è¯åç¼€ -in
        'bikin': 'åš/ä½¿ï¼ˆå£è¯­ï¼Œæ ‡å‡†è¯­: membuatï¼‰',
        'bikinin': 'åšç»™ï¼ˆå£è¯­ï¼Œæ ‡å‡†è¯­: membuatkanï¼‰',
        'maafin': 'åŸè°…ï¼ˆå£è¯­ï¼Œæ ‡å‡†è¯­: memaafkanï¼‰',
        'benerin': 'ä¿®å¤ï¼ˆå£è¯­ï¼Œæ ‡å‡†è¯­: membetulkan/memperbaikiï¼‰',
        'tungguin': 'ç­‰å¾…ï¼ˆå£è¯­ï¼Œæ ‡å‡†è¯­: menungguï¼‰',
        'dengerin': 'å¬ï¼ˆå£è¯­ï¼Œæ ‡å‡†è¯­: mendengarkanï¼‰',
        'bantuin': 'å¸®åŠ©ï¼ˆå£è¯­ï¼Œæ ‡å‡†è¯­: membantuï¼‰',
        'ikutin': 'è·Ÿéšï¼ˆå£è¯­ï¼Œæ ‡å‡†è¯­: mengikutiï¼‰',
        'ajakin': 'é‚€è¯·ï¼ˆå£è¯­ï¼Œæ ‡å‡†è¯­: mengajakï¼‰',
        'tanyain': 'è¯¢é—®ï¼ˆå£è¯­ï¼Œæ ‡å‡†è¯­: menanyakanï¼‰',
        
        # å£è¯­ç¨‹åº¦å‰¯è¯
        'banget': 'éå¸¸ï¼ˆå£è¯­ï¼Œæ ‡å‡†è¯­: sangat/sekaliï¼‰',
        'bgt': 'éå¸¸ï¼ˆå£è¯­ç¼©å†™ï¼Œæ ‡å‡†è¯­: sangatï¼‰',
        'bngt': 'éå¸¸ï¼ˆå£è¯­ç¼©å†™ï¼Œæ ‡å‡†è¯­: sangatï¼‰',
        'bener': 'çœŸçš„ï¼ˆå£è¯­ï¼Œæ ‡å‡†è¯­: benarï¼‰',
        'bnr': 'çœŸçš„ï¼ˆå£è¯­ç¼©å†™ï¼Œæ ‡å‡†è¯­: benarï¼‰',
        
        # å£è¯­è¿è¯/åŠ©è¯
        'emang': 'ç¡®å®ï¼ˆå£è¯­ï¼Œæ ‡å‡†è¯­: memangï¼‰',
        'emg': 'ç¡®å®ï¼ˆå£è¯­ç¼©å†™ï¼Œæ ‡å‡†è¯­: memangï¼‰',
        'sih': 'å‘¢/å•Šï¼ˆå£è¯­è¯­æ°”åŠ©è¯ï¼‰',
        'deh': 'å§ï¼ˆå£è¯­è¯­æ°”åŠ©è¯ï¼‰',
        'dong': 'å˜›ï¼ˆå£è¯­è¯­æ°”åŠ©è¯ï¼‰',
        'kok': 'æ€ä¹ˆï¼ˆå£è¯­ç–‘é—®è¯ï¼‰',
        'dunk': 'å˜›ï¼ˆå£è¯­ï¼Œæ ‡å‡†è¯­: dongï¼‰',
        'ya': 'å¥½å—/å§ï¼ˆå£è¯­è¯­æ°”åŠ©è¯ï¼‰',
        'yah': 'å•Šï¼ˆå£è¯­å¹è¯ï¼‰',
        'kan': 'ä¸æ˜¯å—ï¼ˆå£è¯­åŠ©è¯ï¼Œæ ‡å‡†è¯­: bukanï¼‰',
        'sumpah': 'å‘èª“ï¼ˆå£è¯­å¼ºè°ƒè¯ï¼‰',
        
        # å£è¯­ç–‘é—®è¯
        'gimana': 'æ€ä¹ˆæ ·ï¼ˆå£è¯­ï¼Œæ ‡å‡†è¯­: bagaimanaï¼‰',
        'gmn': 'æ€ä¹ˆæ ·ï¼ˆå£è¯­ç¼©å†™ï¼Œæ ‡å‡†è¯­: bagaimanaï¼‰',
        'kenapa': 'ä¸ºä»€ä¹ˆï¼ˆå£è¯­ï¼Œæ ‡å‡†è¯­: mengapaï¼‰',
        'knp': 'ä¸ºä»€ä¹ˆï¼ˆå£è¯­ç¼©å†™ï¼‰',
        'kayak': 'åƒï¼ˆå£è¯­ï¼Œæ ‡å‡†è¯­: sepertiï¼‰',
        'kaya': 'åƒï¼ˆå£è¯­ï¼Œæ ‡å‡†è¯­: sepertiï¼‰',
        
        # å…¶ä»–å¸¸è§å£è¯­è¯
        'aja': 'å°±/åªï¼ˆå£è¯­ï¼Œæ ‡å‡†è¯­: sajaï¼‰',
        'aj': 'å°±/åªï¼ˆå£è¯­ç¼©å†™ï¼Œæ ‡å‡†è¯­: sajaï¼‰',
        'nih': 'è¿™ï¼ˆå£è¯­ï¼Œæ ‡å‡†è¯­: iniï¼‰',
        'tuh': 'é‚£ï¼ˆå£è¯­ï¼Œæ ‡å‡†è¯­: ituï¼‰',
        'gitu': 'é‚£æ ·ï¼ˆå£è¯­ï¼Œæ ‡å‡†è¯­: begituï¼‰',
        'gini': 'è¿™æ ·ï¼ˆå£è¯­ï¼Œæ ‡å‡†è¯­: beginiï¼‰',
        'cuma': 'åªï¼ˆå£è¯­ï¼Œæ ‡å‡†è¯­: hanyaï¼‰',
        'doang': 'åªï¼ˆå£è¯­ï¼‰',
        'ama': 'å’Œï¼ˆå£è¯­ï¼Œæ ‡å‡†è¯­: denganï¼‰',
        'ma': 'å’Œï¼ˆå£è¯­ç¼©å†™ï¼‰',
        'sama': 'å’Œï¼ˆå£è¯­ï¼Œæ ‡å‡†è¯­: denganï¼‰',
        
        # ä¿šè¯­å’Œæ„Ÿå¹è¯
        'bete': 'çƒ¦èºï¼ˆå£è¯­/ä¿šè¯­ï¼‰',
        'kesel': 'çƒ¦æ¼/ç”Ÿæ°”ï¼ˆä¿šè¯­ï¼‰',
        'dodol': 'ç¬¨è›‹ï¼ˆå£è¯­/ä¿šè¯­ï¼‰',
        'beres': 'æå®šï¼ˆå£è¯­ï¼Œæ ‡å‡†è¯­: selesaiï¼‰',
        'kacau': 'ç³Ÿç³•ï¼ˆå£è¯­ï¼‰',
        'gara-gara': 'å› ä¸ºï¼ˆå£è¯­ï¼Œæ ‡å‡†è¯­: karenaï¼‰',
        'soalnya': 'å› ä¸ºï¼ˆå£è¯­ï¼Œæ ‡å‡†è¯­: karenaï¼‰',
        'makanya': 'æ‰€ä»¥ï¼ˆå£è¯­ï¼Œæ ‡å‡†è¯­: oleh karena ituï¼‰',
        'kelewat': 'è¿‡åˆ†/é”™è¿‡ï¼ˆå£è¯­ï¼Œæ ‡å‡†è¯­: terlalu/melewatkanï¼‰',
        
        # å£è¯­é‚€è¯·/å‘¼å”¤è¯
        'yuk': 'æ¥å§ï¼ˆå£è¯­é‚€è¯·è¯ï¼‰',
        'yo': 'å˜¿ï¼ˆè‹±è¯­å¤–æ¥å£è¯­æ‰“æ‹›å‘¼ï¼‰',
        'ayo': 'æ¥å§ï¼ˆå£è¯­é‚€è¯·è¯ï¼‰',
        'hei': 'å˜¿ï¼ˆå£è¯­æ‰“æ‹›å‘¼ï¼‰',
        'hai': 'å—¨ï¼ˆå£è¯­æ‰“æ‹›å‘¼ï¼‰',
        
        # å£è¯­ç¼©å†™
        'gt': 'é‚£æ ·ï¼ˆå£è¯­ç¼©å†™ï¼Œæ ‡å‡†è¯­: begituï¼‰',
        'bc': 'å› ä¸ºï¼ˆå£è¯­ç¼©å†™ï¼Œæ ‡å‡†è¯­: karenaï¼‰',
        'yg': 'çš„ï¼ˆä¹¦é¢ç¼©å†™ï¼Œä½†å¸¸ç”¨äºå£è¯­ï¼Œæ ‡å‡†è¯­: yangï¼‰',
        'dgn': 'å’Œï¼ˆä¹¦é¢ç¼©å†™ï¼Œä½†å¸¸ç”¨äºå£è¯­ï¼Œæ ‡å‡†è¯­: denganï¼‰',
        
        # ç½‘ç»œ/å¹´è½»äººå£è¯­
        'asik': 'å¥½ç©ï¼ˆå£è¯­ï¼Œæ ‡å‡†è¯­: asyikï¼‰',
        'mantap': 'å¾ˆæ£’ï¼ˆå£è¯­ï¼‰',
        'mantep': 'å¾ˆæ£’ï¼ˆå£è¯­ï¼‰',
        'keren': 'é…·ï¼ˆå£è¯­ï¼‰',
        'oke': 'å¥½ï¼ˆå£è¯­ï¼Œæ ‡å‡†è¯­: baikï¼‰',
        'ok': 'å¥½ï¼ˆå£è¯­ç¼©å†™ï¼‰',
        
        # è‹±è¯­å¤–æ¥å£è¯­
        'please': 'æ‹œæ‰˜ï¼ˆè‹±è¯­å¤–æ¥å£è¯­ï¼‰',
        'happy': 'å¼€å¿ƒï¼ˆè‹±è¯­å¤–æ¥å£è¯­ï¼‰',
        'love': 'çˆ±ï¼ˆè‹±è¯­å¤–æ¥å£è¯­ï¼‰',
        'sorry': 'æŠ±æ­‰ï¼ˆè‹±è¯­å¤–æ¥å£è¯­ï¼‰',
        'thanks': 'è°¢è°¢ï¼ˆè‹±è¯­å¤–æ¥å£è¯­ï¼‰',
        'cool': 'é…·ï¼ˆè‹±è¯­å¤–æ¥å£è¯­ï¼‰',
        'wow': 'å“‡ï¼ˆè‹±è¯­å¤–æ¥å£è¯­ï¼‰',
        'yeah': 'è€¶ï¼ˆè‹±è¯­å¤–æ¥å£è¯­ï¼‰',
        'yup': 'æ˜¯çš„ï¼ˆè‹±è¯­å¤–æ¥å£è¯­ï¼‰',
        'bye': 'å†è§ï¼ˆè‹±è¯­å¤–æ¥å£è¯­ï¼‰',
        
        # å…¶ä»–å¸¸ç”¨å£è¯­
        'traktir': 'è¯·å®¢ï¼ˆå£è¯­ï¼‰',
        'siapin': 'å‡†å¤‡ï¼ˆå£è¯­ï¼Œæ ‡å‡†è¯­: menyiapkanï¼‰',
        'jadian': 'æˆä¸ºæƒ…ä¾£ï¼ˆå£è¯­ï¼‰',
        'galau': 'è¿·èŒ«/çº ç»“ï¼ˆå£è¯­ï¼‰',
        'lebay': 'å¤¸å¼ ï¼ˆå£è¯­ï¼‰',
    }
    
    # ============ å›ºå®šç¤¼è²Œç”¨è¯­/æ­£å¼çŸ­è¯­ï¼ˆéœ€è¦æ’é™¤çš„ï¼‰ ============
    
    FORMAL_PHRASES = {
        ('terima', 'kasih'): ['kasih'],
        ('sama', 'sama'): ['sama'],
        ('selamat', 'pagi'): [],
        ('selamat', 'siang'): [],
        ('selamat', 'malam'): [],
        ('mohon', 'maaf'): [],
        ('dengan', 'hormat'): [],
    }
    
    # ============ æŸ¥æ‰¾å£è¯­åŒ–è¡¨è¾¾ ============
    
    text_lower = text.lower()
    found_colloquial = []
    found_positions = set()
    excluded_positions = set()
    
    # ç¬¬ä¸€æ­¥ï¼šæ ‡è®°æ‰€æœ‰å›ºå®šç¤¼è²Œç”¨è¯­ä¸­éœ€è¦æ’é™¤çš„è¯
    for (word1, word2), exclude_words in FORMAL_PHRASES.items():
        phrase_pattern = r'\b' + re.escape(word1) + r'[\s\-]*' + re.escape(word2) + r'\b'
        phrase_matches = re.finditer(phrase_pattern, text_lower, re.IGNORECASE)
        
        for phrase_match in phrase_matches:
            phrase_start = phrase_match.start()
            phrase_end = phrase_match.end()
            
            for exclude_word in exclude_words:
                word_pattern = r'\b' + re.escape(exclude_word) + r'\b'
                word_matches = re.finditer(word_pattern, text_lower[phrase_start:phrase_end])
                
                for word_match in word_matches:
                    actual_pos = phrase_start + word_match.start()
                    excluded_positions.add(actual_pos)
    
    # ç¬¬äºŒæ­¥ï¼šæŸ¥æ‰¾æ‰€æœ‰å£è¯­åŒ–è¡¨è¾¾
    for colloquial_word, description in COLLOQUIAL_EXPRESSIONS.items():
        pattern = r'\b' + re.escape(colloquial_word) + r'\b'
        matches = re.finditer(pattern, text_lower)
        
        for match in matches:
            start_pos = match.start()
            end_pos = match.end()
            
            if start_pos in excluded_positions:
                continue
            
            if start_pos in found_positions:
                continue
            
            # ç‰¹æ®Šä¸Šä¸‹æ–‡æ£€æŸ¥ï¼škasih
            if colloquial_word == 'kasih':
                context_before = text_lower[max(0, start_pos-10):start_pos].strip()
                if context_before.endswith('terima'):
                    continue
            
            # ç‰¹æ®Šä¸Šä¸‹æ–‡æ£€æŸ¥ï¼šsama
            if colloquial_word == 'sama':
                context_before = text_lower[max(0, start_pos-7):start_pos].strip()
                context_after = text_lower[end_pos:end_pos+7].strip()
                
                if context_before.endswith('sama') or context_after.startswith('sama'):
                    broader_context = text_lower[max(0, start_pos-50):min(len(text), end_pos+50)]
                    if any(formal_word in broader_context for formal_word in ['hormat', 'yth', 'dengan segala']):
                        continue
            
            original_word = text[start_pos:end_pos]
            
            found_colloquial.append({
                'word': original_word,
                'word_lower': colloquial_word,
                'description': description,
                'position': start_pos,
                'context': text[max(0, start_pos-30):min(len(text), end_pos+30)],
                'category': _categorize_colloquial(colloquial_word, description)
            })
            
            found_positions.add(start_pos)
    
    found_colloquial.sort(key=lambda x: x['position'])
    
    # ============ ç»Ÿè®¡ ============
    
    total_count = len(found_colloquial)
    word_counter = Counter([item['word_lower'] for item in found_colloquial])
    category_counter = Counter([item['category'] for item in found_colloquial])
    
    # ============ åˆ¤æ–­æ˜¯å¦é€šè¿‡ ============
    
    passed = (total_count == exact_count)
    
    # ============ ç”Ÿæˆè¯¦ç»†è¯´æ˜ ============
    
    detail_parts = []
    
    if passed:
        detail_parts.append(f"âœ… æ­£ç¡®ï¼šæ‰¾åˆ°æ­£å¥½ {total_count} ä¸ªå£è¯­åŒ–è¡¨è¾¾ï¼ˆè¦æ±‚æ­£å¥½ {exact_count} ä¸ªï¼‰\n")
        
        detail_parts.append("æ‰¾åˆ°çš„å£è¯­åŒ–è¡¨è¾¾ï¼š")
        for i, item in enumerate(found_colloquial, 1):
            detail_parts.append(f"  {i}. {item['word']} - {item['description']} (ä½ç½® {item['position']})")
        
        if len(word_counter) < total_count:
            detail_parts.append(f"\nè¯é¢‘ç»Ÿè®¡ï¼š")
            for word, count in word_counter.most_common(10):
                if count > 1:
                    detail_parts.append(f"  - {word}: {count}æ¬¡")
    
    else:
        if total_count < exact_count:
            shortage = exact_count - total_count
            detail_parts.append(f"âŒ é”™è¯¯ï¼šåªæ‰¾åˆ° {total_count} ä¸ªå£è¯­åŒ–è¡¨è¾¾ï¼Œå°‘äºè¦æ±‚çš„ {exact_count} ä¸ªï¼ˆè¿˜å·® {shortage} ä¸ªï¼‰\n")
        else:
            excess = total_count - exact_count
            detail_parts.append(f"âŒ é”™è¯¯ï¼šæ‰¾åˆ° {total_count} ä¸ªå£è¯­åŒ–è¡¨è¾¾ï¼Œè¶…è¿‡è¦æ±‚çš„ {exact_count} ä¸ªï¼ˆå¤šäº† {excess} ä¸ªï¼‰\n")
        
        if found_colloquial:
            detail_parts.append("å·²æ‰¾åˆ°çš„å£è¯­åŒ–è¡¨è¾¾ï¼š")
            for i, item in enumerate(found_colloquial, 1):
                detail_parts.append(f"  {i}. {item['word']} - {item['description']} (ä½ç½® {item['position']})")
            
            if len(word_counter) < total_count:
                detail_parts.append(f"\nè¯é¢‘ç»Ÿè®¡ï¼š")
                for word, count in word_counter.most_common(10):
                    if count > 1:
                        detail_parts.append(f"  - {word}: {count}æ¬¡")
    
    detail = '\n'.join(detail_parts)
    
    # ============ ç»Ÿè®¡æ•°æ® ============
    
    stats = {
        'total_count': total_count,
        'required_count': exact_count,
        'difference': total_count - exact_count,
        'passed': passed,
        'colloquial_words': [item['word'] for item in found_colloquial],
        'word_counts': dict(word_counter),
        'category_counts': dict(category_counter),
        'unique_count': len(word_counter),
        'all_found': found_colloquial,
        'excluded_count': len(excluded_positions),
        'check_mode': 'exact_match_enhanced_v4'
    }
    
    return passed, detail, stats


def _categorize_colloquial(word: str, description: str) -> str:
    """å°†å£è¯­åŒ–è¡¨è¾¾åˆ†ç±»"""
    if 'äººç§°' in description or word in ['gue', 'gua', 'gw', 'lo', 'lu', 'loe']:
        return 'å£è¯­äººç§°'
    elif 'å¦å®š' in description or word in ['gak', 'nggak', 'ga', 'ngga']:
        return 'å£è¯­å¦å®š'
    elif 'åŠ¨è¯' in description or word.startswith('ng') or word.endswith('in') or word in ['bikin', 'kasih', 'buat', 'nyesel', 'pengen']:
        return 'å£è¯­åŠ¨è¯'
    elif 'ç¨‹åº¦' in description or word in ['banget', 'bener']:
        return 'ç¨‹åº¦å‰¯è¯'
    elif 'åŠ©è¯' in description or word in ['sih', 'deh', 'dong', 'kok', 'ya', 'yah']:
        return 'è¯­æ°”åŠ©è¯'
    elif 'æ—¶é—´' in description or word in ['udah', 'ntar', 'bakal']:
        return 'æ—¶é—´å‰¯è¯'
    elif 'ä¿šè¯­' in description or word in ['bete', 'dodol', 'kacau', 'kesel']:
        return 'ä¿šè¯­'
    elif 'è‹±è¯­' in description:
        return 'è‹±è¯­å¤–æ¥å£è¯­'
    elif 'é‚€è¯·' in description or word in ['yuk', 'ayo']:
        return 'é‚€è¯·è¯'
    else:
        return 'å…¶ä»–å£è¯­'


# ==================== å°å°¼è¯­æ•¬è¯­è¡¨è¾¾æ£€æµ‹ï¼ˆç²¾ç¡®æ•°é‡ç‰ˆ - ä¿®è®¢ç‰ˆ v3ï¼‰ ====================
def check_formal_honorifics(text: str, exact_count: int = 5) -> Tuple[bool, str, Dict]:
    """
    æ£€æµ‹å°å°¼è¯­æ•¬è¯­è¡¨è¾¾ï¼ˆbahasa hormatï¼‰- ä¿®è®¢ç‰ˆ v3
    ä¸¥æ ¼æ¨¡å¼ï¼šå¿…é¡»æ­£å¥½ç­‰äº exact_countï¼Œä¸èƒ½å¤šä¹Ÿä¸èƒ½å°‘
    Args:
        text: è¦æ£€æµ‹çš„æ–‡æœ¬
        exact_count: è¦æ±‚çš„ç²¾ç¡®æ•¬è¯­è¡¨è¾¾æ•°é‡
    Returns:
        (æ˜¯å¦é€šè¿‡, è¯¦ç»†è¯´æ˜, ç»Ÿè®¡æ•°æ®)
    """
    # ============ ç±»å‹æ£€æŸ¥å’Œè½¬æ¢ ============
    if text is None:
        return False, "âŒ é”™è¯¯ï¼šè¾“å…¥æ–‡æœ¬ä¸ºç©ºï¼ˆNoneï¼‰", {
            'total_count': 0,
            'required_count': exact_count,
            'passed': False,
            'honorific_words': [],
            'word_counts': {},
            'by_category': {}
        }
    if isinstance(text, list):
        text_parts = []
        for item in text:
            if isinstance(item, str):
                text_parts.append(item)
            elif isinstance(item, dict):
                for key in ['text', 'content', 'message', 'response']:
                    if key in item and isinstance(item[key], str):
                        text_parts.append(item[key])
                        break
            else:
                text_parts.append(str(item))
        text = ' '.join(text_parts)
    if not isinstance(text, str):
        try:
            text = str(text)
        except:
            return False, f"âŒ é”™è¯¯ï¼šæ— æ³•è½¬æ¢ä¸ºå­—ç¬¦ä¸²", {
                'total_count': 0,
                'required_count': exact_count,
                'passed': False,
                'honorific_words': [],
                'word_counts': {},
                'by_category': {}
            }
    if not text.strip():
        return False, "âŒ é”™è¯¯ï¼šè¾“å…¥æ–‡æœ¬ä¸ºç©º", {
            'total_count': 0,
            'required_count': exact_count,
            'passed': False,
            'honorific_words': [],
            'word_counts': {},
            'by_category': {}
        }
    # ============ æ•¬è¯­è¯åº“ ============
    # ä¼˜å…ˆçº§1ï¼šæ•¬è¯­çŸ­è¯­ï¼ˆå¿…é¡»æ•´ä½“åŒ¹é…ï¼‰
    HONORIFIC_PHRASES = {
        'minta maaf': {'category': 'ç¤¼è²ŒçŸ­è¯­', 'meaning': 'é“æ­‰'},
        'terima kasih': {'category': 'ç¤¼è²ŒçŸ­è¯­', 'meaning': 'æ„Ÿè°¢'},
        'permohonan maaf': {'category': 'ç¤¼è²ŒçŸ­è¯­', 'meaning': 'è¯·æ±‚åŸè°…'},
        'permintaan maaf': {'category': 'ç¤¼è²ŒçŸ­è¯­', 'meaning': 'è¯·æ±‚åŸè°…'},
        'yang terhormat': {'category': 'æ•¬ç§°çŸ­è¯­', 'meaning': 'å°Šæ•¬çš„'},
        'yang tercinta': {'category': 'æ•¬ç§°çŸ­è¯­', 'meaning': 'æŒšçˆ±çš„'},
        'hormat saya': {'category': 'ç¤¼è²ŒçŸ­è¯­', 'meaning': 'æˆ‘çš„æ•¬æ„'},
        'dengan hormat': {'category': 'ç¤¼è²ŒçŸ­è¯­', 'meaning': 'æ•¬ä¸Š'},
        'mohon memaafkan': {'category': 'ç¤¼è²ŒçŸ­è¯­', 'meaning': 'è¯·åŸè°…'},
        'memaafkan kesalahan': {'category': 'ç¤¼è²ŒçŸ­è¯­', 'meaning': 'åŸè°…é”™è¯¯'},
        'sang penyanyi': {'category': 'æ•¬ç§°çŸ­è¯­', 'meaning': 'æ•¬è¯­æ ‡è®°è¯ï¼ˆå°Šæ•¬çš„æ­Œæ‰‹ï¼‰'},
    }
    # ä¼˜å…ˆçº§2ï¼šæ•¬è¯­å•è¯
    HONORIFIC_WORDS = {
        # æ•¬ç§°äººç§°ä»£è¯
        'anda': {'category': 'æ•¬ç§°äººç§°ä»£è¯', 'meaning': 'æ‚¨'},
        'saudara': {'category': 'æ•¬ç§°äººç§°ä»£è¯', 'meaning': 'æ‚¨ï¼ˆåŒè¾ˆï¼‰'},
        'beliau': {'category': 'æ•¬ç§°äººç§°ä»£è¯', 'meaning': 'ä»–/å¥¹ï¼ˆå°Šç§°ï¼‰'},
        # æ•¬ç§°å‘¼è¯å’Œç¼©å†™
        'bapak': {'category': 'æ•¬ç§°å‘¼è¯', 'meaning': 'å…ˆç”Ÿ'},
        'ibu': {'category': 'æ•¬ç§°å‘¼è¯', 'meaning': 'å¥³å£«/å¤ªå¤ª'},
        'sang': {'category': 'æ•¬ç§°å‘¼è¯', 'meaning': 'æ•¬è¯­æ ‡è®°è¯ï¼ˆç”¨äºå°Šæ•¬åœ°æŒ‡ä»£æŸäººï¼‰'},
        'yth': {'category': 'æ•¬ç§°å‘¼è¯', 'meaning': 'å°Šæ•¬çš„ï¼ˆç¼©å†™ï¼‰'},
        # ç¤¼è²ŒåŠ©è¯
        'silakan': {'category': 'ç¤¼è²ŒåŠ©è¯', 'meaning': 'è¯·'},
        'mohon': {'category': 'ç¤¼è²ŒåŠ©è¯', 'meaning': 'æ³è¯·'},
        'tolong': {'category': 'é€šç”¨ç¤¼è²Œè¯', 'meaning': 'è¯·ï¼ˆå¸®å¿™ï¼‰'},
        'sudilah': {'category': 'ç¤¼è²ŒåŠ©è¯', 'meaning': 'è¯·ï¼ˆæ–‡é›…ï¼‰'},
        'kiranya': {'category': 'ç¤¼è²ŒåŠ©è¯', 'meaning': 'å¸Œæœ›ï¼ˆæ­£å¼ï¼‰'},
        # æ­£å¼æ•¬è¯­è¯æ±‡
        'hormat': {'category': 'æ­£å¼æ•¬è¯­', 'meaning': 'æ•¬æ„'},
        'terhormat': {'category': 'æ­£å¼æ•¬è¯­', 'meaning': 'å°Šæ•¬çš„'},
        'berkenan': {'category': 'æ­£å¼æ•¬è¯­', 'meaning': 'æ„¿æ„ï¼ˆæ•¬è¯­ï¼‰'},
        'salam': {'category': 'æ­£å¼æ•¬è¯­', 'meaning': 'æ•¬ç¤¼/é—®å€™'},
    }
    # ============ æŸ¥æ‰¾æ•¬è¯­è¡¨è¾¾ ============
    text_lower = text.lower()
    found_honorifics = []
    category_counts = {}
    phrase_positions = []
    # ç¬¬ä¸€æ­¥ï¼šå…ˆåŒ¹é…çŸ­è¯­ï¼ˆä½¿ç”¨çµæ´»çš„ç©ºç™½ç¬¦åŒ¹é…ï¼‰
    for phrase, info in HONORIFIC_PHRASES.items():
        # å¯¹äºå¤šè¯çŸ­è¯­ï¼Œå…è®¸è¯ä¹‹é—´æœ‰å¤šä¸ªç©ºæ ¼æˆ–æ¢è¡Œç¬¦
        if ' ' in phrase:
            # å°†çŸ­è¯­ä¸­çš„ç©ºæ ¼æ›¿æ¢ä¸ºçµæ´»çš„ç©ºç™½ç¬¦åŒ¹é…
            flexible_phrase = re.escape(phrase)
            flexible_phrase = flexible_phrase.replace(r'\ ', r'\s+')
            pattern = r'\b' + flexible_phrase + r'\b'
        else:
            pattern = r'\b' + re.escape(phrase) + r'\b'
        
        matches = re.finditer(pattern, text_lower)
        for match in matches:
            start_pos = match.start()
            end_pos = match.end()
            original_phrase = text[start_pos:end_pos]
            found_honorifics.append({
                'word': original_phrase,
                'word_lower': phrase,
                'category': info['category'],
                'meaning': info['meaning'],
                'position': start_pos,
                'is_phrase': True
            })
            category = info['category']
            category_counts[category] = category_counts.get(category, 0) + 1
            phrase_positions.append((start_pos, end_pos))
    # ç¬¬äºŒæ­¥ï¼šå†åŒ¹é…å•è¯ï¼ˆæ’é™¤å·²åŒ¹é…çŸ­è¯­çš„ä½ç½®ï¼‰
    for word, info in HONORIFIC_WORDS.items():
        pattern = r'\b' + re.escape(word) + r'\b'
        matches = re.finditer(pattern, text_lower)
        for match in matches:
            start_pos = match.start()
            end_pos = match.end()
            # æ£€æŸ¥æ˜¯å¦åœ¨çŸ­è¯­å†…
            in_phrase = any(p_start <= start_pos < p_end 
                          for p_start, p_end in phrase_positions)
            if not in_phrase:
                original_word = text[start_pos:end_pos]
                found_honorifics.append({
                    'word': original_word,
                    'word_lower': word,
                    'category': info['category'],
                    'meaning': info['meaning'],
                    'position': start_pos,
                    'is_phrase': False
                })
                category = info['category']
                category_counts[category] = category_counts.get(category, 0) + 1
    # æŒ‰ä½ç½®æ’åº
    found_honorifics.sort(key=lambda x: x['position'])
    # ============ ç»Ÿè®¡ ============
    total_count = len(found_honorifics)
    word_counter = Counter([item['word_lower'] for item in found_honorifics])
    # ============ åˆ¤æ–­æ˜¯å¦é€šè¿‡ ============
    passed = (total_count == exact_count)
    # ============ ç”Ÿæˆè¯¦ç»†è¯´æ˜ ============
    detail_parts = []
    if passed:
        detail_parts.append(f"âœ… æ­£ç¡®ï¼šæ‰¾åˆ°æ­£å¥½ {total_count} ä¸ªæ•¬è¯­è¡¨è¾¾ï¼ˆè¦æ±‚æ­£å¥½ {exact_count} ä¸ªï¼‰\n")
        detail_parts.append("æ‰¾åˆ°çš„æ•¬è¯­è¡¨è¾¾ï¼š")
        for i, item in enumerate(found_honorifics, 1):
            detail_parts.append(f"  {i}. {item['word']} - ã€{item['category']}ã€‘{item['meaning']} (ä½ç½® {item['position']})")
        if category_counts:
            detail_parts.append(f"\næŒ‰ç±»åˆ«ç»Ÿè®¡ï¼š")
            for category, count in category_counts.items():
                detail_parts.append(f"  - {category}: {count} ä¸ª")
    else:
        if total_count < exact_count:
            shortage = exact_count - total_count
            detail_parts.append(f"âŒ é”™è¯¯ï¼šåªæ‰¾åˆ° {total_count} ä¸ªæ•¬è¯­è¡¨è¾¾ï¼Œå°‘äºè¦æ±‚çš„ {exact_count} ä¸ªï¼ˆè¿˜å·® {shortage} ä¸ªï¼‰\n")
        else:
            excess = total_count - exact_count
            detail_parts.append(f"âŒ é”™è¯¯ï¼šæ‰¾åˆ° {total_count} ä¸ªæ•¬è¯­è¡¨è¾¾ï¼Œè¶…è¿‡è¦æ±‚çš„ {exact_count} ä¸ªï¼ˆå¤šäº† {excess} ä¸ªï¼‰\n")
        if found_honorifics:
            detail_parts.append("å·²æ‰¾åˆ°çš„æ•¬è¯­è¡¨è¾¾ï¼š")
            for i, item in enumerate(found_honorifics, 1):
                detail_parts.append(f"  {i}. {item['word']} - ã€{item['category']}ã€‘{item['meaning']} (ä½ç½® {item['position']})")
            if category_counts:
                detail_parts.append(f"\næŒ‰ç±»åˆ«ç»Ÿè®¡ï¼š")
                for category, count in category_counts.items():
                    detail_parts.append(f"  - {category}: {count} ä¸ª")
        detail_parts.append("\n\nâš ï¸ æ³¨æ„ï¼š")
        detail_parts.append("  - memaafkan å•ç‹¬å‡ºç°ä¸ç®—æ•¬è¯­ï¼Œéœ€åœ¨çŸ­è¯­ä¸­ï¼ˆå¦‚ï¼šmohon memaafkanï¼‰")
        detail_parts.append("  - hormat å•ç‹¬å‡ºç°ç®—æ•¬è¯­ï¼Œå¸¸è§çŸ­è¯­ï¼šhormat saya, dengan hormat")
        detail_parts.append("  - sang æ˜¯æ•¬è¯­æ ‡è®°è¯ï¼Œç”¨äºå°Šæ•¬åœ°æŒ‡ä»£æŸäººï¼ˆå¦‚ï¼šsang penyanyiï¼‰")
    detail = '\n'.join(detail_parts)
    # ============ ç»Ÿè®¡æ•°æ® ============
    stats = {
        'total_count': total_count,
        'required_count': exact_count,
        'difference': total_count - exact_count,
        'passed': passed,
        'honorific_words': [item['word'] for item in found_honorifics],
        'word_counts': dict(word_counter),
        'by_category': category_counts,
        'unique_count': len(word_counter),
        'all_found': found_honorifics,
        'check_mode': 'exact_match_v3_flexible'
    }
    return passed, detail, stats
# ==================== æµ‹è¯•ç¤ºä¾‹ ====================
if __name__ == "__main__":
    # æµ‹è¯•æ–‡æœ¬
    test_text = """
    Selamat malam, hadirin yang terhormat. Dengan penuh rasa syukur dan kebahagiaan,
    saya menyambut Anda semua di acara tahunan perusahaan kita yang ke-10. Malam ini
    adalah malam yang istimewa, di mana kita berkumpul untuk merayakan pencapaian dan
    kebersamaan kita selama setahun terakhir.
    Pertama-tama, izinkan saya mengucapkan terima kasih yang sebesar-besarnya kepada
    seluruh tamu undangan yang telah meluangkan waktu untuk hadir di sini. Kehadiran
    Anda semua adalah kehormatan besar bagi kami. Terlebih lagi, kami merasa sangat
    beruntung karena malam ini kita akan ditemani oleh beberapa bintang penyanyi
    terkenal yang akan menghibur kita dengan suara merdu mereka.
    Si acara tahunan ini adalah momen yang kita nantikan setiap tahun, di mana kita
    dapat berkumpul, berbagi cerita, dan mempererat hubungan. Sang penyanyi yang akan
    tampil malam ini adalah sosok yang telah menginspirasi banyak orang dengan
    karya-karya mereka. Kami berharap penampilan mereka akan memberikan kenangan
    indah bagi kita semua.
    Sebelum kita memulai rangkaian acara, saya ingin mengajak Anda semua untuk
    menikmati malam ini dengan penuh sukacita. Mohon untuk menjaga ketertiban dan
    kenyamanan selama acara berlangsung. Kami juga memohon kesediaan Anda untuk
    memberikan apresiasi yang hangat kepada para penampil kita malam ini.
    Sekali lagi, terima kasih atas kehadiran Anda. Semoga malam ini menjadi malam
    yang penuh kebahagiaan dan kenangan indah bagi kita semua. Selamat menikmati acara!
    """

# ==================== å°å°¼è¯­å£è¯­åŒ–è¡¨è¾¾æ£€æµ‹ï¼ˆç²¾ç¡®æ•°é‡ç‰ˆ - å®Œå…¨å¢å¼ºç‰ˆ v4ï¼‰ ====================

import re
from typing import Tuple, Dict
from collections import Counter

def check_exact_colloquial_count(text: str, exact_count: int, debug: bool = False) -> Tuple[bool, str, Dict]:
    """
    æ£€æµ‹å°å°¼è¯­å£è¯­åŒ–è¡¨è¾¾æ•°é‡ï¼ˆå®Œå…¨å¢å¼ºç‰ˆ v4ï¼‰
    
    ä¸¥æ ¼æ¨¡å¼ï¼šå¿…é¡»æ­£å¥½ç­‰äº exact_countï¼Œä¸èƒ½å¤šä¹Ÿä¸èƒ½å°‘
    
    Args:
        text: è¦æ£€æµ‹çš„æ–‡æœ¬
        exact_count: è¦æ±‚çš„ç²¾ç¡®å£è¯­åŒ–è¡¨è¾¾æ•°é‡
        debug: æ˜¯å¦è¾“å‡ºè°ƒè¯•ä¿¡æ¯
    
    Returns:
        (æ˜¯å¦é€šè¿‡, è¯¦ç»†è¯´æ˜, ç»Ÿè®¡æ•°æ®)
    """
    
    # ============ ç±»å‹æ£€æŸ¥å’Œè½¬æ¢ ============
    if text is None:
        return False, "âŒ é”™è¯¯ï¼šè¾“å…¥æ–‡æœ¬ä¸ºç©ºï¼ˆNoneï¼‰", {
            'total_count': 0,
            'required_count': exact_count,
            'passed': False,
            'colloquial_words': [],
            'word_counts': {}
        }
    
    if isinstance(text, list):
        text_parts = []
        for item in text:
            if isinstance(item, str):
                text_parts.append(item)
            elif isinstance(item, dict):
                for key in ['text', 'content', 'message', 'response']:
                    if key in item and isinstance(item[key], str):
                        text_parts.append(item[key])
                        break
            else:
                text_parts.append(str(item))
        text = ' '.join(text_parts)
    
    if not isinstance(text, str):
        try:
            text = str(text)
        except:
            return False, f"âŒ é”™è¯¯ï¼šæ— æ³•è½¬æ¢ä¸ºå­—ç¬¦ä¸²", {
                'total_count': 0,
                'required_count': exact_count,
                'passed': False,
                'colloquial_words': [],
                'word_counts': {}
            }
    
    if not text.strip():
        return False, "âŒ é”™è¯¯ï¼šè¾“å…¥æ–‡æœ¬ä¸ºç©º", {
            'total_count': 0,
            'required_count': exact_count,
            'passed': False,
            'colloquial_words': [],
            'word_counts': {}
        }
    
    # ============ å£è¯­åŒ–è¡¨è¾¾è¯åº“ ============
    
    COLLOQUIAL_EXPRESSIONS = {
        # å£è¯­äººç§°ä»£è¯
        'gue': 'æˆ‘ï¼ˆå£è¯­ï¼Œæ ‡å‡†è¯­: sayaï¼‰',
        'gua': 'æˆ‘ï¼ˆå£è¯­ï¼Œæ ‡å‡†è¯­: sayaï¼‰',
        'gw': 'æˆ‘ï¼ˆå£è¯­ï¼Œæ ‡å‡†è¯­: sayaï¼‰',
        'ane': 'æˆ‘ï¼ˆå£è¯­ï¼Œæ ‡å‡†è¯­: sayaï¼‰',
        'lo': 'ä½ ï¼ˆå£è¯­ï¼Œæ ‡å‡†è¯­: kamu/Andaï¼‰',
        'lu': 'ä½ ï¼ˆå£è¯­ï¼Œæ ‡å‡†è¯­: kamu/Andaï¼‰',
        'loe': 'ä½ ï¼ˆå£è¯­ï¼Œæ ‡å‡†è¯­: kamu/Andaï¼‰',
        'elu': 'ä½ ï¼ˆå£è¯­ï¼Œæ ‡å‡†è¯­: kamu/Andaï¼‰',
        
        # å£è¯­å¦å®šè¯
        'nggak': 'ä¸ï¼ˆå£è¯­ï¼Œæ ‡å‡†è¯­: tidakï¼‰',
        'gak': 'ä¸ï¼ˆå£è¯­ï¼Œæ ‡å‡†è¯­: tidakï¼‰',
        'engga': 'ä¸ï¼ˆå£è¯­ï¼Œæ ‡å‡†è¯­: tidakï¼‰',
        'ngga': 'ä¸ï¼ˆå£è¯­ï¼Œæ ‡å‡†è¯­: tidakï¼‰',
        'ga': 'ä¸ï¼ˆå£è¯­ï¼Œæ ‡å‡†è¯­: tidakï¼‰',
        'enggak': 'ä¸ï¼ˆå£è¯­ï¼Œæ ‡å‡†è¯­: tidakï¼‰',
        
        # å£è¯­æ—¶é—´/çŠ¶æ€å‰¯è¯
        'udah': 'å·²ç»ï¼ˆå£è¯­ï¼Œæ ‡å‡†è¯­: sudahï¼‰',
        'dah': 'å·²ç»ï¼ˆå£è¯­ï¼Œæ ‡å‡†è¯­: sudahï¼‰',
        'udh': 'å·²ç»ï¼ˆå£è¯­ï¼Œæ ‡å‡†è¯­: sudahï¼‰',
        'abis': 'ä¹‹åï¼ˆå£è¯­ï¼Œæ ‡å‡†è¯­: setelah/habisï¼‰',
        'ntar': 'ç­‰ä¼šï¼ˆå£è¯­ï¼Œæ ‡å‡†è¯­: nantiï¼‰',
        'nti': 'ç­‰ä¼šï¼ˆå£è¯­ï¼Œæ ‡å‡†è¯­: nantiï¼‰',
        'bakal': 'å°†è¦ï¼ˆå£è¯­ï¼Œæ ‡å‡†è¯­: akanï¼‰',
        
        # å£è¯­åŠ¨è¯ï¼ˆçœç•¥å‰ç¼€ï¼‰
        'tau': 'çŸ¥é“ï¼ˆå£è¯­ï¼Œæ ‡å‡†è¯­: tahuï¼‰',
        'ketemu': 'é‡è§ï¼ˆå£è¯­ï¼Œæ ‡å‡†è¯­: bertemuï¼‰',
        'kenal': 'è®¤è¯†ï¼ˆå£è¯­ï¼Œæ ‡å‡†è¯­: mengenalï¼‰',
        'kasih': 'ç»™ï¼ˆå£è¯­ï¼Œæ ‡å‡†è¯­: beri/berikanï¼‰',
        'buat': 'ä¸ºäº†/åšï¼ˆå£è¯­ï¼Œæ ‡å‡†è¯­: untuk/membuatï¼‰',
        'nyesel': 'åæ‚”ï¼ˆå£è¯­ï¼Œæ ‡å‡†è¯­: menyesalï¼‰',
        'pengen': 'æƒ³è¦ï¼ˆå£è¯­ï¼Œæ ‡å‡†è¯­: inginï¼‰',
        
        # å£è¯­åŒ–åŠ¨è¯ï¼ˆng- å‰ç¼€ï¼‰
        'ngomong': 'è¯´è¯ï¼ˆå£è¯­ï¼Œæ ‡å‡†è¯­: berbicara/mengatakanï¼‰',
        'ngomongin': 'è°ˆè®ºï¼ˆå£è¯­ï¼Œæ ‡å‡†è¯­: membicarakanï¼‰',
        'ngasih': 'ç»™ï¼ˆå£è¯­ï¼Œæ ‡å‡†è¯­: memberiï¼‰',
        'ngasihin': 'ç»™äºˆï¼ˆå£è¯­ï¼Œæ ‡å‡†è¯­: memberikanï¼‰',
        'ngeliat': 'çœ‹ï¼ˆå£è¯­ï¼Œæ ‡å‡†è¯­: melihatï¼‰',
        'ngelakuin': 'åšï¼ˆå£è¯­ï¼Œæ ‡å‡†è¯­: melakukanï¼‰',
        'ngelupain': 'å¿˜è®°ï¼ˆå£è¯­ï¼Œæ ‡å‡†è¯­: melupakanï¼‰',
        'ngerasa': 'æ„Ÿè§‰ï¼ˆå£è¯­ï¼Œæ ‡å‡†è¯­: merasaï¼‰',
        'ngerti': 'æ‡‚ï¼ˆå£è¯­ï¼Œæ ‡å‡†è¯­: mengertiï¼‰',
        'ngulangin': 'é‡å¤ï¼ˆå£è¯­ï¼Œæ ‡å‡†è¯­: mengulangiï¼‰',
        'ngobrol': 'èŠå¤©ï¼ˆå£è¯­ï¼Œæ ‡å‡†è¯­: berbicara/mengobrolï¼‰',
        'ngobrolin': 'èŠå…³äºï¼ˆå£è¯­ï¼Œæ ‡å‡†è¯­: membicarakanï¼‰',
        
        # å£è¯­åŒ–åŠ¨è¯åç¼€ -in
        'bikin': 'åš/ä½¿ï¼ˆå£è¯­ï¼Œæ ‡å‡†è¯­: membuatï¼‰',
        'bikinin': 'åšç»™ï¼ˆå£è¯­ï¼Œæ ‡å‡†è¯­: membuatkanï¼‰',
        'maafin': 'åŸè°…ï¼ˆå£è¯­ï¼Œæ ‡å‡†è¯­: memaafkanï¼‰',
        'benerin': 'ä¿®å¤ï¼ˆå£è¯­ï¼Œæ ‡å‡†è¯­: membetulkan/memperbaikiï¼‰',
        'tungguin': 'ç­‰å¾…ï¼ˆå£è¯­ï¼Œæ ‡å‡†è¯­: menungguï¼‰',
        'dengerin': 'å¬ï¼ˆå£è¯­ï¼Œæ ‡å‡†è¯­: mendengarkanï¼‰',
        'bantuin': 'å¸®åŠ©ï¼ˆå£è¯­ï¼Œæ ‡å‡†è¯­: membantuï¼‰',
        'ikutin': 'è·Ÿéšï¼ˆå£è¯­ï¼Œæ ‡å‡†è¯­: mengikutiï¼‰',
        'ajakin': 'é‚€è¯·ï¼ˆå£è¯­ï¼Œæ ‡å‡†è¯­: mengajakï¼‰',
        'tanyain': 'è¯¢é—®ï¼ˆå£è¯­ï¼Œæ ‡å‡†è¯­: menanyakanï¼‰',
        
        # å£è¯­ç¨‹åº¦å‰¯è¯
        'banget': 'éå¸¸ï¼ˆå£è¯­ï¼Œæ ‡å‡†è¯­: sangat/sekaliï¼‰',
        'bgt': 'éå¸¸ï¼ˆå£è¯­ç¼©å†™ï¼Œæ ‡å‡†è¯­: sangatï¼‰',
        'bngt': 'éå¸¸ï¼ˆå£è¯­ç¼©å†™ï¼Œæ ‡å‡†è¯­: sangatï¼‰',
        'bener': 'çœŸçš„ï¼ˆå£è¯­ï¼Œæ ‡å‡†è¯­: benarï¼‰',
        'bnr': 'çœŸçš„ï¼ˆå£è¯­ç¼©å†™ï¼Œæ ‡å‡†è¯­: benarï¼‰',
        
        # å£è¯­è¿è¯/åŠ©è¯
        'emang': 'ç¡®å®ï¼ˆå£è¯­ï¼Œæ ‡å‡†è¯­: memangï¼‰',
        'emg': 'ç¡®å®ï¼ˆå£è¯­ç¼©å†™ï¼Œæ ‡å‡†è¯­: memangï¼‰',
        'sih': 'å‘¢/å•Šï¼ˆå£è¯­è¯­æ°”åŠ©è¯ï¼‰',
        'deh': 'å§ï¼ˆå£è¯­è¯­æ°”åŠ©è¯ï¼‰',
        'dong': 'å˜›ï¼ˆå£è¯­è¯­æ°”åŠ©è¯ï¼‰',
        'kok': 'æ€ä¹ˆï¼ˆå£è¯­ç–‘é—®è¯ï¼‰',
        'dunk': 'å˜›ï¼ˆå£è¯­ï¼Œæ ‡å‡†è¯­: dongï¼‰',
        'ya': 'å¥½å—/å§ï¼ˆå£è¯­è¯­æ°”åŠ©è¯ï¼‰',
        'yah': 'å•Šï¼ˆå£è¯­å¹è¯ï¼‰',
        'kan': 'ä¸æ˜¯å—ï¼ˆå£è¯­åŠ©è¯ï¼Œæ ‡å‡†è¯­: bukanï¼‰',
        'sumpah': 'å‘èª“ï¼ˆå£è¯­å¼ºè°ƒè¯ï¼‰',
        
        # å£è¯­ç–‘é—®è¯
        'gimana': 'æ€ä¹ˆæ ·ï¼ˆå£è¯­ï¼Œæ ‡å‡†è¯­: bagaimanaï¼‰',
        'gmn': 'æ€ä¹ˆæ ·ï¼ˆå£è¯­ç¼©å†™ï¼Œæ ‡å‡†è¯­: bagaimanaï¼‰',
        'kenapa': 'ä¸ºä»€ä¹ˆï¼ˆå£è¯­ï¼Œæ ‡å‡†è¯­: mengapaï¼‰',
        'knp': 'ä¸ºä»€ä¹ˆï¼ˆå£è¯­ç¼©å†™ï¼‰',
        'kayak': 'åƒï¼ˆå£è¯­ï¼Œæ ‡å‡†è¯­: sepertiï¼‰',
        'kaya': 'åƒï¼ˆå£è¯­ï¼Œæ ‡å‡†è¯­: sepertiï¼‰',
        
        # å…¶ä»–å¸¸è§å£è¯­è¯
        'aja': 'å°±/åªï¼ˆå£è¯­ï¼Œæ ‡å‡†è¯­: sajaï¼‰',
        'aj': 'å°±/åªï¼ˆå£è¯­ç¼©å†™ï¼Œæ ‡å‡†è¯­: sajaï¼‰',
        'nih': 'è¿™ï¼ˆå£è¯­ï¼Œæ ‡å‡†è¯­: iniï¼‰',
        'tuh': 'é‚£ï¼ˆå£è¯­ï¼Œæ ‡å‡†è¯­: ituï¼‰',
        'gitu': 'é‚£æ ·ï¼ˆå£è¯­ï¼Œæ ‡å‡†è¯­: begituï¼‰',
        'gini': 'è¿™æ ·ï¼ˆå£è¯­ï¼Œæ ‡å‡†è¯­: beginiï¼‰',
        'cuma': 'åªï¼ˆå£è¯­ï¼Œæ ‡å‡†è¯­: hanyaï¼‰',
        'doang': 'åªï¼ˆå£è¯­ï¼‰',
        'ama': 'å’Œï¼ˆå£è¯­ï¼Œæ ‡å‡†è¯­: denganï¼‰',
        'ma': 'å’Œï¼ˆå£è¯­ç¼©å†™ï¼‰',
        'sama': 'å’Œï¼ˆå£è¯­ï¼Œæ ‡å‡†è¯­: denganï¼‰',
        
        # ä¿šè¯­å’Œæ„Ÿå¹è¯
        'bete': 'çƒ¦èºï¼ˆå£è¯­/ä¿šè¯­ï¼‰',
        'kesel': 'çƒ¦æ¼/ç”Ÿæ°”ï¼ˆä¿šè¯­ï¼‰',
        'dodol': 'ç¬¨è›‹ï¼ˆå£è¯­/ä¿šè¯­ï¼‰',
        'beres': 'æå®šï¼ˆå£è¯­ï¼Œæ ‡å‡†è¯­: selesaiï¼‰',
        'kacau': 'ç³Ÿç³•ï¼ˆå£è¯­ï¼‰',
        'gara-gara': 'å› ä¸ºï¼ˆå£è¯­ï¼Œæ ‡å‡†è¯­: karenaï¼‰',
        'soalnya': 'å› ä¸ºï¼ˆå£è¯­ï¼Œæ ‡å‡†è¯­: karenaï¼‰',
        'makanya': 'æ‰€ä»¥ï¼ˆå£è¯­ï¼Œæ ‡å‡†è¯­: oleh karena ituï¼‰',
        'kelewat': 'è¿‡åˆ†/é”™è¿‡ï¼ˆå£è¯­ï¼Œæ ‡å‡†è¯­: terlalu/melewatkanï¼‰',
        
        # å£è¯­é‚€è¯·/å‘¼å”¤è¯
        'yuk': 'æ¥å§ï¼ˆå£è¯­é‚€è¯·è¯ï¼‰',
        'yo': 'å˜¿ï¼ˆè‹±è¯­å¤–æ¥å£è¯­æ‰“æ‹›å‘¼ï¼‰',
        'ayo': 'æ¥å§ï¼ˆå£è¯­é‚€è¯·è¯ï¼‰',
        'hei': 'å˜¿ï¼ˆå£è¯­æ‰“æ‹›å‘¼ï¼‰',
        'hai': 'å—¨ï¼ˆå£è¯­æ‰“æ‹›å‘¼ï¼‰',
        
        # å£è¯­ç¼©å†™
        'gt': 'é‚£æ ·ï¼ˆå£è¯­ç¼©å†™ï¼Œæ ‡å‡†è¯­: begituï¼‰',
        'bc': 'å› ä¸ºï¼ˆå£è¯­ç¼©å†™ï¼Œæ ‡å‡†è¯­: karenaï¼‰',
        'yg': 'çš„ï¼ˆä¹¦é¢ç¼©å†™ï¼Œä½†å¸¸ç”¨äºå£è¯­ï¼Œæ ‡å‡†è¯­: yangï¼‰',
        'dgn': 'å’Œï¼ˆä¹¦é¢ç¼©å†™ï¼Œä½†å¸¸ç”¨äºå£è¯­ï¼Œæ ‡å‡†è¯­: denganï¼‰',
        
        # ç½‘ç»œ/å¹´è½»äººå£è¯­
        'asik': 'å¥½ç©ï¼ˆå£è¯­ï¼Œæ ‡å‡†è¯­: asyikï¼‰',
        'mantap': 'å¾ˆæ£’ï¼ˆå£è¯­ï¼‰',
        'mantep': 'å¾ˆæ£’ï¼ˆå£è¯­ï¼‰',
        'keren': 'é…·ï¼ˆå£è¯­ï¼‰',
        'oke': 'å¥½ï¼ˆå£è¯­ï¼Œæ ‡å‡†è¯­: baikï¼‰',
        'ok': 'å¥½ï¼ˆå£è¯­ç¼©å†™ï¼‰',
        
        # è‹±è¯­å¤–æ¥å£è¯­
        'please': 'æ‹œæ‰˜ï¼ˆè‹±è¯­å¤–æ¥å£è¯­ï¼‰',
        'happy': 'å¼€å¿ƒï¼ˆè‹±è¯­å¤–æ¥å£è¯­ï¼‰',
        'love': 'çˆ±ï¼ˆè‹±è¯­å¤–æ¥å£è¯­ï¼‰',
        'sorry': 'æŠ±æ­‰ï¼ˆè‹±è¯­å¤–æ¥å£è¯­ï¼‰',
        'thanks': 'è°¢è°¢ï¼ˆè‹±è¯­å¤–æ¥å£è¯­ï¼‰',
        'cool': 'é…·ï¼ˆè‹±è¯­å¤–æ¥å£è¯­ï¼‰',
        'wow': 'å“‡ï¼ˆè‹±è¯­å¤–æ¥å£è¯­ï¼‰',
        'yeah': 'è€¶ï¼ˆè‹±è¯­å¤–æ¥å£è¯­ï¼‰',
        'yup': 'æ˜¯çš„ï¼ˆè‹±è¯­å¤–æ¥å£è¯­ï¼‰',
        'bye': 'å†è§ï¼ˆè‹±è¯­å¤–æ¥å£è¯­ï¼‰',
        
        # å…¶ä»–å¸¸ç”¨å£è¯­
        'traktir': 'è¯·å®¢ï¼ˆå£è¯­ï¼‰',
        'siapin': 'å‡†å¤‡ï¼ˆå£è¯­ï¼Œæ ‡å‡†è¯­: menyiapkanï¼‰',
        'jadian': 'æˆä¸ºæƒ…ä¾£ï¼ˆå£è¯­ï¼‰',
        'galau': 'è¿·èŒ«/çº ç»“ï¼ˆå£è¯­ï¼‰',
        'lebay': 'å¤¸å¼ ï¼ˆå£è¯­ï¼‰',
    }
    
    # ============ å›ºå®šç¤¼è²Œç”¨è¯­/æ­£å¼çŸ­è¯­ï¼ˆéœ€è¦æ’é™¤çš„ï¼‰ ============
    
    FORMAL_PHRASES = {
        ('terima', 'kasih'): ['kasih'],
        ('sama', 'sama'): ['sama'],
        ('selamat', 'pagi'): [],
        ('selamat', 'siang'): [],
        ('selamat', 'malam'): [],
        ('mohon', 'maaf'): [],
        ('dengan', 'hormat'): [],
        ('dengan', 'segala', 'kerendahan', 'hati'): [],
        ('atas', 'perhatian', 'dan', 'pengertian', 'anda'): [],  # â­ æ–°å¢
    }
    
    # ============ æŸ¥æ‰¾å£è¯­åŒ–è¡¨è¾¾ ============
    
    text_lower = text.lower()
    found_colloquial = []
    found_positions = set()
    excluded_positions = set()
    
    # ç¬¬ä¸€æ­¥ï¼šæ ‡è®°æ‰€æœ‰å›ºå®šç¤¼è²Œç”¨è¯­ä¸­éœ€è¦æ’é™¤çš„è¯
    for phrase_tuple, exclude_words in FORMAL_PHRASES.items():
        # æ„å»ºæ­£åˆ™è¡¨è¾¾å¼ï¼šå¤„ç†å¤šè¯çŸ­è¯­
        if len(phrase_tuple) == 2:
            word1, word2 = phrase_tuple
            phrase_pattern = r'\b' + re.escape(word1) + r'[\s\-]*' + re.escape(word2) + r'\b'
        elif len(phrase_tuple) == 3:
            word1, word2, word3 = phrase_tuple
            phrase_pattern = r'\b' + re.escape(word1) + r'[\s\-]*' + re.escape(word2) + r'[\s\-]*' + re.escape(word3) + r'\b'
        elif len(phrase_tuple) == 4:
            word1, word2, word3, word4 = phrase_tuple
            phrase_pattern = r'\b' + re.escape(word1) + r'[\s\-]*' + re.escape(word2) + r'[\s\-]*' + re.escape(word3) + r'[\s\-]*' + re.escape(word4) + r'\b'
        elif len(phrase_tuple) == 5:
            word1, word2, word3, word4, word5 = phrase_tuple
            phrase_pattern = r'\b' + re.escape(word1) + r'[\s\-]*' + re.escape(word2) + r'[\s\-]*' + re.escape(word3) + r'[\s\-]*' + re.escape(word4) + r'[\s\-]*' + re.escape(word5) + r'\b'
        else:
            continue
        
        phrase_matches = re.finditer(phrase_pattern, text_lower, re.IGNORECASE)
        
        for phrase_match in phrase_matches:
            phrase_start = phrase_match.start()
            phrase_end = phrase_match.end()
            
            for exclude_word in exclude_words:
                word_pattern = r'\b' + re.escape(exclude_word) + r'\b'
                word_matches = re.finditer(word_pattern, text_lower[phrase_start:phrase_end])
                
                for word_match in word_matches:
                    actual_pos = phrase_start + word_match.start()
                    excluded_positions.add(actual_pos)
    
    # ç¬¬äºŒæ­¥ï¼šæŸ¥æ‰¾æ‰€æœ‰å£è¯­åŒ–è¡¨è¾¾
    for colloquial_word, description in COLLOQUIAL_EXPRESSIONS.items():
        pattern = r'\b' + re.escape(colloquial_word) + r'\b'
        matches = re.finditer(pattern, text_lower)
        
        for match in matches:
            start_pos = match.start()
            end_pos = match.end()
            
            if start_pos in excluded_positions:
                continue
            
            if start_pos in found_positions:
                continue
            
            # ç‰¹æ®Šä¸Šä¸‹æ–‡æ£€æŸ¥ï¼škasih
            if colloquial_word == 'kasih':
                context_before = text_lower[max(0, start_pos-10):start_pos].strip()
                if context_before.endswith('terima'):
                    continue
            
            # ç‰¹æ®Šä¸Šä¸‹æ–‡æ£€æŸ¥ï¼šsama
            if colloquial_word == 'sama':
                context_before = text_lower[max(0, start_pos-7):start_pos].strip()
                context_after = text_lower[end_pos:end_pos+7].strip()
                
                if context_before.endswith('sama') or context_after.startswith('sama'):
                    broader_context = text_lower[max(0, start_pos-50):min(len(text), end_pos+50)]
                    if any(formal_word in broader_context for formal_word in ['hormat', 'yth', 'dengan segala']):
                        continue
            
            original_word = text[start_pos:end_pos]
            
            found_colloquial.append({
                'word': original_word,
                'word_lower': colloquial_word,
                'description': description,
                'position': start_pos,
                'context': text[max(0, start_pos-30):min(len(text), end_pos+30)],
                'category': _categorize_colloquial(colloquial_word, description)
            })
            
            found_positions.add(start_pos)
    
    found_colloquial.sort(key=lambda x: x['position'])
    
    # ============ ç»Ÿè®¡ ============
    
    total_count = len(found_colloquial)
    word_counter = Counter([item['word_lower'] for item in found_colloquial])
    category_counter = Counter([item['category'] for item in found_colloquial])
    
    # ============ åˆ¤æ–­æ˜¯å¦é€šè¿‡ ============
    
    passed = (total_count == exact_count)
    
    # ============ ç”Ÿæˆè¯¦ç»†è¯´æ˜ ============
    
    detail_parts = []
    
    if passed:
        detail_parts.append(f"âœ… æ­£ç¡®ï¼šæ‰¾åˆ°æ­£å¥½ {total_count} ä¸ªå£è¯­åŒ–è¡¨è¾¾ï¼ˆè¦æ±‚æ­£å¥½ {exact_count} ä¸ªï¼‰\n")
        
        detail_parts.append("æ‰¾åˆ°çš„å£è¯­åŒ–è¡¨è¾¾ï¼š")
        for i, item in enumerate(found_colloquial, 1):
            detail_parts.append(f"  {i}. {item['word']} - {item['description']} (ä½ç½® {item['position']})")
        
        if len(word_counter) < total_count:
            detail_parts.append(f"\nè¯é¢‘ç»Ÿè®¡ï¼š")
            for word, count in word_counter.most_common(10):
                if count > 1:
                    detail_parts.append(f"  - {word}: {count}æ¬¡")
    
    else:
        if total_count < exact_count:
            shortage = exact_count - total_count
            detail_parts.append(f"âŒ é”™è¯¯ï¼šåªæ‰¾åˆ° {total_count} ä¸ªå£è¯­åŒ–è¡¨è¾¾ï¼Œå°‘äºè¦æ±‚çš„ {exact_count} ä¸ªï¼ˆè¿˜å·® {shortage} ä¸ªï¼‰\n")
        else:
            excess = total_count - exact_count
            detail_parts.append(f"âŒ é”™è¯¯ï¼šæ‰¾åˆ° {total_count} ä¸ªå£è¯­åŒ–è¡¨è¾¾ï¼Œè¶…è¿‡è¦æ±‚çš„ {exact_count} ä¸ªï¼ˆå¤šäº† {excess} ä¸ªï¼‰\n")
        
        if found_colloquial:
            detail_parts.append("å·²æ‰¾åˆ°çš„å£è¯­åŒ–è¡¨è¾¾ï¼š")
            for i, item in enumerate(found_colloquial, 1):
                detail_parts.append(f"  {i}. {item['word']} - {item['description']} (ä½ç½® {item['position']})")
            
            if len(word_counter) < total_count:
                detail_parts.append(f"\nè¯é¢‘ç»Ÿè®¡ï¼š")
                for word, count in word_counter.most_common(10):
                    if count > 1:
                        detail_parts.append(f"  - {word}: {count}æ¬¡")
    
    detail = '\n'.join(detail_parts)
    
    # ============ ç»Ÿè®¡æ•°æ® ============
    
    stats = {
        'total_count': total_count,
        'required_count': exact_count,
        'difference': total_count - exact_count,
        'passed': passed,
        'colloquial_words': [item['word'] for item in found_colloquial],
        'word_counts': dict(word_counter),
        'category_counts': dict(category_counter),
        'unique_count': len(word_counter),
        'all_found': found_colloquial,
        'excluded_count': len(excluded_positions),
        'check_mode': 'exact_match_enhanced_v4'
    }
    
    return passed, detail, stats


def _categorize_colloquial(word: str, description: str) -> str:
    """å°†å£è¯­åŒ–è¡¨è¾¾åˆ†ç±»"""
    if 'äººç§°' in description or word in ['gue', 'gua', 'gw', 'lo', 'lu', 'loe']:
        return 'å£è¯­äººç§°'
    elif 'å¦å®š' in description or word in ['gak', 'nggak', 'ga', 'ngga']:
        return 'å£è¯­å¦å®š'
    elif 'åŠ¨è¯' in description or word.startswith('ng') or word.endswith('in') or word in ['bikin', 'kasih', 'buat', 'nyesel', 'pengen']:
        return 'å£è¯­åŠ¨è¯'
    elif 'ç¨‹åº¦' in description or word in ['banget', 'bener']:
        return 'ç¨‹åº¦å‰¯è¯'
    elif 'åŠ©è¯' in description or word in ['sih', 'deh', 'dong', 'kok', 'ya', 'yah']:
        return 'è¯­æ°”åŠ©è¯'
    elif 'æ—¶é—´' in description or word in ['udah', 'ntar', 'bakal']:
        return 'æ—¶é—´å‰¯è¯'
    elif 'ä¿šè¯­' in description or word in ['bete', 'dodol', 'kacau', 'kesel']:
        return 'ä¿šè¯­'
    elif 'è‹±è¯­' in description:
        return 'è‹±è¯­å¤–æ¥å£è¯­'
    elif 'é‚€è¯·' in description or word in ['yuk', 'ayo']:
        return 'é‚€è¯·è¯'
    else:
        return 'å…¶ä»–å£è¯­'


# ==================== å°å°¼è¯­åŒ…å«ç¤¼è²Œè¯çš„ç¤¼è²Œç¥ˆä½¿å¥æ£€æµ‹ï¼ˆç²¾ç¡®æ•°é‡ç‰ˆ - ä¿®å¤ç‰ˆï¼‰====================

import re
from typing import Tuple, Dict, List
from collections import Counter

def check_polite_imperatives(text: str, exact_count: int = 3) -> Tuple[bool, str, Dict]:
    """
    æ£€æµ‹å°å°¼è¯­åŒ…å«ç¤¼è²Œè¯çš„ç¤¼è²Œç¥ˆä½¿å¥
    
    ä¸¥æ ¼å®šä¹‰ï¼šå¿…é¡»åŒ…å«ç¤¼è²Œè¯ï¼ˆtolong, mohon, silakanç­‰ï¼‰çš„ç¥ˆä½¿å¥
    ä¸¥æ ¼æ¨¡å¼ï¼šå¿…é¡»æ­£å¥½ç­‰äº exact_countï¼Œä¸èƒ½å¤šä¹Ÿä¸èƒ½å°‘
    
    æ”¹è¿›ï¼šè¯†åˆ«ç”¨ dan è¿æ¥çš„å¤šä¸ªç¤¼è²Œç¥ˆä½¿å¥ï¼ˆå¦‚ "silakan duduk dan mari kita rayakan"ï¼‰
    
    åŒ…å«ç¤¼è²Œè¯çš„ç¥ˆä½¿å¥ç»“æ„ï¼š
    - ç¤¼è²Œè¯ + åŠ¨è¯ï¼šsilakan maafkan (è¯·åŸè°…)
    - ç¤¼è²Œè¯ + åŠ¨è¯ + å®¾è¯­ï¼šmohon beri kesempatan (æ³è¯·ç»™æœºä¼š)
    - ç¤¼è²Œè¯ + åŠ¨è¯çŸ­è¯­ï¼štolong dengarkan penjelasan ini (è¯·å¬è¿™ä¸ªè§£é‡Š)
    
    ç¤¼è²Œè¯åŒ…æ‹¬ï¼štolong, mohon, silakan, harap, mari, sudilah, semoga, kiranya
    
    Args:
        text: è¦æ£€æµ‹çš„æ–‡æœ¬
        exact_count: è¦æ±‚çš„ç²¾ç¡®æ•°é‡
    
    Returns:
        (æ˜¯å¦é€šè¿‡, è¯¦ç»†è¯´æ˜, ç»Ÿè®¡æ•°æ®)
    """
    
    # ============ ç±»å‹æ£€æŸ¥å’Œè½¬æ¢ ============
    if text is None:
        return False, "âŒ é”™è¯¯ï¼šè¾“å…¥æ–‡æœ¬ä¸ºç©ºï¼ˆNoneï¼‰", {
            'total_count': 0,
            'required_count': exact_count,
            'passed': False,
            'imperatives': [],
            'by_marker': {}
        }
    
    # å¤„ç†åˆ—è¡¨ç±»å‹
    if isinstance(text, list):
        text_parts = []
        for item in text:
            if isinstance(item, str):
                text_parts.append(item)
            elif isinstance(item, dict):
                for key in ['text', 'content', 'message', 'response']:
                    if key in item and isinstance(item[key], str):
                        text_parts.append(item[key])
                        break
            else:
                text_parts.append(str(item))
        text = ' '.join(text_parts)
    
    if not isinstance(text, str):
        try:
            text = str(text)
        except:
            return False, f"âŒ é”™è¯¯ï¼šæ— æ³•è½¬æ¢ä¸ºå­—ç¬¦ä¸²", {
                'total_count': 0,
                'required_count': exact_count,
                'passed': False,
                'imperatives': [],
                'by_marker': {}
            }
    
    if not text.strip():
        return False, "âŒ é”™è¯¯ï¼šè¾“å…¥æ–‡æœ¬ä¸ºç©º", {
            'total_count': 0,
            'required_count': exact_count,
            'passed': False,
            'imperatives': [],
            'by_marker': {}
        }
    
    # ============ ç¤¼è²Œè¯å®šä¹‰ ============
    
    POLITE_WORDS = {
        'tolong': 'è¯·ï¼ˆå¸®å¿™ï¼‰',
        'mohon': 'æ³è¯·',
        'silakan': 'è¯·',
        'harap': 'å¸Œæœ›/è¯·',
        'mari': 'è®©æˆ‘ä»¬',
        'sudilah': 'è¯·ï¼ˆæ­£å¼ï¼‰',
        'semoga': 'å¸Œæœ›',
        'kiranya': 'å¸Œæœ›ï¼ˆæ­£å¼ï¼‰',
    }
    
    # ============ æ’é™¤æ¨¡å¼ï¼šä¸ç®—ç¥ˆä½¿å¥çš„å›ºå®šçŸ­è¯­ ============
    
    EXCLUDED_PHRASES = [
        r'\bsaya\s+mohon\s+maaf\b',      # "saya mohon maaf" = æˆ‘é“æ­‰ï¼ˆé™ˆè¿°å¥ï¼‰
        r'\bkami\s+mohon\s+maaf\b',      # "kami mohon maaf" = æˆ‘ä»¬é“æ­‰
        r'\bkita\s+mohon\s+maaf\b',      # "kita mohon maaf" = æˆ‘ä»¬é“æ­‰
        r'\bdia\s+mohon\s+maaf\b',       # "dia mohon maaf" = ä»–/å¥¹é“æ­‰
        r'\bmereka\s+mohon\s+maaf\b',    # "mereka mohon maaf" = ä»–ä»¬é“æ­‰
    ]
    
    # ============ æŸ¥æ‰¾åŒ…å«ç¤¼è²Œè¯çš„ç¥ˆä½¿å¥ ============
    
    text_lower = text.lower()
    found_imperatives = []
    marker_counts = {word: 0 for word in POLITE_WORDS.keys()}
    
    # æŸ¥æ‰¾æ‰€æœ‰å¯èƒ½çš„åŒ¹é…
    all_matches = []
    
    for polite_word, meaning in POLITE_WORDS.items():
        # æ¨¡å¼ï¼šç¤¼è²Œè¯ + åç»­åŠ¨è¯çŸ­è¯­ï¼ˆåˆ° dan, å¥å·, é€—å·ç­‰ä¸ºæ­¢ï¼‰
        # æ”¹è¿›ï¼šåŒ¹é…åˆ° dan ä¹‹å‰å°±åœæ­¢ï¼Œé¿å…è·¨è¶Šå¤šä¸ªç¥ˆä½¿å¥
        pattern = r'\b' + re.escape(polite_word) + r'\s+([^.!,\n]+?)(?=\s+dan\s+|\.|!|,|\n|$)'
        
        matches = re.finditer(pattern, text_lower, re.IGNORECASE)
        
        for match in matches:
            start_pos = match.start()
            end_pos = match.end()
            
            full_phrase = text[start_pos:end_pos].strip()
            verb_part = match.group(1).strip()
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯æ’é™¤çš„å›ºå®šçŸ­è¯­ï¼ˆé™ˆè¿°å¥ï¼‰
            is_excluded = False
            for excluded_pattern in EXCLUDED_PHRASES:
                context_start = max(0, start_pos - 20)
                context = text_lower[context_start:end_pos + 20]
                if re.search(excluded_pattern, context):
                    is_excluded = True
                    break
            
            if is_excluded:
                continue
            
            # æ£€æŸ¥æ˜¯å¦åŒ…å«æœ‰æ•ˆåŠ¨è¯ï¼ˆè‡³å°‘2ä¸ªå­—æ¯ï¼‰
            if len(verb_part) < 2:
                continue
            
            all_matches.append({
                'phrase': full_phrase,
                'full_sentence': full_phrase,  # è¿™é‡Œè®°å½•å®é™…çš„ç¥ˆä½¿å¥éƒ¨åˆ†
                'polite_word': polite_word,
                'word_meaning': meaning,
                'verb': verb_part,
                'position': start_pos,
                'end_position': end_pos,
            })
    
    # ============ å»é‡å¤„ç†ï¼ˆé¿å…é‡å åŒ¹é…ï¼‰============
    
    all_matches.sort(key=lambda x: x['position'])
    
    filtered_matches = []
    used_positions = set()
    
    for match in all_matches:
        start = match['position']
        end = match['end_position']
        
        # æ£€æŸ¥æ˜¯å¦ä¸å·²æœ‰åŒ¹é…é‡å 
        is_overlapping = False
        for prev_match in filtered_matches:
            prev_start = prev_match['position']
            prev_end = prev_match['end_position']
            
            # å¦‚æœèµ·å§‹ä½ç½®åœ¨ä¹‹å‰åŒ¹é…çš„èŒƒå›´å†…ï¼Œè®¤ä¸ºæ˜¯é‡å 
            if prev_start <= start < prev_end:
                is_overlapping = True
                break
            
            # å¦‚æœæœ‰ä»»ä½•ä½ç½®é‡å ï¼Œä¹Ÿè®¤ä¸ºæ˜¯é‡å 
            if start < prev_end and end > prev_start:
                is_overlapping = True
                break
        
        if not is_overlapping:
            filtered_matches.append(match)
            # æ ‡è®°ä½¿ç”¨çš„ä½ç½®èŒƒå›´
            for pos in range(start, end):
                used_positions.add(pos)
    
    found_imperatives = filtered_matches
    found_imperatives.sort(key=lambda x: x['position'])
    
    # ç»Ÿè®¡å„ç¤¼è²Œè¯çš„ä½¿ç”¨æ¬¡æ•°
    for item in found_imperatives:
        marker_counts[item['polite_word']] += 1
    
    # ============ ç»Ÿè®¡ ============
    
    total_count = len(found_imperatives)
    passed = (total_count == exact_count)
    
    # ============ ç”Ÿæˆè¯¦ç»†è¯´æ˜ ============
    
    detail_parts = []
    
    if passed:
        detail_parts.append(f"âœ… æ­£ç¡®ï¼šæ‰¾åˆ°æ­£å¥½ {total_count} ä¸ªåŒ…å«ç¤¼è²Œè¯çš„ç¥ˆä½¿å¥ï¼ˆè¦æ±‚æ­£å¥½ {exact_count} ä¸ªï¼‰\n")
        
        detail_parts.append("æ‰¾åˆ°çš„åŒ…å«ç¤¼è²Œè¯çš„ç¥ˆä½¿å¥ï¼š")
        for i, item in enumerate(found_imperatives, 1):
            detail_parts.append(f"\n  {i}. ç¤¼è²Œè¯ï¼šã€{item['polite_word']}ã€‘ï¼ˆ{item['word_meaning']}ï¼‰")
            detail_parts.append(f"     å®Œæ•´å¥å­: {item['full_sentence']}")
        
        # æŒ‰ç¤¼è²Œè¯åˆ†ç±»ç»Ÿè®¡
        active_markers = {k: v for k, v in marker_counts.items() if v > 0}
        if active_markers:
            detail_parts.append(f"\nä½¿ç”¨çš„ç¤¼è²Œè¯ç»Ÿè®¡ï¼š")
            for word, count in active_markers.items():
                detail_parts.append(f"  - {word}: {count} æ¬¡")
    
    else:
        if total_count < exact_count:
            shortage = exact_count - total_count
            detail_parts.append(f"âŒ é”™è¯¯ï¼šåªæ‰¾åˆ° {total_count} ä¸ªåŒ…å«ç¤¼è²Œè¯çš„ç¥ˆä½¿å¥ï¼Œå°‘äºè¦æ±‚çš„ {exact_count} ä¸ªï¼ˆè¿˜å·® {shortage} ä¸ªï¼‰\n")
        else:
            excess = total_count - exact_count
            detail_parts.append(f"âŒ é”™è¯¯ï¼šæ‰¾åˆ° {total_count} ä¸ªåŒ…å«ç¤¼è²Œè¯çš„ç¥ˆä½¿å¥ï¼Œè¶…è¿‡è¦æ±‚çš„ {exact_count} ä¸ªï¼ˆå¤šäº† {excess} ä¸ªï¼‰\n")
        
        if found_imperatives:
            detail_parts.append("å·²æ‰¾åˆ°çš„åŒ…å«ç¤¼è²Œè¯çš„ç¥ˆä½¿å¥ï¼š")
            for i, item in enumerate(found_imperatives, 1):
                detail_parts.append(f"\n  {i}. ç¤¼è²Œè¯ï¼šã€{item['polite_word']}ã€‘ï¼ˆ{item['word_meaning']}ï¼‰")
                detail_parts.append(f"     å®Œæ•´å¥å­: {item['full_sentence']}")
        else:
            detail_parts.append("æœªæ‰¾åˆ°ä»»ä½•åŒ…å«ç¤¼è²Œè¯çš„ç¥ˆä½¿å¥")
    
    detail = '\n'.join(detail_parts)
    
    # ============ ç»Ÿè®¡æ•°æ® ============
    
    stats = {
        'total_count': total_count,
        'required_count': exact_count,
        'difference': total_count - exact_count,
        'passed': passed,
        'imperatives': [item['full_sentence'] for item in found_imperatives],
        'by_marker': marker_counts,
        'unique_markers': len([v for v in marker_counts.values() if v > 0]),
        'all_found': found_imperatives,
        'check_mode': 'exact_match'
    }
    
    return passed, detail, stats


# ============ æµ‹è¯•ä»£ç  ============
if __name__ == "__main__":
    # æµ‹è¯•åŒ…å« dan è¿æ¥çš„å¤šä¸ªç¤¼è²Œç¥ˆä½¿å¥
    test_text = """
    Selamat siang semuanya, terima kasih telah hadir di hari istimewa ini.
    Tolong luangkan waktu untuk saling berkenalan dan menikmati acara ini bersama.
    Mohon jangan ragu untuk berbagi cerita dan kebahagiaan hari ini.
    Silakan menikmati hidangan yang telah disiapkan dan mari kita rayakan hari yang penuh cinta ini.
    Terima kasih!
    """
    
    passed, detail, stats = check_polite_imperatives(test_text, exact_count=4)
    print("=" * 70)
    print("æµ‹è¯•ç»“æœï¼š")
    print(detail)
    print("=" * 70)
    print(f"\nç»Ÿè®¡æ•°æ®:")
    print(f"  - æ€»æ•°: {stats['total_count']}")
    print(f"  - è¦æ±‚: {stats['required_count']}")
    print(f"  - é€šè¿‡: {stats['passed']}")
    print(f"\næ‰¾åˆ°çš„ç¥ˆä½¿å¥:")
    for i, imp in enumerate(stats['imperatives'], 1):
        print(f"  {i}. {imp}")


    
def check_si_usage(text):
    """
    æ£€æŸ¥å°å°¼è¯­å† è¯ 'si' çš„ä½¿ç”¨æ˜¯å¦æ­£ç¡®
    è¿”å›: (score, message)
    """
    import re
    
    # å¤„ç†è¾“å…¥ç±»å‹
    if isinstance(text, list):
        text = ' '.join(str(item) for item in text)
    elif not isinstance(text, str):
        text = str(text)
    
    # æŸ¥æ‰¾æ‰€æœ‰ 'si' + åè¯çš„æ¨¡å¼
    si_pattern = r'\b[Ss]i\s+([a-zA-Z]+)'
    matches = re.findall(si_pattern, text)
    
    if not matches:
        return 1, "âœ“ æœªä½¿ç”¨ 'si' å† è¯"
    
    # ä¸åº”è¯¥ä¸ 'si' æ­é…çš„è¯ï¼ˆè¡¨ç¤ºå°Šè´µã€ç¥åœ£çš„è¯ï¼‰
    forbidden_with_si = [
        'raja', 'ratu', 'pangeran', 'putri', 'sultan',  # ç‹å®¤
        'kaisar', 'maharaja', 'permaisuri',
        'dewa', 'dewi', 'allah', 'tuhan',  # ç¥çµ
        'hakim', 'menteri', 'presiden', 'gubernur',  # é«˜å®˜
        'wali', 'bupati', 'walikota',
        'profesor', 'doktor', 'guru',  # å­¦æœ¯å°Šç§°
    ]
    
    errors = []
    
    # æ£€æŸ¥æ¯ä¸ªåŒ¹é…
    for match in matches:
        word_after_si = match.lower().strip()
        
        # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨äº†ä¸å½“çš„è¯
        is_forbidden = False
        for forbidden in forbidden_with_si:
            if word_after_si == forbidden or word_after_si.startswith(forbidden + ' '):
                errors.append(f"'si {match}'")
                is_forbidden = True
                break
        
        if is_forbidden:
            continue
    
    if errors:
        error_list = ", ".join(errors)
        return 0, f"âŒ 'si' ä½¿ç”¨ä¸å½“ï¼Œä¸åº”ç”¨äºå°Šè´µæˆ–ç¥åœ£çš„ç§°å‘¼: {error_list}ã€‚'si' åº”è¯¥ç”¨äºæ˜µç§°ã€å°åŠ¨ç‰©æˆ–å¸¦æœ‰äº²æ˜µ/ç•¥å¸¦è´¬ä¹‰çš„ç§°å‘¼ã€‚"
    
    return 1, f"âœ“ 'si' å† è¯ä½¿ç”¨æ­£ç¡®ï¼ˆå…± {len(matches)} å¤„ï¼‰"


def check_sang_usage(text):
    """
    æ£€æŸ¥å°å°¼è¯­å† è¯ 'sang' çš„ä½¿ç”¨æ˜¯å¦æ­£ç¡®
    è¿”å›: (score, message)
    """
    import re
    
    # å¤„ç†è¾“å…¥ç±»å‹
    if isinstance(text, list):
        text = ' '.join(str(item) for item in text)
    elif not isinstance(text, str):
        text = str(text)
    
    # æŸ¥æ‰¾æ‰€æœ‰ 'sang' + åè¯çš„æ¨¡å¼
    sang_pattern = r'\b[Ss]ang\s+([a-zA-Z\s]+?)(?=\.|,|!|\?|\s+[a-z]{2,}\s|$|\n)'
    matches = re.findall(sang_pattern, text)
    
    if not matches:
        return 1, "âœ“ æœªä½¿ç”¨ 'sang' å† è¯"
    
    # ä¸åº”è¯¥ä¸ 'sang' æ­é…çš„è¯ï¼ˆè¡¨ç¤ºå‘å¾®ã€æ¸ºå°çš„è¯ï¼‰
    inappropriate_with_sang = [
        'tikus', 'kecoa', 'lalat', 'nyamuk', 'kutu',  # å®³è™«å°åŠ¨ç‰©
        'cacing', 'ulat',  # å°è™«å­
        'sampah', 'kotoran',  # æ±¡ç§½ä¹‹ç‰©
        'semut kecil', 'kutu busuk', 'tikus got',  # ç»„åˆè´¬ä¹‰è¯
    ]
    
    # é€‚åˆä¸ 'sang' æ­é…çš„è¯ï¼ˆè¡¨ç¤ºå°Šè´µã€é‡è¦çš„è¯ï¼‰
    appropriate_with_sang = [
        'raja', 'ratu', 'pangeran', 'putri', 'sultan', 'kaisar',  # ç‹å®¤
        'harimau', 'singa', 'elang', 'naga', 'gajah', 'ular', 'buaya',  # å¨ä¸¥çš„åŠ¨ç‰©
        'pemimpin', 'komandan', 'ketua', 'kepala', 'jenderal',  # é¢†å¯¼è€…
        'matahari', 'bulan', 'bintang', 'langit', 'laut', 'angin',  # æ‹ŸäººåŒ–è‡ªç„¶
        'dewi', 'dewa', 'bidadari',  # ç¥çµ
        'pahlawan', 'pejuang', 'juara', 'pemenang',  # è‹±é›„
        'guru', 'bijak', 'arif', 'penyair', 'seniman',  # æ™ºè€…/è‰ºæœ¯å®¶
        'kancil', 'kelinci', 'burung', 'beruang', 'serigala',  # ç«¥è¯ä¸­å¸¸è§çš„ä¸»è§’åŠ¨ç‰©
        'rubah', 'kerbau', 'kuda', 'monyet', 'rusa',
        'kura', 'penyu', 'lumba',
    ]
    
    errors = []
    warnings = []
    
    # æ£€æŸ¥æ¯ä¸ªåŒ¹é…
    for match in matches:
        words_after_sang = match.lower().strip()
        
        # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨äº†æ˜ç¡®ä¸å½“çš„è¯
        found_inappropriate = False
        for inappropriate in inappropriate_with_sang:
            if inappropriate in words_after_sang:
                errors.append(f"'sang {match.strip()}'")
                found_inappropriate = True
                break
        
        if found_inappropriate:
            continue
        
        # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨äº†åˆé€‚çš„è¯
        is_appropriate = False
        for appropriate in appropriate_with_sang:
            if appropriate in words_after_sang:
                is_appropriate = True
                break
        
        # å¦‚æœæ²¡æœ‰åŒ¹é…åˆ°åˆé€‚çš„è¯ï¼Œä½†ä¹Ÿä¸æ˜¯æ˜ç¡®é”™è¯¯çš„ï¼Œç»™å‡ºè­¦å‘Š
        if not is_appropriate:
            first_word = words_after_sang.split()[0] if words_after_sang else ""
            # åªåœ¨ç‰¹å®šæƒ…å†µä¸‹ç»™å‡ºè­¦å‘Šï¼Œé¿å…è¿‡äºä¸¥æ ¼
            if len(first_word) > 0 and first_word not in ['yang', 'ini', 'itu']:
                warnings.append(f"'sang {match.strip()}'")
    
    if errors:
        error_list = ", ".join(errors)
        return 0, f"âŒ 'sang' ä½¿ç”¨ä¸å½“ï¼Œä¸åº”ç”¨äºå‘å¾®æˆ–ä¸å—å°Šæ•¬çš„äº‹ç‰©: {error_list}ã€‚'sang' åº”è¯¥ç”¨äºå°Šè´µçš„äººç‰©ã€å¨ä¸¥çš„åŠ¨ç‰©æˆ–æ‹ŸäººåŒ–çš„è‡ªç„¶ç°è±¡ã€‚"
    
    if warnings:
        # è­¦å‘Šä¸å½±å“å¾—åˆ†ï¼Œåªæ˜¯æç¤º
        warning_list = ", ".join(warnings)
        return 1, f"âœ“ 'sang' å† è¯ä½¿ç”¨åŸºæœ¬æ­£ç¡®ï¼ˆå…± {len(matches)} å¤„ï¼‰ï¼Œä½†è¯·æ³¨æ„: {warning_list} å¯èƒ½ä¸æ˜¯æœ€å…¸å‹çš„ç”¨æ³•"
    
    return 1, f"âœ“ 'sang' å† è¯ä½¿ç”¨æ­£ç¡®ï¼ˆå…± {len(matches)} å¤„ï¼‰"

# ==================== å°å°¼è¯­å®¾è¯­å‰ç½®å¼ºè°ƒå¥æ£€æµ‹ï¼ˆä¿®æ­£ç‰ˆï¼‰====================

import re
from typing import Tuple, Dict, List

def check_fronted_emphasis(text: str, exact_count: int = 1) -> Tuple[bool, str]:
    """
    æ£€æµ‹å°å°¼è¯­ä¸­çš„å®¾è¯­å‰ç½®å¼ºè°ƒå¥ï¼ˆåªæ£€æµ‹ä¸»å¥ï¼‰
    
    æ ¸å¿ƒå®šä¹‰ï¼š
    å°†æ­£å¸¸è¯­åºä¸­çš„å®¾è¯­æå‰åˆ°å¥é¦–ï¼Œç”¨é€—å·åˆ†éš”ï¼Œä»¥å¼ºè°ƒè¯¥å®¾è¯­ã€‚
    
    ç»“æ„ï¼š[å®¾è¯­åè¯çŸ­è¯­], [ä¸»è¯­] + [åŠç‰©åŠ¨è¯] + ...
    
    æ’é™¤ï¼š
    1. ç§°å‘¼è¯­
    2. çŠ¶è¯­
    3. ä¸»ç³»è¡¨ç»“æ„
    4. å›ºå®šè¡¨è¾¾
    5. "ä¸»è¯­, å…³ç³»ä»å¥, è°“è¯­" ç»“æ„ï¼ˆè¿™ä¸æ˜¯å®¾è¯­å‰ç½®ï¼‰
    
    ç¤ºä¾‹ï¼š
    âœ… Adik, aku ajak ke mal
       æ­£å¸¸ï¼šAku ajak adik ke mal
       å‰ç½®ï¼šä¸»å¥å®¾è¯­(adik)è¢«æå‰
    
    âœ… Hubungan kita, aku sangat menghargainya
       æ­£å¸¸ï¼šAku menghargai hubungan kita
       å‰ç½®ï¼šä¸»å¥å®¾è¯­(hubungan kita)è¢«æå‰
    
    âœ… Adik yang dekat denganku, aku ajak
       "yang dekat denganku" æ˜¯å®šè¯­ä»å¥ï¼Œä¿®é¥° adik
       ç®€åŒ–åï¼šAdik, aku ajakï¼ˆæ˜¯å®¾è¯­å‰ç½®ï¼‰
    
    âŒ Kesalahpahaman ini, yang mungkin membuatmu tidak nyaman, sangat aku sesali
       è¿™æ˜¯ "ä¸»è¯­, å…³ç³»ä»å¥æ’å…¥è¯­, è°“è¯­" ç»“æ„
       ä¸æ˜¯å®¾è¯­å‰ç½®
    
    âŒ Terima kasih, aku senang
       å›ºå®šè¡¨è¾¾ï¼Œä¸æ˜¯å®¾è¯­å‰ç½®
    
    Args:
        text: è¦æ£€æµ‹çš„æ–‡æœ¬
        exact_count: è¦æ±‚çš„ç²¾ç¡®æ•°é‡
    
    Returns:
        (æ˜¯å¦é€šè¿‡, è¯¦ç»†è¯´æ˜)
    """
    
    # ============ ç±»å‹æ£€æŸ¥å’Œè½¬æ¢ ============
    if text is None:
        return False, "âŒ é”™è¯¯ï¼šè¾“å…¥æ–‡æœ¬ä¸ºç©ºï¼ˆNoneï¼‰"
    
    if isinstance(text, list):
        text_parts = []
        for item in text:
            if isinstance(item, str):
                text_parts.append(item)
            elif isinstance(item, dict):
                for key in ['text', 'content', 'message', 'response']:
                    if key in item and isinstance(item[key], str):
                        text_parts.append(item[key])
                        break
            else:
                text_parts.append(str(item))
        text = ' '.join(text_parts)
    
    if not isinstance(text, str):
        try:
            text = str(text)
        except:
            return False, f"âŒ é”™è¯¯ï¼šæ— æ³•è½¬æ¢ä¸ºå­—ç¬¦ä¸²"
    
    if not text.strip():
        return False, "âŒ é”™è¯¯ï¼šè¾“å…¥æ–‡æœ¬ä¸ºç©º"
    
    # ============ ç§°å‘¼è¯­åˆ—è¡¨ï¼ˆéœ€è¦æ’é™¤ï¼‰============
    vocatives = {
        'sayang', 'kamu', 'sobat', 'teman', 'bapak', 'ibu', 'mas', 'mbak',
        'pak', 'bu', 'bang', 'kak', 'dik', 'nak', 'ananda', 'ayah', 'bunda',
        'saudara', 'saudari', 'tuan', 'nyonya', 'nona', 'adik', 'kakak',
        'yang terhormat', 'yth', 'kepada', 'dear'
    }
    
    # ============ å›ºå®šè¡¨è¾¾/æ„Ÿå¹è¯­ï¼ˆéœ€è¦æ’é™¤ï¼‰============
    fixed_expressions = [
        'terima kasih', 'selamat pagi', 'selamat siang', 'selamat sore', 
        'selamat malam', 'selamat datang', 'selamat tinggal', 'selamat jalan',
        'maaf', 'mohon maaf', 'permisi', 'assalamualaikum', 'waalaikumsalam',
        'salam sejahtera', 'halo', 'hai', 'sampai jumpa', 'salam', 'hormat saya'
    ]
    
    # ============ ç³»åŠ¨è¯åˆ—è¡¨ï¼ˆéœ€è¦æ’é™¤ä¸»ç³»è¡¨ç»“æ„ï¼‰============
    copula_verbs = {'adalah', 'ialah', 'yaitu', 'merupakan'}
    
    # ============ ä»£è¯åç¼€ ============
    pronoun_suffixes = {'ku', 'mu', 'nya', 'kah', 'lah'}
    
    # ============ åŠç‰©åŠ¨è¯åˆ—è¡¨ ============
    transitive_verbs = {
        # åŸºæœ¬å½¢å¼
        'ajak', 'pilih', 'beli', 'lihat', 'hargai', 'percaya', 'jelaskan', 'harap',
        'lupakan', 'buat', 'cari', 'kirim', 'terima', 'berikan', 'kasih',
        'cinta', 'sayang', 'tunggu', 'kenal', 'ingat', 'lupa', 'mengerti', 'pahami',
        'ketahui', 'tahu', 'minta', 'ambil', 'bawa', 'ucapkan', 'sampaikan',
        'sesali', 'rayakan', 'nikmati', 'jaga', 'upayakan', 'pergi',
        
        # me- å‰ç¼€
        'mengajak', 'memilih', 'membeli', 'melihat', 'menghargai', 'mempercayai',
        'menjelaskan', 'mengharapkan', 'melupakan', 'menginginkan', 'membuat',
        'mencari', 'mengirim', 'menerima', 'memberikan', 'mencintai', 'menyayangi',
        'menunggu', 'mengenal', 'mengingat', 'memahami', 'mengetahui', 'meminta',
        'mengambil', 'membawa', 'mengucapkan', 'menyampaikan',
        'menyesali', 'merayakan', 'menikmati', 'menjaga', 'mengupayakan',
        
        # å£è¯­å½¢å¼
        'kuajak', 'kupilih', 'kubeli', 'kulihat', 'kuhargai', 'kupercaya',
        'kujelaskan', 'kuharap', 'kulupakan', 'kucari', 'kubuat', 'kusesali',
        'kujaga',
        'kauajak', 'kaupilih', 'kaubeli',
    }
    
    # åŠ©åŠ¨è¯ï¼ˆéœ€è¦è·³è¿‡ï¼‰
    auxiliary_verbs = {
        'ingin', 'mau', 'akan', 'harus', 'bisa', 'dapat', 'boleh',
        'perlu', 'hendak', 'mesti', 'sudah', 'telah', 'sedang', 'masih',
        'pernah', 'sempat', 'bakal'
    }
    
    # ç¨‹åº¦å‰¯è¯ï¼ˆéœ€è¦è·³è¿‡ï¼‰
    adverbs = {
        'sangat', 'sekali', 'amat', 'benar', 'paling', 'lebih', 'kurang',
        'cukup', 'terlalu', 'agak', 'sedikit', 'banyak', 'selalu', 'sering',
        'jarang', 'kadang', 'pernah', 'benar-benar', 'sebenarnya', 'memang',
        'tidak', 'belum', 'jangan'
    }
    
    # ä¸»è¯­ä»£è¯
    subject_pronouns = {
        'aku', 'ku', 'saya', 'kamu', 'kau', 'mu', 'dia', 'ia',
        'kami', 'kita', 'kalian', 'mereka', 'beliau', 'anda'
    }
    
    # æ’é™¤çš„èµ·å§‹è¯ï¼ˆè¿™äº›æ˜¯çŠ¶è¯­ï¼Œä¸æ˜¯å®¾è¯­ï¼‰
    adverbial_starters = {
        'di', 'ke', 'dari', 'pada', 'untuk', 'dengan', 'tanpa',
        'ketika', 'saat', 'setelah', 'sebelum', 'kemarin', 'besok',
        'selama', 'sejak', 'sampai', 'hingga', 'tadi', 'nanti',
        'karena', 'sebab', 'meskipun', 'walaupun', 'jika', 'kalau'
    }
    
    found_patterns = []
    
    # ============ æ ¸å¿ƒæ”¹è¿›ï¼šæ£€æµ‹å¹¶æ’é™¤"ä¸»è¯­ + å…³ç³»ä»å¥ + è°“è¯­"ç»“æ„ ============
    def is_subject_with_relative_clause(sentence):
        """
        æ£€æµ‹æ˜¯å¦æ˜¯ "ä¸»è¯­, å…³ç³»ä»å¥, è°“è¯­" ç»“æ„
        ä¾‹å¦‚ï¼šKesalahpahaman ini, yang mungkin membuatmu tidak nyaman, sangat aku sesali
        
        ç‰¹å¾ï¼š
        1. ç¬¬ä¸€ä¸ªé€—å·åç´§è·Ÿ yang
        2. ç¬¬äºŒä¸ªé€—å·åæ˜¯è°“è¯­ï¼ˆæ²¡æœ‰ä¸»è¯­+åŠ¨è¯ç»“æ„ï¼‰
        
        è¿”å›ï¼šTrue è¡¨ç¤ºæ˜¯è¿™ç§ç»“æ„ï¼ˆéœ€è¦æ’é™¤ï¼‰
        """
        # åŒ¹é…æ¨¡å¼ï¼šåè¯, yang ..., åŠ¨è¯/å‰¯è¯
        pattern = r'^[^,]+,\s*yang\s+[^,]+,\s*(.+)$'
        match = re.match(pattern, sentence.strip(), re.IGNORECASE)
        
        if not match:
            return False
        
        # æ£€æŸ¥ç¬¬äºŒä¸ªé€—å·åçš„éƒ¨åˆ†
        after_second_comma = match.group(1).strip().lower()
        words = after_second_comma.split()
        
        if not words:
            return False
        
        # å¦‚æœç¬¬ä¸€ä¸ªè¯æ˜¯å‰¯è¯æˆ–å¦å®šè¯ï¼Œå¾ˆå¯èƒ½æ˜¯ "ä¸»è¯­, å…³ç³»ä»å¥, è°“è¯­" ç»“æ„
        # ä¾‹å¦‚ï¼šsangat aku sesali, tidak aku lupa
        first_word = words[0]
        if first_word in adverbs or first_word in {'tidak', 'belum', 'jangan'}:
            return True
        
        # å¦‚æœç›´æ¥æ˜¯åŠ¨è¯ï¼ˆæ²¡æœ‰ä¸»è¯­ï¼‰ï¼Œä¹Ÿå¯èƒ½æ˜¯è¿™ç§ç»“æ„
        # ä½†è¿™ç§æƒ…å†µè¾ƒå°‘è§ï¼Œæš‚ä¸å¤„ç†
        
        return False
    
    # ============ æ”¹è¿›ï¼šå¤„ç†å®šè¯­ä»å¥ ============
    def extract_main_clause_only(sentence):
        """
        æå–ä¸»å¥ï¼Œç§»é™¤å®šè¯­ä»å¥
        
        å¤„ç†ä¸¤ç§æƒ…å†µï¼š
        1. "åè¯ yang ..., ä¸»è¯­ åŠ¨è¯" -> "åè¯, ä¸»è¯­ åŠ¨è¯" ï¼ˆå®¾è¯­å‰ç½®ï¼‰
        2. "åè¯, yang ..., è°“è¯­" -> è¿”å› Noneï¼ˆä¸æ˜¯å®¾è¯­å‰ç½®ï¼Œæ˜¯ä¸»è¯­+å…³ç³»ä»å¥ï¼‰
        """
        
        # å…ˆæ£€æŸ¥æ˜¯å¦æ˜¯ "ä¸»è¯­ + å…³ç³»ä»å¥ + è°“è¯­" ç»“æ„
        if is_subject_with_relative_clause(sentence):
            return None  # è¿”å› None è¡¨ç¤ºéœ€è¦æ’é™¤
        
        # ç§»é™¤ "yang ... " æ ¼å¼çš„å®šè¯­ä»å¥ï¼ˆåœ¨é€—å·å‰ï¼‰
        # ä¾‹å¦‚ï¼š"Adik yang dekat denganku, aku ajak" -> "Adik, aku ajak"
        cleaned = re.sub(r'\s+yang\s+[^,]+(?=,)', '', sentence)
        
        # ç§»é™¤ bahwa å¼•å¯¼çš„å®¾è¯­ä»å¥
        cleaned = re.sub(r'\s+bahwa\s+[^.!?]+', '', cleaned)
        
        return cleaned
    
    # ============ æŒ‰å¥å­åˆ†å‰² ============
    sentences = re.split(r'[.!?]+', text)
    
    for sentence in sentences:
        sentence = sentence.strip()
        if not sentence:
            continue
        
        # ============ å…³é”®æ”¹è¿›ï¼šæå–ä¸»å¥å¹¶æ’é™¤ç‰¹æ®Šç»“æ„ ============
        sentence_main_only = extract_main_clause_only(sentence)
        
        # å¦‚æœè¿”å› Noneï¼Œè¯´æ˜æ˜¯ "ä¸»è¯­ + å…³ç³»ä»å¥ + è°“è¯­" ç»“æ„ï¼Œè·³è¿‡
        if sentence_main_only is None:
            continue
        
        # ============ æŸ¥æ‰¾é€—å·åˆ†éš”çš„ç»“æ„ ============
        comma_parts = sentence_main_only.split(',', 1)
        if len(comma_parts) != 2:
            continue
        
        potential_object = comma_parts[0].strip()
        main_clause = comma_parts[1].strip()
        
        # ============ éªŒè¯å‰ç½®éƒ¨åˆ† ============
        
        # 1. å‰ç½®éƒ¨åˆ†ä¸èƒ½ä¸ºç©ºæˆ–å¤ªçŸ­
        if len(potential_object) < 2:
            continue
        
        # 2. æ’é™¤ç§°å‘¼è¯­
        potential_lower = potential_object.lower()
        if potential_lower in vocatives:
            continue
        
        potential_words = potential_lower.split()
        if len(potential_words) == 1 and potential_words[0] in vocatives:
            continue
        
        # 3. æ’é™¤å›ºå®šè¡¨è¾¾
        is_fixed_expr = False
        for expr in fixed_expressions:
            if potential_lower.startswith(expr):
                is_fixed_expr = True
                break
        
        if is_fixed_expr:
            continue
        
        # 4. æ’é™¤çŠ¶è¯­ï¼ˆåœ°ç‚¹ã€æ—¶é—´ç­‰ï¼‰
        first_word = potential_words[0]
        if first_word in adverbial_starters:
            continue
        
        # 5. æ’é™¤å®Œæ•´å¥å­ï¼ˆåŒ…å«ä¸»å¥åŠ¨è¯ï¼‰
        has_main_verb = False
        for verb in copula_verbs:
            if verb in potential_lower:
                has_main_verb = True
                break
        
        if has_main_verb:
            continue
        
        # ============ éªŒè¯ä¸»å¥éƒ¨åˆ† ============
        
        main_words = main_clause.lower().split()
        if len(main_words) < 2:
            continue
        
        # ============ æ£€æŸ¥æ˜¯å¦ä»¥ dan/atau å¼€å¤´ï¼ˆå¹¶åˆ—å¥ï¼‰============
        if main_words[0] in {'dan', 'atau', 'tetapi', 'namun', 'tapi'}:
            continue
        
        # ============ æ£€æŸ¥æ˜¯å¦æ˜¯ä¸»ç³»è¡¨ç»“æ„ ============
        has_copula = False
        for copula in copula_verbs:
            if copula in main_words:
                has_copula = True
                break
        
        if has_copula:
            continue
        
        # ============ æŸ¥æ‰¾ä¸»è¯­å’ŒåŠç‰©åŠ¨è¯ ============
        
        found_subject = None
        found_verb = None
        
        # éå†ä¸»å¥ï¼Œå¯»æ‰¾åŠ¨è¯å’Œä¸»è¯­
        for i, word in enumerate(main_words):
            # è·³è¿‡å‰¯è¯å’ŒåŠ©åŠ¨è¯
            if word in adverbs or word in auxiliary_verbs:
                continue
            
            # ============ å¤„ç†ä»£è¯åç¼€ ============
            clean_word = word
            for suffix in pronoun_suffixes:
                if word.endswith(suffix) and len(word) > len(suffix) + 2:
                    clean_word = word[:-len(suffix)]
                    break
            
            # å»æ‰å‰ç¼€æ£€æŸ¥
            verb_root = clean_word
            for prefix in ['me', 'mem', 'men', 'meng', 'meny', 'ku', 'kau', 'di']:
                if clean_word.startswith(prefix) and len(clean_word) > len(prefix) + 2:
                    verb_root = clean_word[len(prefix):]
                    break
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯åŠç‰©åŠ¨è¯
            is_transitive = (word in transitive_verbs or 
                           clean_word in transitive_verbs or 
                           verb_root in transitive_verbs)
            
            if is_transitive:
                found_verb = word
                
                # ============ å‘å‰å’Œå‘åæŸ¥æ‰¾ä¸»è¯­ ============
                # å…ˆå‘å‰æŸ¥æ‰¾ï¼ˆæ ‡å‡†è¯­åºï¼šä¸»è¯­åœ¨åŠ¨è¯å‰ï¼‰
                for j in range(max(0, i-4), i):
                    if main_words[j] in subject_pronouns:
                        found_subject = main_words[j]
                        break
                
                # å¦‚æœæ²¡æ‰¾åˆ°ï¼Œå‘åæŸ¥æ‰¾ï¼ˆå€’è£…ï¼šä¸»è¯­åœ¨åŠ¨è¯åï¼‰
                if not found_subject:
                    for j in range(i+1, min(len(main_words), i+4)):
                        if main_words[j] in subject_pronouns:
                            found_subject = main_words[j]
                            break
                
                # æ‰¾åˆ°å°±é€€å‡º
                if found_subject:
                    break
        
        # ============ ç¡®è®¤æ˜¯å®¾è¯­å‰ç½® ============
        
        if found_subject and found_verb:
            # è¿˜åŸæ­£å¸¸è¯­åº
            restored = f"{found_subject} {found_verb} {potential_object}"
            
            found_patterns.append({
                'original_sentence': sentence,  # åŸå§‹å¥å­ï¼ˆåŒ…å«ä»å¥ï¼‰
                'main_clause_only': sentence_main_only,  # åªæœ‰ä¸»å¥çš„ç®€åŒ–ç‰ˆ
                'fronted_object': potential_object,
                'subject': found_subject,
                'verb': found_verb,
                'main_clause': main_clause,
                'restored_order': restored,
            })
    
    # ============ å»é‡ ============
    unique_patterns = []
    seen = set()
    
    for item in found_patterns:
        # åŸºäºä¸»å¥ç»“æ„å»é‡
        key = (item['fronted_object'].lower(), item['subject'], item['verb'])
        if key not in seen:
            unique_patterns.append(item)
            seen.add(key)
    
    # ============ ç»Ÿè®¡å’Œåˆ¤æ–­ ============
    
    total_count = len(unique_patterns)
    passed = (total_count == exact_count)
    
    # ============ ç”Ÿæˆè¯´æ˜ ============
    
    detail_parts = []
    
    if passed:
        detail_parts.append(f"âœ… æ­£ç¡®ï¼šæ‰¾åˆ°æ­£å¥½ {total_count} ä¸ªä¸»å¥å®¾è¯­å‰ç½®å¼ºè°ƒå¥ï¼ˆè¦æ±‚æ­£å¥½ {exact_count} ä¸ªï¼‰\n")
    else:
        if total_count < exact_count:
            shortage = exact_count - total_count
            detail_parts.append(f"âŒ é”™è¯¯ï¼šåªæ‰¾åˆ° {total_count} ä¸ªä¸»å¥å®¾è¯­å‰ç½®å¼ºè°ƒå¥ï¼Œå°‘äºè¦æ±‚çš„ {exact_count} ä¸ªï¼ˆè¿˜å·® {shortage} ä¸ªï¼‰\n")
        else:
            excess = total_count - exact_count
            detail_parts.append(f"âŒ é”™è¯¯ï¼šæ‰¾åˆ° {total_count} ä¸ªä¸»å¥å®¾è¯­å‰ç½®å¼ºè°ƒå¥ï¼Œè¶…è¿‡è¦æ±‚çš„ {exact_count} ä¸ªï¼ˆå¤šäº† {excess} ä¸ªï¼‰\n")
    
    if unique_patterns:
        detail_parts.append("æ‰¾åˆ°çš„ä¸»å¥å®¾è¯­å‰ç½®å¼ºè°ƒå¥è¯¦æƒ…ï¼š")
        for i, item in enumerate(unique_patterns, 1):
            detail_parts.append(f"\n  [{i}] ä¸»å¥å®¾è¯­å‰ç½®å¼ºè°ƒå¥")
            detail_parts.append(f"      åŸå§‹å¥å­: {item['original_sentence']}")
            detail_parts.append(f"      ä¸»å¥ç®€åŒ–: {item['main_clause_only']}")
            detail_parts.append(f"      å‰ç½®å®¾è¯­: {item['fronted_object']}")
            detail_parts.append(f"      ä¸»è¯­: {item['subject']}")
            detail_parts.append(f"      åŠ¨è¯: {item['verb']}")
            detail_parts.append(f"      è¿˜åŸè¯­åº: {item['restored_order']}")
    else:
        detail_parts.append("âŒ æœªæ‰¾åˆ°ä»»ä½•ä¸»å¥å®¾è¯­å‰ç½®å¼ºè°ƒå¥")
    
    detail = '\n'.join(detail_parts)
    
    return passed, detail


# ============ æµ‹è¯•ä»£ç  ============
if __name__ == "__main__":
    # æµ‹è¯•æ¡ˆä¾‹1ï¼šæ ‡å‡†å®¾è¯­å‰ç½®
    test_text1 = """
    Adik yang sangat dekat denganku, aku ajak untuk membantu.
    Hubungan kita, aku sangat menghargainya.
    Hadiah yang spesial adalah tujuan utamaku.
    """
    
    print("=" * 70)
    print("æµ‹è¯•æ¡ˆä¾‹1ï¼šåº”è¯¥æ‰¾åˆ°2ä¸ªä¸»å¥å®¾è¯­å‰ç½®å¼ºè°ƒå¥")
    print("=" * 70)
    passed, detail = check_fronted_emphasis(test_text1, exact_count=2)
    print(detail)
    print()
    
    # æµ‹è¯•æ¡ˆä¾‹2ï¼šéœ€è¦æ’é™¤çš„ç»“æ„
    test_text2 = """
    Kesalahpahaman ini, yang mungkin membuatmu merasa tidak nyaman, sangat aku sesali.
    Sayang, kemarin aku pergi ke mal.
    Terima kasih atas pengertianmu, dan aku sangat senang.
    Di mal, kita bertemu secara kebetulan.
    """
    
    print("=" * 70)
    print("æµ‹è¯•æ¡ˆä¾‹2ï¼šåº”è¯¥æ‰¾åˆ°0ä¸ªï¼ˆéƒ½éœ€è¦æ’é™¤ï¼‰")
    print("=" * 70)
    passed, detail = check_fronted_emphasis(test_text2, exact_count=0)
    print(detail)
    print()
    
    # æµ‹è¯•æ¡ˆä¾‹3ï¼šæ··åˆæ¡ˆä¾‹
    test_text3 = """
    Hadiah ulang tahun untukmu, aku pergi ke mal kemarin untuk membelinya. 
    Adik yang sangat dekat denganku, aku ajak untuk membantu memberikan pendapat tentang hadiah yang tepat. 
    Kesalahpahaman yang terjadi, aku ingin menjelaskan bahwa orang yang bersamaku adalah adikku. 
    Hubungan kita, aku sangat menghargainya dan tidak ingin ada kesalahpahaman yang mengganggu.
    Kesalahpahaman ini, yang mungkin membuatmu tidak nyaman, sangat aku sesali.
    """
    
    print("=" * 70)
    print("æµ‹è¯•æ¡ˆä¾‹3ï¼šæ··åˆæµ‹è¯•")
    print("=" * 70)
    passed, detail = check_fronted_emphasis(test_text3, exact_count=4)
    print(detail)
    print()


# ==================== å°å°¼è¯­å€Ÿè¯æ£€æµ‹ï¼ˆæ¯ä¸ªåº—åå¿…é¡»åŒ…å«æŒ‡å®šæ•°é‡ï¼‰====================

import re
from typing import Tuple, List, Dict
from collections import Counter

def check_indonesian_loanwords_each(text: str, required_count: int, initial_letter: str, letter_count: int) -> Tuple[bool, str]:
    """
    æ£€æµ‹æ¯ä¸ªåº—åæ˜¯å¦éƒ½åŒ…å«æŒ‡å®šæ•°é‡çš„ã€ä»¥æŒ‡å®šå­—æ¯å¼€å¤´ä¸”ç”±æŒ‡å®šå­—æ¯æ•°ç»„æˆçš„è‹±è¯­/æ³•è¯­/è·å…°è¯­å€Ÿè¯
    
    Args:
        text: è¦æ£€æµ‹çš„æ–‡æœ¬ï¼ˆåŒ…å«å¤šä¸ªåº—åï¼‰
        required_count: æ¯ä¸ªåº—åè¦æ±‚çš„å€Ÿè¯æ•°é‡
        initial_letter: è¦æ±‚çš„é¦–å­—æ¯ï¼ˆå¦‚ 'S', 'M', 'F' ç­‰ï¼‰
        letter_count: è¦æ±‚çš„å­—æ¯æ•°ï¼ˆå¦‚ 5, 6, 7 ç­‰ï¼‰
    
    Returns:
        (æ˜¯å¦é€šè¿‡, è¯¦ç»†è¯´æ˜)
    """
    
    # ============ ç±»å‹æ£€æŸ¥å’Œè½¬æ¢ ============
    if text is None:
        return False, "âŒ é”™è¯¯ï¼šè¾“å…¥æ–‡æœ¬ä¸ºç©ºï¼ˆNoneï¼‰"
    
    # å¤„ç†åˆ—è¡¨ç±»å‹
    if isinstance(text, list):
        store_names = []
        for item in text:
            if isinstance(item, str):
                store_names.append(item.strip())
            elif isinstance(item, dict):
                for key in ['name', 'text', 'content', 'title']:
                    if key in item and isinstance(item[key], str):
                        store_names.append(item[key].strip())
                        break
        text = store_names
    
    # å¤„ç†å­—ç¬¦ä¸²ç±»å‹
    if isinstance(text, str):
        lines = text.strip().split('\n')
        store_names = []
        for line in lines:
            line = line.strip()
            # ç§»é™¤ç¼–å·ã€ç¬¦å·ç­‰
            line = re.sub(r'^[\d\-\*\â€¢]+[\.\)]\s*', '', line)
            line = re.sub(r'^[\-\*\â€¢]+\s*', '', line)
            # æå–å†’å·æˆ–ç ´æŠ˜å·å‰çš„éƒ¨åˆ†ä½œä¸ºåº—å
            if ':' in line or 'ï¼š' in line:
                line = re.split(r'[:ï¼š]', line)[0].strip()
            if 'â€”' in line or 'â€“' in line:
                parts = re.split(r'[â€”â€“]', line)
                if len(parts) > 0:
                    line = parts[0].strip()
            if line and len(line) > 0:
                store_names.append(line)
        text = store_names
    
    if not text or (isinstance(text, list) and len(text) == 0):
        return False, "âŒ é”™è¯¯ï¼šæœªæ‰¾åˆ°ä»»ä½•åº—å"
    
    # ============ å°å°¼è¯­å€Ÿè¯è¯åº“ï¼ˆæŒ‰é¦–å­—æ¯å’Œå­—æ¯æ•°åˆ†ç±»ï¼‰============
    
    INDONESIAN_LOANWORDS = {
        'S': {    
            5: ['serba'],
            6: ['sistem',  'sukses', 'sentra', 'simpel', 'sentral'],
            7: ['spesial'],        
        },
        'M': {
            4: ['moda'],
        },
        'F': {
            5: ['fokus'],            
            7: ['favorit',  'fantasi'],            
        },
        'K': {    
            6: ['komplit', 'komplet'],
            7: ['kreatif'],
            8: ['kualitas'],            
        },
        'P': {
            7: [ 'populer',  'praktis'],            
        },
        'T': {
            7: [ 'tradisi'],
        },
        'E': { 
            6: ['ekspres',  'elegan'],
            7: ['ekonomis', 'efisien'],
            8: [ 'ekonomi']
        }
    }
    
    # ============ è¾…åŠ©å‡½æ•°ï¼šæ£€æŸ¥æ˜¯å¦ä¸ºå€Ÿè¯ ============
    
    def is_valid_loanword(word: str, letter: str, length: int) -> bool:
        """æ£€æŸ¥è¯æ˜¯å¦æ˜¯æœ‰æ•ˆçš„å€Ÿè¯"""
        word_lower = word.lower().strip('.,;:!?()[]{}"""\'\'â€”â€“-')
        letter_lower = letter.lower()
        
        # æ£€æŸ¥é¦–å­—æ¯
        if not word_lower.startswith(letter_lower):
            return False
        
        # æ£€æŸ¥å­—æ¯æ•°
        if len(word_lower) != length:
            return False
        
        # æ£€æŸ¥æ˜¯å¦åœ¨å€Ÿè¯è¯åº“ä¸­
        if letter.upper() in INDONESIAN_LOANWORDS:
            if length in INDONESIAN_LOANWORDS[letter.upper()]:
                if word_lower in INDONESIAN_LOANWORDS[letter.upper()][length]:
                    return True
        
        # å®½æ¾æ¨¡å¼ï¼šå¦‚æœä¸åœ¨è¯åº“ä½†ç¬¦åˆå€Ÿè¯ç‰¹å¾
        # å€Ÿè¯ç‰¹å¾ï¼šåŒ…å«ç‰¹å®šå­—æ¯ç»„åˆ
        loanword_patterns = [
            r'ph', r'th', r'ch', r'tion', r'sion', 
            r'sch', r'ck', r'ff', r'ss', r'tial', r'cial'
        ]
        
        for pattern in loanword_patterns:
            if re.search(pattern, word_lower):
                return True
        
        return False
    
    # ============ æ£€æµ‹æ¯ä¸ªåº—åä¸­çš„å€Ÿè¯ ============
    
    initial_upper = initial_letter.upper()
    initial_lower = initial_letter.lower()
    
    store_details = []
    failed_stores = []
    all_passed = True
    
    for store_idx, store_name in enumerate(text, 1):
        if not isinstance(store_name, str):
            store_name = str(store_name)
        
        store_name_clean = store_name.strip()
        words = store_name_clean.split()
        
        store_loanwords = []
        
        for word in words:
            word_clean = word.strip('.,;:!?()[]{}"""\'\'â€”â€“-')
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯æœ‰æ•ˆå€Ÿè¯
            if is_valid_loanword(word_clean, initial_letter, letter_count):
                store_loanwords.append(word_clean)
        
        loanword_count = len(store_loanwords)
        is_passed = (loanword_count == required_count)
        
        store_details.append({
            'index': store_idx,
            'store_name': store_name_clean,
            'loanwords': store_loanwords,
            'count': loanword_count,
            'required': required_count,
            'passed': is_passed
        })
        
        if not is_passed:
            all_passed = False
            failed_stores.append({
                'index': store_idx,
                'store_name': store_name_clean,
                'count': loanword_count,
                'required': required_count,
                'difference': loanword_count - required_count
            })
    
    # ============ åˆ¤æ–­æ˜¯å¦é€šè¿‡ ============
    
    passed = all_passed
    
    # ============ ç”Ÿæˆè¯¦ç»†è¯´æ˜ ============
    
    detail_parts = []
    
    # æ„å»ºæ¡ä»¶æè¿°
    condition_desc = f"ä»¥å­—æ¯ '{initial_letter}' å¼€å¤´ä¸”ç”± {letter_count} ä¸ªå­—æ¯ç»„æˆçš„å€Ÿè¯"
    
    if passed:
        detail_parts.append(f"âœ… æ­£ç¡®ï¼šæ‰€æœ‰ {len(store_details)} ä¸ªåº—åéƒ½åŒ…å«æ­£å¥½ {required_count} ä¸ª{condition_desc}\n")
        
        detail_parts.append("å„åº—åæ£€æµ‹ç»“æœï¼š")
        for detail in store_details:
            detail_parts.append(f"  {detail['index']}. {detail['store_name']}")
            if detail['loanwords']:
                detail_parts.append(f"     âœ“ åŒ…å«å€Ÿè¯: {', '.join(detail['loanwords'])} ({detail['count']}ä¸ª)")
            else:
                detail_parts.append(f"     âœ“ ï¼ˆæ°å¥½0ä¸ªå€Ÿè¯ï¼‰")
    
    else:
        failed_count = len(failed_stores)
        passed_count = len(store_details) - failed_count
        
        detail_parts.append(f"âŒ é”™è¯¯ï¼šæœ‰ {failed_count} ä¸ªåº—åä¸ç¬¦åˆè¦æ±‚ï¼ˆå…± {len(store_details)} ä¸ªåº—åï¼Œ{passed_count} ä¸ªé€šè¿‡ï¼‰\n")
        
        detail_parts.append("âœ… é€šè¿‡çš„åº—åï¼š")
        passed_stores = [d for d in store_details if d['passed']]
        if passed_stores:
            for detail in passed_stores:
                detail_parts.append(f"  {detail['index']}. {detail['store_name']}")
                if detail['loanwords']:
                    detail_parts.append(f"     âœ“ åŒ…å«å€Ÿè¯: {', '.join(detail['loanwords'])} ({detail['count']}ä¸ª)")
        else:
            detail_parts.append("  ï¼ˆæ— ï¼‰")
        
        detail_parts.append("\nâŒ æœªé€šè¿‡çš„åº—åï¼š")
        for fail in failed_stores:
            detail_parts.append(f"  {fail['index']}. {fail['store_name']}")
            if fail['difference'] > 0:
                detail_parts.append(f"     âœ— æ‰¾åˆ° {fail['count']} ä¸ªå€Ÿè¯ï¼Œè¶…è¿‡è¦æ±‚çš„ {fail['required']} ä¸ªï¼ˆå¤š {fail['difference']} ä¸ªï¼‰")
            else:
                shortage = abs(fail['difference'])
                detail_parts.append(f"     âœ— æ‰¾åˆ° {fail['count']} ä¸ªå€Ÿè¯ï¼Œå°‘äºè¦æ±‚çš„ {fail['required']} ä¸ªï¼ˆå°‘ {shortage} ä¸ªï¼‰")
            
            # æ˜¾ç¤ºè¯¥åº—åæ‰¾åˆ°çš„å€Ÿè¯
            failed_detail = next(d for d in store_details if d['index'] == fail['index'])
            if failed_detail['loanwords']:
                detail_parts.append(f"     æ‰¾åˆ°çš„å€Ÿè¯: {', '.join(failed_detail['loanwords'])}")
        
        # æä¾›å‚è€ƒå»ºè®®
        detail_parts.append(f"\nğŸ’¡ ç¬¦åˆæ¡ä»¶çš„ '{initial_letter}' å¼€å¤´ã€{letter_count} å­—æ¯çš„å€Ÿè¯å‚è€ƒï¼š")
        if initial_upper in INDONESIAN_LOANWORDS:
            if letter_count in INDONESIAN_LOANWORDS[initial_upper]:
                examples = INDONESIAN_LOANWORDS[initial_upper][letter_count]
                detail_parts.append(f"  {', '.join(examples[:15])}")
            else:
                detail_parts.append(f"  ï¼ˆè¯åº“ä¸­æš‚æ—  {letter_count} å­—æ¯çš„ {initial_letter} å¼€å¤´å€Ÿè¯ï¼‰")
                # æ˜¾ç¤ºç›¸è¿‘å­—æ¯æ•°çš„å€Ÿè¯
                nearby_lengths = sorted([l for l in INDONESIAN_LOANWORDS[initial_upper].keys() 
                                        if abs(l - letter_count) <= 2])
                if nearby_lengths:
                    detail_parts.append(f"\n  ç›¸è¿‘å­—æ¯æ•°çš„å€Ÿè¯ï¼š")
                    for length in nearby_lengths:
                        examples = INDONESIAN_LOANWORDS[initial_upper][length]
                        detail_parts.append(f"    {length}å­—æ¯: {', '.join(examples[:5])}")
        else:
            detail_parts.append(f"  ï¼ˆè¯åº“ä¸­æš‚æ—  {initial_letter} å¼€å¤´çš„å€Ÿè¯ï¼‰")
    
    detail = '\n'.join(detail_parts)
    
    return passed, detail


# ==================== æµ‹è¯•ä»£ç  ====================

if __name__ == "__main__":
    # æµ‹è¯•æ¡ˆä¾‹1ï¼šè¦æ±‚æ¯ä¸ªåº—ååŒ…å«1ä¸ª5å­—æ¯çš„Så¼€å¤´å€Ÿè¯
    test_case_1 = [
        "Super Makmur Jaya",
        "Toko Smart Indonesia",
        "Pasar Sport Sejahtera"
    ]
    
    print("=" * 80)
    print("æµ‹è¯•æ¡ˆä¾‹1ï¼šè¦æ±‚æ¯ä¸ªåº—ååŒ…å«1ä¸ª5å­—æ¯çš„Så¼€å¤´å€Ÿè¯")
    print("åº—åï¼š")
    for name in test_case_1:
        print(f"  - {name}")
    
    result, detail = check_indonesian_loanwords_each(test_case_1, 1, 'S', 5)
    print(f"\nç»“æœï¼š{'âœ… é€šè¿‡' if result else 'âŒ å¤±è´¥'}")
    print(f"\n{detail}")
    
    # æµ‹è¯•æ¡ˆä¾‹2ï¼šå¤±è´¥æ¡ˆä¾‹ï¼ˆç¬¬3ä¸ªåº—åæ²¡æœ‰ç¬¦åˆæ¡ä»¶çš„å€Ÿè¯ï¼‰
    test_case_2 = [
        "Super Makmur Jaya",
        "Toko Smart Indonesia",
        "Pasar Sejahtera Sentosa"  # Sentosaä¸æ˜¯å€Ÿè¯
    ]
    
    print("\n" + "=" * 80)
    print("æµ‹è¯•æ¡ˆä¾‹2ï¼šè¦æ±‚æ¯ä¸ªåº—ååŒ…å«1ä¸ª5å­—æ¯çš„Så¼€å¤´å€Ÿè¯ï¼ˆç¬¬3ä¸ªä¸ç¬¦åˆï¼‰")
    print("åº—åï¼š")
    for name in test_case_2:
        print(f"  - {name}")
    
    result, detail = check_indonesian_loanwords_each(test_case_2, 1, 'S', 5)
    print(f"\nç»“æœï¼š{'âœ… é€šè¿‡' if result else 'âŒ å¤±è´¥'}")
    print(f"\n{detail}")
    
    # æµ‹è¯•æ¡ˆä¾‹3ï¼šå¤±è´¥æ¡ˆä¾‹ï¼ˆç¬¬1ä¸ªåº—åæœ‰2ä¸ªç¬¦åˆæ¡ä»¶çš„å€Ÿè¯ï¼‰
    test_case_3 = [
        "Super Smart Mart",  # æœ‰2ä¸ª5å­—æ¯Så¼€å¤´å€Ÿè¯
        "Toko Sport Indonesia",
        "Pasar Salon Makmur"
    ]
    
    print("\n" + "=" * 80)
    print("æµ‹è¯•æ¡ˆä¾‹3ï¼šè¦æ±‚æ¯ä¸ªåº—ååŒ…å«1ä¸ª5å­—æ¯çš„Så¼€å¤´å€Ÿè¯ï¼ˆç¬¬1ä¸ªæœ‰2ä¸ªï¼‰")
    print("åº—åï¼š")
    for name in test_case_3:
        print(f"  - {name}")
    
    result, detail = check_indonesian_loanwords_each(test_case_3, 1, 'S', 5)
    print(f"\nç»“æœï¼š{'âœ… é€šè¿‡' if result else 'âŒ å¤±è´¥'}")
    print(f"\n{detail}")
    
    # æµ‹è¯•æ¡ˆä¾‹4ï¼šè¦æ±‚æ¯ä¸ªåº—ååŒ…å«2ä¸ª6å­—æ¯çš„Må¼€å¤´å€Ÿè¯
    test_case_4 = [
        "Modern Mandiri Mart",  # Modern(6), Mandiri(7), Mart(4)
        "Toko Makmur Indonesia"  # Makmur(6)
    ]
    
    print("\n" + "=" * 80)
    print("æµ‹è¯•æ¡ˆä¾‹4ï¼šè¦æ±‚æ¯ä¸ªåº—ååŒ…å«2ä¸ª6å­—æ¯çš„Må¼€å¤´å€Ÿè¯")
    print("åº—åï¼š")
    for name in test_case_4:
        print(f"  - {name}")
    
    result, detail = check_indonesian_loanwords_each(test_case_4, 2, 'M', 6)
    print(f"\nç»“æœï¼š{'âœ… é€šè¿‡' if result else 'âŒ å¤±è´¥'}")
    print(f"\n{detail}")
    
    # æµ‹è¯•æ¡ˆä¾‹5ï¼šè¦æ±‚0ä¸ªï¼ˆå…è®¸æ²¡æœ‰å€Ÿè¯ï¼‰
    test_case_5 = [
        "Toko Makmur Jaya",
        "Pasar Sejahtera"
    ]
    
    print("\n" + "=" * 80)
    print("æµ‹è¯•æ¡ˆä¾‹5ï¼šè¦æ±‚æ¯ä¸ªåº—ååŒ…å«0ä¸ª5å­—æ¯çš„Så¼€å¤´å€Ÿè¯")
    print("åº—åï¼š")
    for name in test_case_5:
        print(f"  - {name}")
    
    result, detail = check_indonesian_loanwords_each(test_case_5, 0, 'S', 5)
    print(f"\nç»“æœï¼š{'âœ… é€šè¿‡' if result else 'âŒ å¤±è´¥'}")
    print(f"\n{detail}")
